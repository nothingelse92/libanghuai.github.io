<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Live and Learn"><meta name="keywords" content=""><meta name="author" content="Out of Memory,undefined"><meta name="copyright" content="Out of Memory"><title>Live and Learn【Out of Memory】</title><link rel="stylesheet" href="../../css/fan.css"><link rel="stylesheet" href="../../css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="../../favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="../../js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="author-info"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Out of Memory</div><div class="author-info-description">Live and Learn</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/libanghuai" target="_blank">GitHub<i class="icon-dot bg-color8"></i></a><a class="links-button button-hover" href="mailto:libanghuai@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color1"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1185719433&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color10"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="../../archives"><span class="pull-top">日志</span><span class="pull-bottom">123</span></a><a class="author-info-articles-tags article-meta" href="../../tags"><span class="pull-top">标签</span><span class="pull-bottom">39</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Out of Memory</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><div id="recent-posts"><!-- each post in page.posts.sort('date', -1).limit(10).toArray()--><!-- config中配置按照什么排序--><div class="recent-post-item"><a class="post-title" href="../../2019/12/30/ACNet-Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-Asymmetric-Convolution-Blocks/">ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1908.03930" target="_blank" rel="noopener">https://arxiv.org/abs/1908.03930</a></p>
<p>ICCV2019的一篇工作,论文提出了Asymmetric Convolutional Block这样一个新的模块，出发点是作者发现对于我们常用的dxd的卷积核来说通常对于效果影响最大的是中心十字架状的skeleton：</p>
<p><img src="ACNet-Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-Asymmetric-Convolution-Blocks-截屏2019-12-3022.14.00.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/12/30/ACNet-Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-Asymmetric-Convolution-Blocks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/12/23/MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning/">MIC: Mining Interclass Characteristics for Improved Metric Learning </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Metric-Learning/">Metric Learning</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Roth_MIC_Mining_Interclass_Characteristics_for_Improved_Metric_Learning_ICCV_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2019/papers/Roth_MIC_Mining_Interclass_Characteristics_for_Improved_Metric_Learning_ICCV_2019_paper.pdf</a></p>
<p>ICCV2019一篇关于Metric Learning的论文，Metric Learning相关的工作是在做特征表示相关的内容，这篇论文的出发点是从object特征中去除非特征主体的特征从而能对于后续的任务更好的学习。论文不是很好懂需要仔细琢磨。</p>
<p><img src="MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning-截屏2019-12-2422.49.45.png" alt=""></p>
<p>对于一般的分类任务我们通常会把特征分成inter-class类间特征、intra-class类内特征, 那么这篇论文中类似，inter-class特征可以解释为区分特征主体的特征，比如区分狗和猫，intra-class特征可以解释为通用的一些特征，比如光照、角度等。对于一般的分类模型通常会忽略intra-class的特征直接用gt label来监督模型训练强行区分不同的类别。<br>那么本论文主要想细分这两个特征从而更精确的分类，假设输入图为img，f为抽特征的CNN，那么img的特征可以表示为f(img), 对于一般的metric learning通常会用一个encoder E在f(img)的基础上进一步对特征进行映射，从而基于E(f(img))做进一步的处理，比如计算相似度等，这个E的输出也可以理解为对这个img在高维特征空间的embedding。</p>
<p>类别一般的metric learning，本论文有两个encoder， encoder E<sub>α</sub> 编码inter-class特征，这个学习很容易，直接用gt的label监督就好，另一个encoder E<sub>β</sub> 编码intra-class特征，这部分不是很容易学习因为没有gt.那么论文中是怎么做的呢？</p>
<ol>
<li>首先对于类别n的所有的图片，计算每一张图的img的f(img)特征值，f通常是在ImageNet上pretrain过的某个model</li>
<li>计算所有f(img<sub>i</sub>)的mean和standard deviation（标准差）</li>
<li>最后利用公式Z<sub>i</sub> = (f(img<sub>i</sub>) - mean<sub>n</sub>) / StandardDeviation<sub>n</sub>进行特征转化</li>
</ol>
<p>这样做的目的是因为即使多张图片具体相同的intra-class特征，但是由于拍摄地点、拍摄行为等因素还是会导致不太一样，所以通过一个基本的转化得到的Z集合一定程度上消除了这种bias。</p>
<p>那么所有的图片经过上述的处理之后对于特征集合Z就可以聚类了，假设可以聚成K类{C<sub>1</sub>……C<sub>k</sub>},那么C集合就可以作为E<sub>β</sub>训练的label了！你可以把C集合想像成角度、光照、遮挡等一系列属性。</p>
<p>因为E<sub>β</sub>和E<sub>α</sub>共享f并且end2end训练，所以两者之前会相互影响，两者学习的特征也难免会有overlap，那么为了限制两者分别去学习inter-class类间特征和intra-class类内特征，作者也做了一些优化（ Minimizing Mutual Information）。具体可以参考下面这个公式，R是一个小网络用来将E<sub>β</sub>编码的信息映射到E<sub>α</sub>的空间上，⊙就是普通的element wise乘，r代表梯度反转，是对抗学习中常用的一种方式，和实际的需求也比较接近：</p>
<p><img src="MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning-截屏2019-12-2423.35.20.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/12/23/MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/12/22/High-level-Semantic-Feature-Detection-A-New-Perspective-for-Pedestrian-Detection/">High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Pedestrian-Detection/">Pedestrian Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Face/">Face</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_High-Level_Semantic_Feature_Detection_A_New_Perspective_for_Pedestrian_Detection_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_High-Level_Semantic_Feature_Detection_A_New_Perspective_for_Pedestrian_Detection_CVPR_2019_paper.pdf</a><br>这是CVPR2019的行人检测论文，和Center and Scale Prediction: A Box-free Approach for Object Detection是同一篇论文，倒是后者的标题更能体现出论文所提的方法，论文主要就是异于Anchor Based的方法提出了预测中心点+框的scale的新方法来解决行人检测的问题或者严格来讲解决一般刚性物体的检测问题. 从行人检测的角度来说是一个比较新颖也比较值得去思考的方法。下图是它整体的Pipeline：</p>
<p><img src="High-level-Semantic-Feature-Detection-A-New-Perspective-for-Pedestrian-Detection-屏幕快照 2019-12-22 下午4.35.00.png" alt=""></p>
<p>论文所提的方法三言两语倒是可以说出大概，但是一些细节还是值得去琢磨的：</p>
<ol>
<li>模型结构很简单，Res50，stage 2 - 5通过deconv将高层feature map统一到一个固定的resolution然后concat到一起，其实比较类似FPN了，r倍downsample之后接一层Conv3x3然后再分别接两个Conv1x1引出两个branch，一个预测中心点，一个预测Scale，两者都是用heatmap出.</li>
<li>模型结构很简单，比较重要的就是gt是怎么生成的以及如何去监督了，首先说center的heatmap，它的大小（w x h）和downsample之后的feature map维度是一致的, 给定一个bounding box可以得到一个标注的中心，那么原则上只有这个点的gt为1，其余位置gt都为0，那么论文中为了更加符合实际场景同时也更加便于训练，用了一个比较常用的高斯分布来生成gt。而至于scale map，对于给定的中心点，它的取值是log(h),h为boundingbox的高度，这样可以通过事先定义的ratio反算出具体的框。同样的在实际生成gt的时候会围绕中心点半径2的范围内都置为log(h)其余为0；</li>
<li>Loss方面center的定位可以看为classification任务所以用ce loss，而scale的预测可以理解为regression任务所以用smoothL1 loss。</li>
<li>因为gt的监督是基于r倍downsample的所以为了更精确的预测会自然想到加一个offset分支来更精确的回归：<br>L = λ<sub>c</sub>L<sub>center</sub> + λ<sub>s</sub>L<sub>scale</sub> + λ<sub>o</sub>L<sub>offset</sub></li>
</ol>
<p>最后就是这篇论文刷的点还是比较高的，也会是做检测比较好的一个方向。</p>
</div></div><a class="button-hover more" href="../../2019/12/22/High-level-Semantic-Feature-Detection-A-New-Perspective-for-Pedestrian-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/12/22/Bi-box-Regression-for-Pedestrian-Detection-and-Occlusion-Estimation/">Bi-box Regression for Pedestrian Detection and Occlusion Estimation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf</a></p>
<p>ECCV2018的一篇行人检测的问题，论文的做法其实比较简单，就是让模型在学习全身框的同时也出可见框的结果，两者可以起到互补的作用，整体模型结构也比较简单就是在backbone之后接两个平行的业务层，一个出可见框的结果一个出全身框的结构，两个平行业务层的结构是一致的，具体的逻辑可以参考论文里的下面这张图：</p>
<p><img src="Bi-box-Regression-for-Pedestrian-Detection-and-Occlusion-Estimation-屏幕快照 2019-12-22 下午2.40.42.png" alt=""></p>
<p>那么这篇论文需要注意的更多的是一些细节：</p>
<ol>
<li>虽然模型最后是出两个平行的业务层一个出可见框的结果一个出全身框的结果，但是两者的proposal或者anchor是一摸一样的！假设目前对于同一个标注的gt，可见框为Box<sub>visible</sub>,全身框为Box<sub>full body</sub>,那么对于个给定的proposal P, 当IoU(P, Box<sub>full body</sub>) &gt; $\alpha$ &amp;&amp; IoB(P, Box<sub>visible</sub>) &gt;  $\beta$ 是P为正样本，否则P就是负样本。</li>
<li>因为两个branch用的是同一个proposal，然后基于这个proposal去分别回归offset，那么最后在后处理的时候这个proposal就只能有一个score，作者给了三个方法，第一只考虑可见分支，fc出的结果来个softmax就好，第二是只考虑全身分支，同样fc出的结果来个softmax就好，第三就是融合两个分支的结果把两个fc的结果对应相加然后再接个softmax就好，这样类似Bagging的做法来增强模型的鲁棒性。</li>
<li>论文另外一个需要注意的就是训练的细节了，全身框那个分支的回归就是和一般的Faster RCNN一样，直接只回归正样本的offset，但是对于可见框的回归是同时回归正样本和负样本的。这一点还是比较make sense的，因为论文中需要同一个proposal去同时回归全身框和可见框，所以对于那些高度遮挡的case，正样本proposal会比较少，反而是无遮挡或者轻微遮挡的case因为可见框和全身框的Overlap很大一般不会有明显的影响，因此这就会导致最后模型两个分支学的东西几乎是一样的。那么对于可见框分支对负样本怎么学习呢？作者将其学习目标定义为这个proposal的中心，所以它的gt为(0, 0, -INF, -INF)（实际使用的时候是(0, 0, -3, -3)），因为是log-space所以是-INF，这个可以看论文，这样就可以显示的将遮挡情况让模型进行感知。感觉这个做法会对FP的处理会比较有帮助。</li>
</ol>
</div></div><a class="button-hover more" href="../../2019/12/22/Bi-box-Regression-for-Pedestrian-Detection-and-Occlusion-Estimation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/12/19/SSA-CNN-Semantic-Self-Attention-CNN-for-Pedestrian-Detection/">SSA-CNN: Semantic Self-Attention CNN for Pedestrian Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-19</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Attention/">Attention</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Pedestrian-Detection/">Pedestrian Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1902.09080.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1902.09080.pdf</a></p>
<p>关于行人检测的论文，出发点的话感觉和这篇论文是比较类似的<a href="http://libanghuai.com/2019/12/08/Mask-Guided-Attention-Network-for-Occluded-Pedestrian-Detection/">Mask-Guided Attention Network for Occluded Pedestrian Detection</a>。主要想利用mask作为额外的监督来辅助提升检测的效果。两篇论文的Segmentation的标注也都是粗粒度的，MGAN是利用可见框标注作为mask，本篇论文是直接用的全身框作为mask。至于具体的做法可以参考论文里给的这张图：</p>
<p><img src="SSA-CNN-Semantic-Self-Attention-CNN-for-Pedestrian-Detection-屏幕快照 2019-12-19 下午10.04.51.png" alt=""></p>
<p>整体框架结构依然follow的Faster RCNN逻辑，论文所提的SSA-CNN主要分成两个部分：<strong>SSA-RPN</strong>和<strong>SSA-RCNN</strong>:</p>
<ol>
<li>SSA-RPN部分，从backbone（VGG16）的conv4_3和conv5_3分别引出一个segmentation分支，这个seg分支本身用全身框的mask去监督，这个分支的feature map然后再和对应的conv4_3或者conv5_3 feature map concat到一起引出正常的RPN业务分支cls和bbox，然后用预定义的target anchor去监督。有一个需要注意的地方是 <strong>只有conv5_3这个分支出的预测结果才会被接下来的SSA-RCNN使用，conv4_3这个分支出的seg类似一个外挂只做额外的监督，inference的时候不用</strong></li>
<li>SSA-RCNN部分，这个部分和传统的Faster RCNN的Fast RCNN分支处理不完全一样，一般的Fast RCNN是利用RPN的proposal经过ROIPooling得到对应的feature去做cls和reg，但对于SSA-RCNN则是直接利用SSA-RPN的proposal去原图中抠取，然后padding resize之后作为SSA-RCNN的输入，这样做的原因作者也在论文中阐明了: <strong>the pooling bins collapse if ROI’s input resolution is smaller than output</strong>，比如输入112x112映射到ROIPooling那就对应着7x7，但是CityPersons和 Caltech数据集有大量小于112 x 112大小的人，所以这个问题就会变得比较严重。其他SSA-RCNN模型部分就和SSA-RPN很像了，conv4_3和conv5_3引出的seg分支的feature map会先concat到一起，然后再和conv5_3 concat然后接入cls+reg来做行人的定位。</li>
</ol>
<p>这篇论文其他应该就没有需要注意的了。</p>
</div></div><a class="button-hover more" href="../../2019/12/19/SSA-CNN-Semantic-Self-Attention-CNN-for-Pedestrian-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/12/19/Pedestrian-Detection-with-Autoregressive-Network-Phases/">Pedestrian Detection with Autoregressive Network Phases</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Brazil_Pedestrian_Detection_With_Autoregressive_Network_Phases_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Brazil_Pedestrian_Detection_With_Autoregressive_Network_Phases_CVPR_2019_paper.pdf</a></p>
<p>不算老的论文，CVPR2019的关于行人检测的论文。</p>
<p><img src="Pedestrian-Detection-with-Autoregressive-Network-Phases-截屏2019-12-1923.59.25.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/12/19/Pedestrian-Detection-with-Autoregressive-Network-Phases/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/12/16/RetinaFace-Single-stage-Dense-Face-Localisation-in-the-Wild/">RetinaFace: Single-stage Dense Face Localisation in the Wild</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-16</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Face/">Face</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Multi-Task/">Multi-Task</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1905.00641" target="_blank" rel="noopener">https://arxiv.org/abs/1905.00641</a></p>
<p>这篇论文就很糙快猛了，multitask(人脸检测+人脸关键点定位+人脸3D Mesh + 人脸分类)涨点：<br><img src="RetinaFace-Single-stage-Dense-Face-Localisation-in-the-Wild-屏幕快照 2019-05-05 下午5.45.42.jpg" alt=""><br>Loss也比较直接，Lcls为softmax loss，Lbox和Lpts为L1 Loss，Lpixel具体如下，R为3D图片在2D平面的投影，I为gt：<br><img src="RetinaFace-Single-stage-Dense-Face-Localisation-in-the-Wild-截屏2019-12-1622.39.25.png" alt=""><br>其他这篇论文似乎就没有什么可以描述的了…</p>
</div></div><a class="button-hover more" href="../../2019/12/16/RetinaFace-Single-stage-Dense-Face-Localisation-in-the-Wild/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/12/16/MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment/">MobileFAN: Transferring Deep Hidden Representation for Face Alignment</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-15</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Face/">Face</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Mimick/">Mimick</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1908.03839" target="_blank" rel="noopener">https://arxiv.org/abs/1908.03839</a></p>
<p>一篇人脸关键点的论文,论文的核心应该是模型蒸馏，但是单纯的基于mobilenetv2网络在多个benchmark也刷了挺不错的点，甚至是好于CVPR2018的LAB，MobileFAN就是mobilenetv2加上出heatmap直接出出来的结果，MobileFAN + KD就是加上知识蒸馏的结果，下表是WFLW上面的点,坦白讲点还是很高的：<br><img src="MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment-屏幕快照 2019-08-16 下午6.41.34.png" alt=""><br>知识蒸馏方面论文中提了两个方面：<br>一是<strong>Feature-Aligned Distillation</strong>，出发点是希望teacher和student网络学习的分布是一致的，两个网络deconv层的spatial维度是一致的差别就在于channel不一致，作者简单的用1x1把两者拉统一，Feature-Aligned Distillation和之前用大模型带小模型的做法基本是一致的，学习的时候直接mse监督两个feature map。<br><img src="MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment-Screenshot from 2019-08-15 10-54-20.png" alt=""><br>另一个是<strong>Feature-Similarity Distillation</strong>，这部分主要想让teacher和student网络表示的空间信息是一致的，比如人脸结构轮廓等，这一部分首先teacher和student网络分别去计算不同pixel之间的相似度：<br><img src="MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment-截屏2019-12-1622.24.41.png" alt=""><br>然后两个网络feature map之间计算相似度：<br><img src="MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment-截屏2019-12-1622.24.46.png" alt=""><br>总之感觉从结果上看还是挺惊艳的，模型蒸馏感觉对于小模型可以好好搞一下。</p>
</div></div><a class="button-hover more" href="../../2019/12/16/MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/12/15/Improving-Occlusion-and-Hard-Negative-Handling-for-Single-Stage-Pedestrian-Detectors/">Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Noh_Improving_Occlusion_and_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/Noh_Improving_Occlusion_and_CVPR_2018_paper.pdf</a><br>一篇行人检测的论文，论文选择在single stage架构上做文章，主要解决两个问题，一个是遮挡问题，一个是FP问题，先说遮挡问题，遮挡问题论文提出来的整体解法和zhangshifeng的那篇Occlusion-aware RCNN论文做法算是很像的了，Occlusion-aware RCNN把行人整体分成了5个part，每个part会出一个score来对feature map做attention，这篇论文就是把行人平均分成M x N(论文中去M = 6， N = 3)个part来做和Occlusion-aware RCNN类似的事情，细节的话可以参考下面这张图：</p>
<p><img src="Improving-Occlusion-and-Hard-Negative-Handling-for-Single-Stage-Pedestrian-Detectors-截屏2019-12-1523.50.06.png" alt=""></p>
<p>首先模型会出基本的reg结果，这个没有什么特殊的，这个方案特殊就是特殊在这个”Occlusion-aware Score”, 对于每一个anchor模型会去学一个M x N的矩阵，代表每一个part的confidence，这个矩阵的gt生成也比较直接，如果某一个grid中的可见行人部分(这个解决可见框标注)面积 &gt; t * area(grid)，论文中t取0.4，那么这个part的score就为1，否则就为0，因为对于每一个anchor我们只需要一个score，那么论文中也说出了两种从这个M x N矩阵中生成一个score的方法，1. argmax 2. 接几层网络以这个矩阵为输入去直接学这个score，后一个方法会考虑不同的遮挡pattern(w1….wp)会比第一个更灵活点（w也是学习的,维度也是MxN）.</p>
<p>论文另一个核心就是对FP的处理了，作者的出发点还是增强score的表现力，具体做法可以参考下面这张图，核心是这个”Grid Classifier”,对于网络中间的一些feature map比如是L1…Ln,假设对于第l个feature map，其大小为w<sub>l</sub> x h<sub>l</sub> x c<sub>l</sub>,那么通过一个1x1的conv就可以得到一个w<sub>l</sub> x h<sub>l</sub>的confidence map（至于这个confidence map如何监督也比较直接，gt就是把原来的输入图划分成w<sub>l</sub> x h<sub>l</sub> 大小，每一个小grid的gt值就是这个小grid于gt框的交面积与这个grid面积的比值），那么将这个confidence map插值到原来的输入图大小WxH，所有的1-L层feature map都做类似的事情，那么就会得到L个WxH大小的confidence map,取平均之后就可以得到最终的一个WxH的confidence map，那么对于某一个预测框，最终的score就是这个confidence map上对应预测框内部元素的均值，这个score和前面Occlusion-aware Score想乘就是最终的boundingbox的score。这一步主要是想通过多层的信息共同参与打分来增加模型的鲁棒性。</p>
<p><img src="Improving-Occlusion-and-Hard-Negative-Handling-for-Single-Stage-Pedestrian-Detectors-屏幕快照 2019-12-16 下午8.37.04.png" alt=""></p>
<p>这篇论文给我的感觉就是很复杂，trick的地方很多，分 part算score的做法也有太多类似的论文。另外论文报的点似乎没有和sota比较，在部分数据集上的结果与sota相比还是差距比较大的</p>
</div></div><a class="button-hover more" href="../../2019/12/15/Improving-Occlusion-and-Hard-Negative-Handling-for-Single-Stage-Pedestrian-Detectors/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/12/10/How-to-Write-Clean-Code/">How to Write Clean Code(Updating)</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-10</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Engineering/">Engineering</a></div></div><div class="post-content"><div class="main-content content"></div></div><a class="button-hover more" href="../../2019/12/10/How-to-Write-Clean-Code/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/12/10/Learning-Efficient-Single-stage-Pedestrian-Detectors-by-Asymptotic-Localization-Fitting/">Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-12</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf</a></p>
<p>ECCV2018行人检测的论文，在清一色的Faster RCNN系列文章中也算是一股清流了，论文用Single Shot的模型来做行人检测，也是为数不多把模型速度作为卖点和核心的论文。论文的整个核心可以理解为Cascade RCNN + RefineDet(ALF思想类似Cascade RCNN、One Stage的解决方案类似RefineDet，毕竟Cascade RCNN和RefineDet本身有些设计理念是很像的).</p>
<p><img src="Learning-Efficient-Single-stage-Pedestrian-Detectors-by-Asymptotic-Localization-Fitting-截屏2019-12-1123.53.08.png" alt=""></p>
<p>论文的核心概念是Asymptotic Localization Fitting(ALFNet), 上图示意图还是比较明显的，3个橙色feature maps是resnet/mobilenet的stage 3、4、5，绿色是直接在stage 5上接conv延伸出来的一层feature map，整体构成了类似FPN的逻辑。<br>在每个stage上都会通过CPB模块渐进式对框进行refine，这一步就和Cascade RCNN的逻辑比较像了，每个CPB都会对anchor进行一次refine，每次正负样本的IOU阈值都逐步提高，从而不断的提高框的回归精度，这里面有一个需要注意的地方就是anchor的生成只用了一个ratio: 0.41…就是CityPerson数据集的统计值，这还是有点hack数据集的意思的…</p>
</div></div><a class="button-hover more" href="../../2019/12/10/Learning-Efficient-Single-stage-Pedestrian-Detectors-by-Asymptotic-Localization-Fitting/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/12/08/Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd/">Repulsion Loss: Detecting Pedestrians in a Crowd</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL：<a href="https://zpascal.net/cvpr2018/Wang_Repulsion_Loss_Detecting_CVPR_2018_paper.pdf" target="_blank" rel="noopener">https://zpascal.net/cvpr2018/Wang_Repulsion_Loss_Detecting_CVPR_2018_paper.pdf</a></p>
<p>一篇解决行人检测遮挡场景的论文，切入点是loss，作者认为行人检测问题在遮挡场景一个比较显著的难点是boundingbox的<strong>shift</strong>问题，简言之就是由于多个人密集聚在一起的时候因为个体之间特征过于相似所以网络在预测定位的时候会产生偏移的现象，从而导致位置不够精确甚至会产生FP，论文的figure1给了一个比较简单的示意图(虚线红色框就是描述这个现象的dt框)：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-0923.54.13.png" alt=""></p>
<p>OK,那就承接上面这张图直接说一下这篇论文的核心内容，repulsion Loss:</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.03.37.png" alt=""></p>
<p>Repulsion Loss主要分成三个组成部分，L<sub>Attr</sub>, 就是一般的回归Loss比如L1、L2…之类的，毕竟为了refine定位框必要的监督信息还是需要的：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.06.10.png" alt=""></p>
<p>另一部分叫 L<sub>RepGT</sub>，这一项的目的是希望每一个proposal与相临近的gt框(不是当前proposal match的gt框)相远离，也就是对于第一张图，如果要预测紫色框，那么L<sub>RepGT</sub>就用来控制紫色框与蓝色框相远离从而缓解shift的问题，做法的话很直接，对于给定的proposal选择一个与之IoU最大的gt框然后计算Loss：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.14.24.png" alt=""></p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.14.41.png" alt=""></p>
<p>这里也引入了一个超参数来平滑loss：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.14.47.png" alt=""></p>
<p>Repulsion Loss的最后一项叫L<sub>RepBox</sub>, 其主要目的是希望对于任意match到不同gt的两个proposal之间需要尽可能的远离，然后形式上也比较类似了：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.19.12.png" alt=""></p>
<p>整体感觉这篇论文讲的点都还make sense但是都有点治标不治本</p>
</div></div><a class="button-hover more" href="../../2019/12/08/Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/12/08/Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs/">Occluded Pedestrian Detection Through Guided Attention in CNNs</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-08</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Occluded_Pedestrian_Detection_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Occluded_Pedestrian_Detection_CVPR_2018_paper.pdf</a></p>
<p>同样是解决遮挡场景的行人检测问题，同样又是attention的逻辑…但是论文中对于基于attention去做的insight还是比较有意思的。作者在做实验可视化的时候发现网络输出的feature map对于人体不同的部位分别有不同的几组channel会对其高响应，从而也就引出了论文为什么要对channel进行attention的潜在原因，论文给的示意图还是比较直接的：</p>
<p><img src="Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs-截屏2019-12-0823.07.25.png" alt=""></p>
<p>方法上还是万年不变的faster rcnn框架再辅助一个attention分支（论文中示意的attention net），attention分支会出一个fc，维度和roi pooling之后的feature map channel数保持一致：</p>
<p><img src="Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs-截屏2019-12-0823.06.03.png" alt=""></p>
<p>至于如何attention作者也提出了三个想法，第一个就是SE Net的做法，roi pooling之后的feature直接作为attention net的输入，参数的更新直接自学习（下图的第一个Self attention net），第二个做法是利用标注的可见框，做法比较trick，需要去统计数据集，作者统计了CityPerson数据集然后把行人框分成(1) fully visible; (2) upper body visi- ble; (3) left body visible; (4) right body visible四个pattern，然后attention net核心就是一个分类模型，在学习过程中一旦pattern被确认会用conv再抽一波特征（论文没有说清楚我猜是类似ROI Pooling去crop pattern对应部位的特征），示意图就是下图的第二个Visible-box attention net。第三个做法…emm感觉更trick了，作者直接拿来一个预训练好的skeleton模型，既然模型不同channel对不同part高响应那就干脆把高响应的heatmap作为guidance来指导模型训练，做法很直接，但是个人觉得这种做法最后去分析问题的时候是不是还得去优化skeleton模型……</p>
<p><img src="Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs-截屏2019-12-0823.06.54.png" alt=""></p>
<p>然后这篇论文就没什么可以介绍的了。</p>
</div></div><a class="button-hover more" href="../../2019/12/08/Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/12/08/Occlusion-aware-R-CNN-Detecting-Pedestrians-in-a-Crowd/">Occlusion-aware R-CNN - Detecting Pedestrians in a Crowd</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-08</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Shifeng_Zhang_Occlusion-aware_R-CNN_Detecting_ECCV_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/papers/Shifeng_Zhang_Occlusion-aware_R-CNN_Detecting_ECCV_2018_paper.pdf</a></p>
<p>这篇论文算是去年去参加ECCV2018 poster展台最火的一个了，论文同样是做遮挡场景的行人检测的，方法也是花样attention。</p>
<p>论文同样基于faster rcnn的框架来做行人检测，作者所提方法有两个核心，一个是<strong>Part Occlusion aware RoI Pooling Unit</strong>，作者把人的body分成5个part分别过ROI Pooling得到固定的输出大小，论文中是7x7，然后经过<strong>Occlusion process unit</strong>得到每个part的可见与否的置信度c，c再分别和对应的part feature相乘得到加权后的feature，最后连同body本身大part的feature通过element wise sum得到最后attention的feature：</p>
<p><img src="Occlusion-aware-R-CNN-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-0822.21.19.png" alt=""></p>
<p>论文所提方法的另外一个核心是<strong>Aggregation Loss</strong>，这玩意其实和<strong>Repulsion Loss</strong>这篇论文的理论算是很像的了，核心想法是希望同一个gt的proposals(anchors)之间需要尽可能的近:</p>
<p><img src="Occlusion-aware-R-CNN-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-0822.34.14.png" alt=""></p>
<p>{t<sub>1</sub><sup>*</sup>, t<sub>2</sub><sup>*</sup>…}是涉及多个proposal（anchor）的gt集合，集合长度为ρ，{Φ1, · · · , Φρ}是上述对应的gt相对应的anchors的集合，公式写的其实很直白不赘述。</p>
</div></div><a class="button-hover more" href="../../2019/12/08/Occlusion-aware-R-CNN-Detecting-Pedestrians-in-a-Crowd/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/12/08/Mask-Guided-Attention-Network-for-Occluded-Pedestrian-Detection/">Mask-Guided Attention Network for Occluded Pedestrian Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Attention/">Attention</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1901.06651.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.06651.pdf</a></p>
<p>行人检测的一篇论文，用attention的方式解遮挡问题(这一类方法的确很多…而且做法很类似…), 这篇论文能中ICCV2019给我的感觉还是很方的…</p>
<p><img src="Mask-Guided-Attention-Network-for-Occluded-Pedestrian-Detection-截屏2019-12-0822.08.06.png" alt=""></p>
<p>做法很简单，上图中的蓝色框就是标准的基于Faster RCNN框架的行人检测逻辑，额外加了一个论文提出的MGA模块（上图中的红色框），MGA是啥呢…其实就是<strong>可见区域</strong>的mask，作者将经过ROI的feature送入到MGA模块中，几层conv之后经过sigmoid得到HxWx1的mask，这个mask再和原来的feature相乘就得到更新后的feature，这个feature再去做fast rcnn那一套逻辑，然后这篇论文的主要内容就没了…看指标点还是比较高的…之前在做人脸检测的时候用过一摸一样的方法只是当时没有明显的涨点（也有可能因为当时不是拿来做遮挡的场景的？）</p>
<p><img src="Mask-Guided-Attention-Network-for-Occluded-Pedestrian-Detection-截屏2019-12-0822.13.04.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/12/08/Mask-Guided-Attention-Network-for-Occluded-Pedestrian-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/09/21/Robust-Convolutional-Neural-Network-Cascade-for-Facial-Land-mark-Localization-Exploiting-Training-Data-Augmentation/">Robust Convolutional Neural Network Cascade for Facial Land- mark Localization Exploiting Training Data Augmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Cascade/">Cascade</a></div></div><div class="post-content"><div class="main-content content"><p>2018年发表的一篇关于landmark localization的论文, 具体说一下论文的内容，论文的contribution分成两部分：cascade 的网络设计+ data augmentation</p>
<ol>
<li>Cascade网络结构：第一阶段网络根据检测器出的框crop出人脸回归出初始的landmark位置，这一部分网络通常比较大以保证第一阶段得到的landmark位置比较接近ground truth，第二阶段是一个component-wise的landmark refine网络，主要用来回归mouth、eye等形变比较常见的一些部位的landmark，最后结合第一阶段其他landmark的结果作为最终的输出。至于网络基本的结构是基于VGG的。</li>
</ol>
<p><img src="Robust-Convolutional-Neural-Network-Cascade-for-Facial-Land-mark-Localization-Exploiting-Training-Data-Augmentation-thumbnail_image002.png" alt=""></p>
<ol start="2">
<li>Data Augmentation：这一部分感觉也是比较常见的一些操作，论文主要阐述了平移的aug，范围是长宽的30%以内，加之random rotation、random Gaussian blurring、flip等，其实和目前postfilter的训练data aug是差不多的，论文强调了同样的aug在网络的第一阶段和第二阶段都应用了。<br>Data augmentation的示意：</li>
</ol>
<p><img src="Robust-Convolutional-Neural-Network-Cascade-for-Facial-Land-mark-Localization-Exploiting-Training-Data-Augmentation-thumbnail_image003.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/09/21/Robust-Convolutional-Neural-Network-Cascade-for-Facial-Land-mark-Localization-Exploiting-Training-Data-Augmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/09/11/Deep-Convolutional-Network-Cascade-for-Facial-Point-Detection/">Deep Convolutional Network Cascade for Facial Point Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Cascade/">Cascade</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_cvpr_2013/papers/Sun_Deep_Convolutional_Network_2013_CVPR_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2013/papers/Sun_Deep_Convolutional_Network_2013_CVPR_paper.pdf</a></p>
<p>关于cascade landmark localization的一篇论文，是一篇比较老的CVPR论文，论文主要提出cascade的网络结构来更准的定位landmark位置</p>
<ol>
<li><p>Cascade结构：论文提出了三级的cascade结构，每一级网络都可以理解为是component-wise的设计</p>
<p>a) 第一级网络：这一级网络分成三个网路分支，第一个网络分支以整张脸为输入，回归全部的5个landmark点；第二个网络分支以脸的上半部为输入，回归两个眼睛中心点和鼻尖的位置共3个landmark点；第三个网络分支以脸的下半部为输入，回归鼻尖和两个嘴角的位置共3个landmark点；每个landmark的最终位置为三个网络的输出的均值；</p>
<p>b) 第二级和第三级网络：这两级网络的作用是一样的，每一级都有多路网络分支，每一个网络分支都只回归一个点的位置，每一个landmark点有两个网络分支去回归，两者的均值作为最终的结果，因此第二级和第三级网络分别都有10个网络分支，他们的输入都是基于第一级网络的输出landmark，从对应的landmark位置以一定的大小比例crop出输入，第三级比第二级的size更小。这两级网络学习的是相对位置的offset。</p>
<p><img src="Deep-Convolutional-Network-Cascade-for-Facial-Point-Detection-thumbnail_image002.png" alt=""></p>
<p>这是第一级网络分支的基本结构：</p>
<p><img src="Deep-Convolutional-Network-Cascade-for-Facial-Point-Detection-thumbnail_image003.png" alt=""><br>最终landmark的位置通过计算得到：</p>
<p><img src="Deep-Convolutional-Network-Cascade-for-Facial-Point-Detection-thumbnail_image004.png" alt=""></p>
</li>
</ol>
</div></div><a class="button-hover more" href="../../2019/09/11/Deep-Convolutional-Network-Cascade-for-Facial-Point-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/08/03/Deep-Convolutional-Neural-Networks-with-Merge-and-Run-Mappings/">Deep Convolutional Neural Networks with Merge-and-Run Mappings</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://www.ijcai.org/proceedings/2018/0440.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2018/0440.pdf</a></p>
<p>MSRA wangjingdong组的研究工作，貌似虽然中了2018年的IJCAI但是实际上这篇论文应该挂出来蛮久了。<br>论文的主要内容也很直接，就是想训练更深、更宽的网络结构，并因此提出了Merge and Run模块：</p>
<p><img src="Deep-Convolutional-Neural-Networks-with-Merge-and-Run-Mappings-截屏2019-12-2123.28.36.png" alt=""></p>
<p>Merge-and-Run模块其实和Inception-Resnet的结构有一点类似，Inception-Resnet结构在模块内是一个parallel的结构，Merge-and-Run模块本身也是两个分支parallel连接，只是不同于前者输入直接和输出sum，后者是把两个分支输入的均值分别和两个分支的输出sum，求均值和sum的操作分别被称作“merge”和 “run”, 本身结构的设计也有利于深度网络的学习,<br><img src="Deep-Convolutional-Neural-Networks-with-Merge-and-Run-Mappings-thumbnail_image004.png" alt=""> = M是幂等矩阵和resnet的恒等变换类似有助于深度网络的训练:</p>
<p><img src="Deep-Convolutional-Neural-Networks-with-Merge-and-Run-Mappings-截屏2019-12-2123.30.00.png" alt=""></p>
<p>作者也论证了这样的结构在保证相同数目结构单元的情况下比resnet、inception-restnet这样的结构更浅、更宽，因此也适合训练很深的网络结构：</p>
<ol>
<li><p>更浅：假设每一个block（上图中的1组conv）有B个layer，三个结构总的block数分别是2L、L、L个，那么对于一般的resnet结构平均深度就为BL + 2，resnet-inception结构平均深度为2BL/3 + 2，而本文提出的merge-and-run结构平均深度就为BL/3+2，+2为上图输出的FC和输入的conv，至于平均深度的计算个人理解就是block数除以可能的path数。</p>
</li>
<li><p>更宽：对于merge-and-run模块宽度更宽的论证论文做了简单的两组论证，都比较直观，inception-resnet parallel的结构比普通的resnet有更多的分支，merge-and-run与inception-resnet的差别是inception-resnet最后的结果会融合在一起而merge-and-run 最后的结果还会是两个分支：</p>
</li>
</ol>
<p><img src="Deep-Convolutional-Neural-Networks-with-Merge-and-Run-Mappings-截屏2019-12-2123.32.04.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/08/03/Deep-Convolutional-Neural-Networks-with-Merge-and-Run-Mappings/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/07/16/AAR-CNNs-Auto-Adaptive-Regularized-Convolutional-Neural-Networks/">AAR-CNNs: Auto Adaptive Regularized Convolutional Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Regularization/">Regularization</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Attention/">Attention</a></div></div><div class="post-content"><div class="main-content content"><p>IJCAI2018一篇关于regularization的论文，论文的motivation是想在更细的维度做regularization，BN可以理解为batch维度的regularization，而本文提出的AAR-CNN则可以理解为channel维度和pixel维度的regularization，论文结合了SE-Net的相关内容，整篇看来其实可以理解为pixel（channel）维度的attention。</p>
<ol>
<li><p>AE-Net：Abstract Extent（AE）直译过来就是抽象程度的意思，作者认为CNN网络中不同的layer的抽象程度是不一样的，论文中也分别从pixel level和channel level来定义了AE。对于pixel level的AE，假设输入为(s1,s2,s3,s4), s1为mini-batch size，s2为channel，s3为h，s4为w，初始化AE的表达为(1,1,s3,s4),然后通过conv (1,s1,1,1) + dimshuffle + conv(1,s2,3,3)得到最后的输出v2(s1,s2,s3,s4),最后做pixel wise的tanh就得到最后的AE的表达了，channel level的做法类似通过两层FC得到：</p>
<p><img src="AAR-CNNs-Auto-Adaptive-Regularized-Convolutional-Neural-Networks-thumbnail_image002.png" alt=""></p>
</li>
<li><p>SE-Net：这一部分就是完全来自于之前读的Squeeze-and-Excitation Networks这篇论文，对于输入的每一个channel加attention：</p>
<p><img src="AAR-CNNs-Auto-Adaptive-Regularized-Convolutional-Neural-Networks-thumbnail_image003.png" alt=""></p>
</li>
</ol>
<p>所以整个pipeline就可以整合成如下的示意图，AE-net和SE-net的输出相乘就是最后regularization的表达，将其施加于输出之上产生的结果再和输出sum得到最后的输出：</p>
<p><img src="AAR-CNNs-Auto-Adaptive-Regularized-Convolutional-Neural-Networks-thumbnail_image004.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/07/16/AAR-CNNs-Auto-Adaptive-Regularized-Convolutional-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/07/08/Learning-non-maximum-suppression/">Learning non-maximum suppression</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/NMS/">NMS</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Hosang_Learning_Non-Maximum_Suppression_CVPR_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/papers/Hosang_Learning_Non-Maximum_Suppression_CVPR_2017_paper.pdf</a></p>
<p>这篇论文的主要研究内容从论文题目就可以比较直观地看出来，作者想利用神经网络本身来实现NMS的功能将网络实现完全的end2end。针对目前检测模型固有的特性（如region proposal），作者提出要实现这样的功能需要从两个角度来做，一是从loss的角度对同一个物体多次检测加以惩罚，二是利用object的近邻信息来辅助网络来学习是否对同一个物体进行了多次检测，整篇文章也是从这两个角度分析。</p>
<p><img src="Learning-non-maximum-suppression-thumbnail_image002.png" alt=""></p>
<ol>
<li><strong>Loss</strong>：本文中作者设计的Loss Function，有一些细节在论文中没有提及，可以参考CVPR2016的End-to-end people detection in crowded scenes论文中的Hungarian Loss，利用GT和DT作为图的节点，GT和DT之间的距离、两者是否overlap等信息作为边的权重构造一个完整的图，利用匈牙利算法计算最大（小）匹配（对应上图的matching），yi {-1,1}就是匹配的结果,代表第i个DT是否被匹配上，Si为DT新的score信息，对应上图中的new detection score：</li>
</ol>
<p><img src="Learning-non-maximum-suppression-thumbnail_image003.png" alt=""></p>
<ol start="2">
<li><strong>GossipNet</strong>：作者设计的一个网络结构来整合DT的近邻信息来辅助判断（上图的整个示例），GNet主要有多个Block组成（论文中用的是16个），每一个block的基本结构如下图，因为是要整合近邻的信息，那么首先第一步是对于一个DTi，对于IoU(DTi, DTj) &gt; 0.2时，认为DTj属于DTi的近邻。第二步是构造下图的pairwise context，这个又可以分成两块，一块是concat(DTi, DTj), 另一块是两个DT之间的固有信息比如IoU、L1 distance、L2 distance、置信度等等，所有这些信息构成一个vector作为含3层FC的MLP网络的输入，输出就是pairwise context的另一部分。最后pairwise context信息经过多层FC和Pooling作为下一个block的输入:</li>
</ol>
<p><img src="Learning-non-maximum-suppression-thumbnail_image004.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/07/08/Learning-non-maximum-suppression/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/07/01/Improving-Object-Detection-With-One-Line-of-Code/">Improving Object Detection With One Line of Code</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/NMS/">NMS</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1704.04503.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.04503.pdf</a><br>Source: <a href="https://github.com/bharatsingh430/soft-nms" target="_blank" rel="noopener">https://github.com/bharatsingh430/soft-nms</a></p>
<p>同样是一篇关于改进NMS的论文，不同于Learning non-maximum suppression，这篇论文主要分析了传统NMS算法的弊端，对其处理的逻辑进行了优化，并且号称只修改One Line of Code。传统的NMS本质是一个贪心的算法，首先选择一个confidence最高的bbox，然后剔除掉和它IoU比较大的bbox，循环这个步骤得到最后的bbox集合，那么它很明显的一个弊端就是下图的这种情况，利用传统的NMS被绿色框标记的有可能被抑制从而产生一个miss(IoU &gt; th)。如果把IoU阈值提高又有可能增加FP，因此论文针对目前的NMS弊端提出了Soft-NMS算法。Soft-nms的整个算法伪代码在下图的右侧，在原有NMS算法的基础上只进行了小幅度的改动，从复杂度上面看也几乎没有变化。</p>
<p> <img src="Improving-Object-Detection-With-One-Line-of-Code-屏幕快照 2019-12-19 下午9.47.59.png" alt=""></p>
<p> Soft-NMS：对于传统的NMS算法，对bbox的抑制可以理解为将bbox的score置为0，将bbox从候选框中完全剔除掉，那么作者为了缓解上图那样的问题提出soft-nms方法，将被抑制bbox的score设置为一个和IoU值相关的衰减函数，具体如下，传统的NMS算法则可以理解成soft-nms的特例，但考虑到上述的算分逻辑是不连续的，分数的突然抖动会对bbox的序列影响较大，因此作者follow相同的抑制逻辑，利用第二个指数形式的公式对bbox分数进行转化，同样满足离当前bbox越远的bbox分数影响越小，反之越大。</p>
<p><img src="Improving-Object-Detection-With-One-Line-of-Code-屏幕快照 2019-12-19 下午9.49.33.png" alt=""></p>
<p>Soft NMS直观的理解：对于传统的NMS，两个Object的框如果IoU比较大的话其中一个框有可能会被干掉，那么Soft NMS为了缓解这个问题会把其中一个分数较低的Object框分降低，这样它匹配的优先级会降低，在多轮迭代之后有可能会被保留下来，从而可以整体提高recall.</p>
<p>当然了SoftNMS本身的做法有点治标不治本，首先框没有明确的ID信息，这种做法同样有可能保留FP，另外对于这种Overlap也没有本质的解决</p>
</div></div><a class="button-hover more" href="../../2019/07/01/Improving-Object-Detection-With-One-Line-of-Code/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/06/28/Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm/">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://wywu.github.io/projects/LAB/LAB.html" target="_blank" rel="noopener">https://wywu.github.io/projects/LAB/LAB.html</a><br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image002.png" alt=""><br>本论文提出了一种基于边界信息的landmark定位方法，通过回归landmark 构成的boundary可以一定程度上解决遮挡等一些问题，boundary的一般性也得以融合多个不同的landmark标注数据集进行一同训练。此外论文也贡献了包含1w张图片的数据集WFLW。</p>
<p>论文所提的整个方法主要分成三个部分：</p>
<ol>
<li>Boundary heatmap estimator：这一部分是一个Hourglass结构，用来初步生成boundary的heatmap，需要说明的是，为了增强模型在有遮挡情况下的表现，论文引入了message passing layer来传递不同boundary之间的信息和同一个boundary不同stack之间的信息。这一部分的细节在这篇论文的补充材料里面写的比较清楚，不管是inter-level还是intra-level信息的传递都是不同feature 之间的特征融合（conv + entry-wise sum ），intra-level是不同的stack之间，inter-level是k个（k代表boundary的个数）boundary heatmap之间。<br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image003.png" alt=""></li>
<li>Boundary-aware landmarks regressor ：该模块主要用来回归heatmap：<br>a. Boundary由区域的landmark插值生成<br>b. Input image fusion： I为输入图片，Mi为第i个heatmap，乘号为element-wise dot product，加号为channel-wise concatenation<br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image004.png" alt=""><br>c.  Feature map fusion：F为feature map M为heatmap，其他和上面类似<br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image005.png" alt=""><br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image006.png" alt=""></li>
<li>Boundary effectiveness discriminator：这一部分主要引入对抗学习的思想，第一部分Boundary heatmap estimator生成的heatmap有效性被定义为：<br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image007.png" alt=""><br>M为生成的heatmap，S为对应的landmark集合，Dist为gt对应的distance matric map ，θ 和δ 分别是距离和概率的阈值，整个公式需要保证比较好的heatmap对应的landmark要尽可能多的离gt近<br>Discriminator的Loss：<br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image008.png" alt=""><br>Adversarial Loss：<br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image009.png" alt=""></li>
<li>此外论文也提供了一个WFLW数据集，包含1w张图片<br>和目前不少的landmark localization方法类似，论文所提方法也是基于区域的想法去解决定位的问题，只是通过一些插值的操作做了比较细致的处理，感觉对于遮挡等一些问题会比较有帮助。</li>
</ol>
</div></div><a class="button-hover more" href="../../2019/06/28/Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/06/28/Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection/">Quantization Mimic: Towards Very Tiny CNN for Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Mimick/">Mimick</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1805.02152.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1805.02152.pdf</a><br>关于模型压缩的论文，论文致力于研究更加小型化的模型，论文定义“Very Tiny”为压缩模型的每一层channel数是原来模型的1/16或更小.从论文的标题也可以看出，论文提出的方法是结合目前比较常见的模型小型化方式：quantization 和 mimic。具体的模型结构在论文中给出了比较通俗的图例：<br><img src="Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection-image002.png" alt=""></p>
<p>具体：</p>
<ol>
<li>训练标准的大模型，论文中实验了R-FCN和Faster R-CNN两个网络模型。然后利用论文定义的量化函数Q将大模型转化成量化模型。<br><img src="Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection-image003.png" alt=""><br>公式中的三个参数α、β、γ取自D序列<img src="Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection-image004.png" alt="">, 其中s就是均匀量化的步长。函数Q在论文中说的比较不清楚，实际上参考论文中给出的示意图会比较好理解，如果在图例上以（0，0）为原点标上x, y轴看上去会比较直观。取s=1，α = 0、β = 1、γ = 2，那么对于0.5&lt;x&lt;1.5, Q(x) = 1， 然后再取α = 1、β = 2、γ = 3…依次可以得到论文中的图例。<br><img src="Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection-image005.png" alt=""></li>
<li>至于论文中的mimic部分，沿用之前曾阅读的一篇CVPR2017的论文《Mimicking Very Efficient Network for Object Detection》，利用Feature Map Mimic Learning来训练小模型，整个模型的损失函数如下：<br><img src="Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection-屏幕快照 2019-06-28 下午6.02.41.png" alt=""><br>Lm为mimic loss，其实就是两个模型feature map的L2 loss，L中的前两个Loss分别是RPN的cls和reg loss，后两个则是R-FCN或者Faster R-CNN cls和reg loss。</li>
<li>为了小模型能更好的从量化后的大模型中学习，论文对小模型也进行了量化，从模型的结构示意图中可以直观的看到。与其他的模型小型化方法在Wider Face数据集上的比较，可以看到Quantization Mimic效果还是比较明显的。<br><img src="Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection-image007.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2019/06/28/Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/06/24/Mimicking-Very-Efficient-Network-for-Object-Detection/">Mimicking Very Efficient Network for Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Mimick/">Mimick</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf</a><br>这篇论文可以理解为关于模型压缩的论文，论文采取大模型来训练小模型的方法，作者claim这是第一次将mimic方法运用到物体检测领域。<br>之前mimic方法通常用在分类任务中，Mimic方法的出发点是希望大模型学习到的特征可以传递给小模型，这篇论文主要有如下的contribution：</p>
<ol>
<li>Feature Map Mimic Learning：不同于分类任务中从大模型的soft targets或者logits来学习小模型，结合物体检测这个具体任务，论文提出大模型的feature map来监督训练小模型，但是CNN网络的最后一层feature map都是一些高维特征，对于一些小物体的表现会比较弱，因此论文以proposal为单位来监督训练，训练目标为：<br><img src="Mimicking-Very-Efficient-Network-for-Object-Detection-屏幕快照 2019-06-28 下午5.52.58.png" alt=""><br>L(w)为最终的Loss， Lm(W)为大小模型feature map的L2 Loss， Lgt(W)为RPN中cls和reg的Loss，ui是从大模型feature map采样得到的特征，vi是从小模型中采样得到的特征,  r为回归函数负责将vi映射到ui的维度上。后期作者多Lm（W）加上norm进行优化：<br><img src="Mimicking-Very-Efficient-Network-for-Object-Detection-image003.png" alt=""></li>
<li>具体模型结构分为两个阶段：<ol>
<li>第一阶段可以理解为对RPN的训练：大模型为预训练好的Faster RCNN或者R-FCN，小模型的最后是一个RPN网络，同一张训练图片同时经过大模型和小模型得到对应的feature maps，利用小模型RPN产生的proposal进行上面提到的feature map mimic learning进行训练。</li>
<li>第二阶段可以理解为对Faster RCNN或者R-FCN的训练，在这一部分在会加入分类任务中的logits mimic learning利用大模型的logits来监督学习。<br><img src="Mimicking-Very-Efficient-Network-for-Object-Detection-image004.png" alt=""><br>除此之外，论文还介绍了在小模型中加入deconv层来解决输入图片较小的情况：<br><img src="Mimicking-Very-Efficient-Network-for-Object-Detection-image005.png" alt=""></li>
</ol>
</li>
</ol>
</div></div><a class="button-hover more" href="../../2019/06/24/Mimicking-Very-Efficient-Network-for-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/06/20/Tone-Mapping/">Tone Mapping</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Basic/">Basic</a></div></div><div class="post-content"><div class="main-content content"><p>Tone Mapping 可以简单理解为将HDR（High Dynamic Range） 的图像映射到LDR（Low Dynamic Range）的图像中，DR的定义可以理解为同一张图片中所有像素点最大亮度和最小亮度的log的差值，RGB场景下亮度的定义为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y = 0.2126R + 0.7152G + 0.0722B</span><br></pre></td></tr></table></figure></p>
<p>那么很显然在LDR下DR的取值只能到2.4，而在真实场景中DR的值甚至可以达到9以上.<br>针对Tone Mapping的操作也衍生出了很多优化的方法，比如<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.352.2669&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">这篇论文</a><br>这篇论文的应用场景是视频的压缩，那么针对这个场景常用的Tone Mapping逻辑一般如下， l为输入的HDR原图，θ为Tone Mapping操作的参数，v为HDR图经过Tone Mapping之后得到的LDR图, v~ 为v经过视频压缩、解压缩之后得到的LDR图，l~为v~经过Tone Mapping逆过程得到的伪HDR的图，所以将l和l～的diff作为整个优化过程的object function来优化θ的参数即可，比如来优化这两个值的MSE：<br><img src="Tone-Mapping-屏幕快照 2019-06-21 上午10.28.40.png" alt=""><br>而本论文中提出的方法则是完全简化了这个过程，所以整个论文的核心就落到了distortion model的构建中：<br><img src="Tone-Mapping-屏幕快照 2019-06-21 上午10.51.17.png" alt=""><br>求解distortion model的核心又是tone curve，类似直方图均衡的方法，为和人类视觉系统的感官逻辑保持一致，tone curve histogram的计算是基于HDR图的log10(亮度)来计算的：<br><img src="Tone-Mapping-屏幕快照 2019-06-21 上午10.53.27.png" alt=""><br>整个curve是分段的线性映射，每一段的宽度都为δ （论文中取的0.1），横坐标可以理解为HDR域的情况，纵坐标可以理解为映射到LDR域的情况，每一段curve的计算，s<sub>k</sub>为斜率：<br><img src="Tone-Mapping-屏幕快照 2019-06-21 上午10.54.40.png" alt=""><br><img src="Tone-Mapping-屏幕快照 2019-06-21 上午11.08.03.png" alt=""><br>因为curve是分段线性的，所以tone mapping的逆向公式也就很好计算了：<br><img src="Tone-Mapping-屏幕快照 2019-06-21 上午11.08.26.png" alt=""><br>单看论文中提出的简化的tone mapping计算方法和原始方法的最主要差别就是把压缩损失那一部分给去掉了，而实际作者是用了一个分布函数来简化了这个操作p<sub>C</sub>就是压缩损失的分布函数,p<sub>L</sub>实际就是histogram计算出来的比率了：<br><img src="Tone-Mapping-屏幕快照 2019-06-21 上午11.09.13.png" alt=""><br>作者通过层层推导之后把上述的公式简化到：<br><img src="Tone-Mapping-屏幕快照 2019-06-21 上午11.12.04.png" alt=""><br><img src="Tone-Mapping-屏幕快照 2019-06-21 上午11.12.39.png" alt=""><br>然后利用Karush-Kuhn-Tucker (KKT)方法来优化计算得到s数组的取值，这样就可以直接求解整个tone mapping的逻辑了，至于具体的kkt方法有兴趣的同学可以自行去查找了或者直接refer论文去看细节。<br>这个方法最后的效果还是很惊艳的。</p>
</div></div><a class="button-hover more" href="../../2019/06/20/Tone-Mapping/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/06/15/Lightweight-Real-time-Makeup-Try-on-in-Mobile-Browsers-with-Tiny-CNN-Models-for-Facial-Tracking/">Lightweight Real-time Makeup Try-on in Mobile Browsers with Tiny CNN Models for Facial Tracking</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1906.02260" target="_blank" rel="noopener">https://arxiv.org/abs/1906.02260</a><br>一篇主要做轻量级Landmark应用的论文，整体novelty有限，主要是在MobileNetV2的基础上实现了一个轻量级的Facial Landmark模型，在iPhone XR上可以实现20ms的inference速度</p>
<p>论文中所用的模型结构整体是一个Two Stage的逻辑， 出发点其实和之前看的一篇landmark machine论文很像，第一阶段出一个比较糙的heatmap来标识landmark点大概的位置，第二阶段再基于第一阶段的结果crop出小的patch继续refine：<br><img src="Lightweight-Real-time-Makeup-Try-on-in-Mobile-Browsers-with-Tiny-CNN-Models-for-Facial-Tracking-屏幕快照 2019-06-14 下午12.55.35.png" alt=""></p>
<p>几个需要明确的点：</p>
<ol>
<li>模型的主体主要参考MobileNetV2，大量使用Inverted Residual Block</li>
<li>ROI Crop主要是参考Mask RCNN用ROI Align取代ROI Pooling从而可以把不同的crop出来的patch concat到一起</li>
<li>最后通过一组Group Conv得到第二阶段相对于第一阶段的offset heatmap，两个阶段最后的输出加和就是最后的结果了</li>
</ol>
<p>另外需要提及的就是Loss的计算，论文没有用标注的heatmap之间计算loss的方式，而是根据每一个heatmap上的分布计算出具体的(x,y)值，最后直接监督最后的坐标：<br><img src="Lightweight-Real-time-Makeup-Try-on-in-Mobile-Browsers-with-Tiny-CNN-Models-for-Facial-Tracking-屏幕快照 2019-06-20 上午11.26.39.png" alt=""><br>结果方面：<br>下表算是简单的ablation study：<br><img src="Lightweight-Real-time-Makeup-Try-on-in-Mobile-Browsers-with-Tiny-CNN-Models-for-Facial-Tracking-屏幕快照 2019-06-20 上午11.27.00.png" alt=""><br>下表是和LAB的比较，整体还是有差距的：<br><img src="Lightweight-Real-time-Makeup-Try-on-in-Mobile-Browsers-with-Tiny-CNN-Models-for-Facial-Tracking-屏幕快照 2019-06-20 上午11.27.27.png" alt=""><br>最后是速度的测试：<br><img src="Lightweight-Real-time-Makeup-Try-on-in-Mobile-Browsers-with-Tiny-CNN-Models-for-Facial-Tracking-屏幕快照 2019-06-20 上午11.27.49.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/06/15/Lightweight-Real-time-Makeup-Try-on-in-Mobile-Browsers-with-Tiny-CNN-Models-for-Facial-Tracking/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/06/10/Improving-Landmark-Localization-with-Semi-Supervised-Learning/">Improving Landmark Localization with Semi-Supervised Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Semi-Supervised/">Semi-Supervised</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1709.01591?context=cs" target="_blank" rel="noopener">https://arxiv.org/abs/1709.01591?context=cs</a><br>这篇论文是关于landmark检测的，作者认为目前公开的数据集中标注landmark终究量比较少，但是标注属性（比如分类）的数据集实际上有很多，因此本论文提出半监督的神经网络模型结合标注landmark的数据集和标注属性的数据集来提高landmark定位的准确性。</p>
<p><img src="Improving-Landmark-Localization-with-Semi-Supervised-Learning-image002.png" alt=""><br>上面这张图基本涵盖了这篇论文的全部内容，论文所提方法主要分为三个部分：</p>
<ol>
<li>利用标注的landmark数据集来训练CNN模型（图中的S代表S个landmark标注样本）</li>
<li>Sequential Multi-Tasking：对应上图的第二个示例，就是利用标注属性的数据集来辅助landmark位置的学习，整个模型结构是一个串行的网络结构，下图是更加细致的结构表示，网络的前半段是一个标准的CNN网络，最后的一层feature map在经过soft-argmax之后输出预测的landmark位置，2xn个坐标值又会作为后半段网络的输入，后半段网络整体是一个MLP网络，用来预测Image的属性（图中的M代表M个属性标注样本）；<br><img src="Improving-Landmark-Localization-with-Semi-Supervised-Learning-image003.png" alt=""></li>
<li>Equivariant Landmark Transformation（ELT）：这一部分对应上图的第三个示例，是一个无监督的网络结构。这一部分主要是增强网络对图片各种旋转变换的鲁棒性，网路结构的设计出发点是，变换矩阵T作用于图片I产生的图像I’在经过网络后得到的预测landmark L’应该和图片I经过网络得到的landmark L经过T作用后的landmark 保持一致，因此本质是一个无监督的处理过程。</li>
<li>那么综合上面三部分，整个模型的loss就可以被定义为以下这个公式，D为image和对应attribute组成的pair list，K为landmark数量， ˜Lk, Lk (I ) and S分别是landmark GT、landmark预测值、标注landmark的样本量，公式的前两部分分别上图的第二、第三部分的网络的loss，第三部分是上图第一部分网络的loss：<br><img src="Improving-Landmark-Localization-with-Semi-Supervised-Learning-屏幕快照 2019-06-25 下午2.05.21.png" alt=""><br>论文在6个不同的数据集上做了对比实验，下图是在人脸landmark数据集MultiPIE上做的对比实验：<br><img src="Improving-Landmark-Localization-with-Semi-Supervised-Learning-image005.png" alt=""><br><img src="Improving-Landmark-Localization-with-Semi-Supervised-Learning-屏幕快照 2019-06-25 下午2.06.10.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2019/06/10/Improving-Landmark-Localization-with-Semi-Supervised-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/06/05/Seeing-Small-Faces-from-Robust-Anchor’s-Perspective/">Seeing Small Faces from Robust Anchor’s Perspective</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1802.09058.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1802.09058.pdf</a><br>这篇论文是关于anchor的设计，论文提出了一些anchor设计的策略来检测小脸。下图(a)是作者统计的传统基于anchor的模型在不同人脸大小下的recall，下图(b)则是作者将所有的人脸按大小分组，然后计算每一个组里每张人脸与anchor的最大IoU，对group中所有的max IoU取均值就是图(b)的average IoU。通过统计分析作者认为之所以对于小脸recall比较低的情况是因为小脸和初始化的anchor IoU较小，因此论文提出EMO Score来评估gt和anchor之间的联系并提出了一些anchor设计的策略。</p>
<p><img src="Seeing-Small-Faces-from-Robust-Anchor’s-Perspective-image002.png" alt=""></p>
<ol>
<li>Expected Max Overlapping Scores（EMO）：（x，y）是人脸中心的坐标，H，W分别是图片的高宽，p（x , y）则是概率密度函数，后半部分则是max IoU的计算，EMO描述的是一个anchor可以match到一个face的期望。<br><img src="Seeing-Small-Faces-from-Robust-Anchor’s-Perspective-image003.png" alt=""><br>作者以下图为例，假设人脸(中心为x)和左上角的anchor（中心为 +）拥有最大的IoU，那么人脸中心的取值范围就是图示的x’、y’小矩形，假设anchor中心的stride为Sa那么x’= y’= Sa / 2， 取anchor大小为lxl所以EMO:<br><img src="Seeing-Small-Faces-from-Robust-Anchor’s-Perspective-image004.png" alt=""><br>右下图则给出了公式中变量Sa、l的曲线图，l越大EMO越大、Sa越小EMO越大<br><img src="Seeing-Small-Faces-from-Robust-Anchor’s-Perspective-image005.png" alt=""><br><img src="Seeing-Small-Faces-from-Robust-Anchor’s-Perspective-image006.png" alt=""><br>根据EMO的分析，论文提出了一些anchor设计的策略：</li>
<li>Stride Reduction with Enlarged Feature Maps：减小anchor 的stride Sa和增大feature map的scale是等价的，因为它们与原图的关系是一致的。论文给出了三种增大feature map scale的网络结构，分别是Bilinear Upsampling、Bilinear Upsampling with Skip Connetction以及空洞卷积，upsampling的实现则是利用deconv layer。<br><img src="Seeing-Small-Faces-from-Robust-Anchor’s-Perspective-image007.png" alt=""></li>
<li>Extra Shifted Anchors：通过增加辅助的anchor来降低Sa，下图(a)是目前的基本anchor设计，图(b)是在原来对角anchor的中心加入辅助anchor，从而得到Sa ^2 = Sf ^2 / 2,图(c)则是在b的基础上在水平、竖直两个anchor的中点再加入两组辅助anchor将Sa降到原来的一半。<br><img src="Seeing-Small-Faces-from-Robust-Anchor’s-Perspective-image008.png" alt=""><br>最后的效果：<br><img src="Seeing-Small-Faces-from-Robust-Anchor’s-Perspective-image009.png" alt=""></li>
<li>Face Shift Jittering：在训练过程中每一次迭代都随机移动图片中的人脸（整张图片平移，对应的脸也相应平移），以此来增加某一些小脸和anchor的IoU。</li>
<li>Hard Face Compensation：max IoU始终低于阈值的人脸被称为hard faces，对于每一个hard face则按照IoU从大到小依次取前N个anchor作为positive（论文中通过实验取N为5）。<br>最后的实验效果：<br><img src="Seeing-Small-Faces-from-Robust-Anchor’s-Perspective-image010.png" alt=""><br>在不同图片大小上的表现：<br><img src="Seeing-Small-Faces-from-Robust-Anchor’s-Perspective-image011.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2019/06/05/Seeing-Small-Faces-from-Robust-Anchor’s-Perspective/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/06/01/Style-Aggregated-Network-for-Facial-Landmark-Detection/">Style Aggregated Network for Facial Landmark Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/GAN/">GAN</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Style_Aggregated_Network_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Style_Aggregated_Network_CVPR_2018_paper.pdf</a><br>这篇论文主要想解决的问题是不同的图片风格对landmark定位的影响，比如论文中给出的例子对于同一张图片的不同风格（原图、灰度图、以及加入光照的图片）通过嘴部特写可以看到明显的差别。因此论文提出 Style-Aggregated Network (SAN) 整合不同的图片风格来更好的检测人脸lmk.<br><img src="Style-Aggregated-Network-for-Facial-Landmark-Detection-image002.png" alt=""></p>
<p>SAN结构可以分成两个部分：</p>
<ol>
<li>Style-Aggregated Face Generation Module：这一部分论文主要利用CycleGAN来进行Style transfer主要做法是对于给定的一个数据集，利用PhotoShop将其转化成Light、Gray、Sketch三种风格的图片，加上原来的数据集总共就四类风格的数据集，这四类的数据集再作为resnet-152的输入训练出一个4分类的分类器，网络最后global average pooling的输出被视为风格特征表示，因此原数据集的风格特征在经过聚类之后就可以得到图片风格的label，这是论文对没有标注的数据集进行标注的方式，最后训练CycleGAN并将不同的风格图片相融合得到最后的style-aggregated人脸。<br><img src="Style-Aggregated-Network-for-Facial-Landmark-Detection-image003.png" alt=""></li>
<li>Facial Landmark Prediction Module：这个模块主要是标准的CNN网络结构，论文中的结构示意图写的很清楚，Prediction module接受原图和上一步产生的style-aggregated图作为输入，在经过一段CNN网络提取特征之后以cascade的形式经过三个阶段（FC，Fully-Convolution）不断的refine landmark位置信息，由于模块里加入了Pooling层，因此最后的size是小于原图的所以最后的输出是通过 bicubic interpolation插值得到所有的位置信息。<br><img src="Style-Aggregated-Network-for-Facial-Landmark-Detection-image004.png" alt=""><br>实验对比，可以发现效果还比较明显：<br><img src="Style-Aggregated-Network-for-Facial-Landmark-Detection-image005.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2019/06/01/Style-Aggregated-Network-for-Facial-Landmark-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/05/25/Deep-Regionlets-for-Object-Detection/">Deep Regionlets for Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://arxiv.org/abs/1712.02408" target="_blank" rel="noopener">http://arxiv.org/abs/1712.02408</a><br>论文主要将regionlet的概念引入到CNN网络结构中提出了一个新的物体检测模型，检测模型主要分为两个部分：</p>
<ol>
<li>Region Selection Network：这一部分主要从RPN网络生成的bounding box中生成一些区域（Region），Region的生成借助STN网络（三层FC，6维输出）学习到的仿射变换矩阵来实现，因此这一部分输出的Region形状不一定是矩形，初始化时Region是整个boudding box的均分小矩形；</li>
<li>Deep Regionlet Learning： 这一部分主要用来生成Regionlet以及对应的特征表示，Regionlet同样通过学习仿射变换矩阵从Region中生成，只是Regionlet最终的特征还借助于论文提出的Gating Network（多层FC） 来学习Rgionlet每一个位置的权重，最终两者的乘积会作为最后的输出特征。网络最后会接入Pooling层来融合特征。<br><img src="Deep-Regionlets-for-Object-Detection-image002.png" alt=""><br><img src="Deep-Regionlets-for-Object-Detection-image003.png" alt=""><br>在coco、voc上的实验结果：<br><img src="Deep-Regionlets-for-Object-Detection-image004.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2019/05/25/Deep-Regionlets-for-Object-Detection/#more">阅读全文</a></div></div><div id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="../../"><i class="fas fa-angle-left"></i></a><a class="page-number" href="../../">1</a><span class="page-number current">2</span><a class="page-number" href="../3/">3</a><span class="space">&hellip;</span><a class="page-number" href="../5/">5</a><a class="extend next" rel="next" href="../3/"><i class="fas fa-angle-right"></i></a></div></div></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fas fa-user"></i></span><span id="busuanzi_value_site_uv"></span><span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fas fa-eye"></i></span><span id="busuanzi_value_site_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Out of Memory</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/copy.js"></script><!--script(src=url)--></body></html>