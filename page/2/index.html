<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Live and Learn"><meta name="keywords" content=""><meta name="author" content="Out of Memory,undefined"><meta name="copyright" content="Out of Memory"><title>Live and Learn【Out of Memory】</title><link rel="stylesheet" href="../../css/fan.css"><link rel="stylesheet" href="../../css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="../../favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="../../js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="author-info"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Out of Memory</div><div class="author-info-description">Live and Learn</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/libanghuai" target="_blank">GitHub<i class="icon-dot bg-color4"></i></a><a class="links-button button-hover" href="mailto:libanghuai@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color8"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1185719433&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color9"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="../../archives"><span class="pull-top">日志</span><span class="pull-bottom">94</span></a><a class="author-info-articles-tags article-meta" href="../../tags"><span class="pull-top">标签</span><span class="pull-bottom">33</span></a><a class="author-info-articles-categories article-meta" href="../../categories"><span class="pull-top">分类</span><span class="pull-bottom">2</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Out of Memory</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><div id="recent-posts"><!-- each post in page.posts.sort('date', -1).limit(10).toArray()--><!-- config中配置按照什么排序--><div class="recent-post-item"><a class="post-title" href="../../2019/05/25/Deep-Regionlets-for-Object-Detection/">Deep Regionlets for Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://arxiv.org/abs/1712.02408" target="_blank" rel="noopener">http://arxiv.org/abs/1712.02408</a><br>论文主要将regionlet的概念引入到CNN网络结构中提出了一个新的物体检测模型，检测模型主要分为两个部分：</p>
<ol>
<li>Region Selection Network：这一部分主要从RPN网络生成的bounding box中生成一些区域（Region），Region的生成借助STN网络（三层FC，6维输出）学习到的仿射变换矩阵来实现，因此这一部分输出的Region形状不一定是矩形，初始化时Region是整个boudding box的均分小矩形；</li>
<li>Deep Regionlet Learning： 这一部分主要用来生成Regionlet以及对应的特征表示，Regionlet同样通过学习仿射变换矩阵从Region中生成，只是Regionlet最终的特征还借助于论文提出的Gating Network（多层FC） 来学习Rgionlet每一个位置的权重，最终两者的乘积会作为最后的输出特征。网络最后会接入Pooling层来融合特征。<br><img src="Deep-Regionlets-for-Object-Detection-image002.png" alt=""><br><img src="Deep-Regionlets-for-Object-Detection-image003.png" alt=""><br>在coco、voc上的实验结果：<br><img src="Deep-Regionlets-for-Object-Detection-image004.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2019/05/25/Deep-Regionlets-for-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/05/21/Regionlets-for-Generic-Object-Detection/">Regionlets for Generic Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://www.xiaoyumu.com/s/PDF/Regionlets.pdf" target="_blank" rel="noopener">http://www.xiaoyumu.com/s/PDF/Regionlets.pdf</a><br>这是一个传统的用于物体检测的方法，论文的主要贡献是提出了regionlet的概念以及基于regionlet的物体检测方法论文定义物体检测中有三个范围概念：Bounding Box、Region、Regionlet，Bounding Box就是目标候选框，Region是用于Bounding Box的特征提取，位于Bounding Box内，作者认为Region的粒度过大不足以表示局部的特征，因此在Region内部提出更小的范围Regionlet：<br><img src="Regionlets-for-Generic-Object-Detection-image002.png" alt=""></p>
<p>基于Regionlet的检测模型分为两个部分：</p>
<ol>
<li>提取每一个Regionlet的特征：这一步通常用HOG、LBP等特征来表示；</li>
<li>融合每一个Regionlet的特征：对于Regionlet r，通过第一步我们可以得到他的特征T(r)，对于Region R利用下面的公式可以得到R具体的特征表示，算法从regionlet特征中选择一个一维特征（行或列），并从中选择特征最强的那个作为R的一个一维特征，具体的选择方式则通过boosting 模型来学习，文中采用RealBoost最后利用从regionlet中抽取到的特征来训练boosting分类器来选择最合适的bounding box。<br><img src="Regionlets-for-Generic-Object-Detection-image003.png" alt=""><br><img src="Regionlets-for-Generic-Object-Detection-image004.png" alt=""><br>具体的实验结果，方法提出的比较早所以实验结果可能并没有实际的参考价值：<br><img src="Regionlets-for-Generic-Object-Detection-image005.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2019/05/21/Regionlets-for-Generic-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/05/17/Object-Detection-in-Video-with-Spatiotemporal-Sampling-Networks/">Object Detection in Video with Spatiotemporal Sampling Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/VID/">VID</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1803.05549.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1803.05549.pdf</a><br>这是ECCV2018 关于物体检测的一篇论文,论文主要提出一种时空采样网络STSN来提高视频中的物体检测效果。论文出发点还是整合多帧的信息来提高当前帧的检测效果，论文提出的STSN结构和FGFA比较类似，基本都会涉及到backbone网络提取特征、特征聚合等操作.</p>
<p>论文主要的贡献在于Spatiotemporal Feature Sampling，对于给定的帧I以及临近帧的范围K，I分别和K个临近帧形成pair作为STSN的输入，输入的两帧在经过Defornable CNN（基于ResNet-101）之后将输出concat到一起作为Spatiotemporal Feature Sampling 的输入，Spatiotemporal Feature Sampling部分同样利用Deformable CNN得到最后的offset field再结合临近帧的feature map经过一层deformable convolutional 得到最后的采样特征，所有pair采样之后得到的特征经过特征聚合作为detector的输入进行物体检测。STSN特征聚合部分和FGFA保持一致，大体流程论文中有比较简洁的示意。<br><img src="Object-Detection-in-Video-with-Spatiotemporal-Sampling-Networks-image001.png" alt=""><br>量化结果：<br><img src="Object-Detection-in-Video-with-Spatiotemporal-Sampling-Networks-image002.png" alt=""><br>效果图：<br><img src="Object-Detection-in-Video-with-Spatiotemporal-Sampling-Networks-image003.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/05/17/Object-Detection-in-Video-with-Spatiotemporal-Sampling-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/05/12/Flow-Guided-Feature-Aggregation-for-Video-Object-Detection/">Flow-Guided Feature Aggregation for Video Object Detection </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/VID/">VID</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1703.10025.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.10025.pdf</a><br>这篇论文是MSRA daijifeng组的研究工作，主要提出了一种神经网络结构来进行视频中的物体识别，视频中的物体检测典型的特征就是有些帧的物体因为运动模糊、遮挡、奇怪的pose导致难以检测，但是这个帧的附近帧中可能物体是处于一个正常的状态，因此论文考虑通过整合多帧信息来提高物体检测的效果，从而提出了FGFA (flow-guided feature aggregation) 网络。</p>
<p>下面这张图是简单的case，可以比较直接的描述论文想要解决的问题：<br><img src="Flow-Guided-Feature-Aggregation-for-Video-Object-Detection-屏幕快照 2019-06-25 下午1.14.23.png" alt=""><br>FGFA整个模型可以分成四个部分：</p>
<ol>
<li>Flow network： 这一部分主要利用Flying Chairs Dataset预训练的FlowNet;</li>
<li>Feature network：这一部分主要利用ResNet-50、ResNet101和Deformable CNN，在基础上对模型结构做了一定的修改去掉了最后的average pooling、fc以及一些其他细节操作;</li>
<li>Embedding network：这一部分包含三个卷积层：1x1x512、3x3x512、1x1x2048;</li>
<li>Detection network：这一部分主要利用RFCN网络;</li>
</ol>
<p>模型大体的Pipeline是对于当前帧I，给定临近帧的范围K，那么首先利用Feature network对临近帧进行特征提取，对于每一个临近帧J用Flow network计算与帧I的flow field，再利用双线性warp函数整合flow field和帧J对应的feature map进行flow-guided warp，最后结果将作为帧I特征聚合的一部分。帧I特征的聚合主要涉及权重矩阵的计算，权重矩阵的计算主要利用Embedding network对flow-guided warp的结果和原始的feature map进行特征整合，然后利用输出的特征计算对应的权重矩阵，具体的计算过程论文中的伪代码写的比较清楚，最后聚合所有2K个临近帧的特征作为Detection network的输入得到最后的检测结果.<br>伪代码：<br><img src="Flow-Guided-Feature-Aggregation-for-Video-Object-Detection-屏幕快照 2019-06-25 下午1.16.13.png" alt=""><br>整个PipeLine的描述：<br><img src="Flow-Guided-Feature-Aggregation-for-Video-Object-Detection-image003.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/05/12/Flow-Guided-Feature-Aggregation-for-Video-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/05/10/Flownet-Learning-optical-flow-with-convolutional-networks/">Flownet: Learning optical flow with convolutional networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/VID/">VID</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_iccv_2015/papers/Dosovitskiy_FlowNet_Learning_Optical_ICCV_2015_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_iccv_2015/papers/Dosovitskiy_FlowNet_Learning_Optical_ICCV_2015_paper.pdf</a><br>这是一篇ICCV2015的论文，主要是利用CNN来进行光流的计算，在视频的相关应用中通常会涉及到光流的概念，可参考Stanford CS131了解相关内容。论文主要提出了两种CNN结构：</p>
<ol>
<li>FlowNetSimple：直接将输入的两帧叠加到一起输入网络，经过一段CNN网络后得到最后的feature map， 但Pooling操作会使feature map的size通常小于原图，所以论文中通过refinement实现到原图size的转换。</li>
<li>FlowNetCorr：不同于前者，这个网络模型是将两帧分别输入网络，经过CNN特征提取后将两者的feature map合并到一起，合并的方式是利用correlation layer ，correlation layer对输入的两个feature map进行类似卷积的操作，假设区块大小为(2k + 1) * (2k  + 1)，两个feature map中对应的中心点为x1、x2，correlation值就按以下公式计算，与卷积不同的地方就是没有filter，而是两个区块直接相乘。如果约束x1在另一个feature map上的相关范围为D，那么correlation layer就可以得到w x h x D^2大小的输出。之后再经过一段CNN网络进行特征抽取。<br><img src="Flownet-Learning-optical-flow-with-convolutional-networks-image001.png" alt=""></li>
<li>两个模型在最后阶段都会涉及到refinement，refinement部分主要通过upconvolutional 操作来扩展feature map，只是每一层的输入除了上一层的feature map之外还会结合特征抽取部分相同size的feature map，这样在考虑High-Level特征信息的同时也会考虑局部的相关信息。<br><img src="Flownet-Learning-optical-flow-with-convolutional-networks-image002.png" alt=""><br><img src="Flownet-Learning-optical-flow-with-convolutional-networks-image003.png" alt=""><br>数据方面因为没有足够的训练数据，作者构造了Flying Chairs数据集，图片背景来自Flickr，前景为3D椅子模型。<br><img src="Flownet-Learning-optical-flow-with-convolutional-networks-image004.png" alt=""><br>最后的效果图：<br><img src="Flownet-Learning-optical-flow-with-convolutional-networks-image005.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2019/05/10/Flownet-Learning-optical-flow-with-convolutional-networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/05/01/Pose-Invariant-3D-Face-Alignment/">Pose-Invariant 3D Face Alignment </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/3D/">3D</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Jourabloo_Pose-Invariant_3D_Face_ICCV_2015_paper.pdf" target="_blank" rel="noopener">https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Jourabloo_Pose-Invariant_3D_Face_ICCV_2015_paper.pdf</a><br>利用3D模型来处理大Pose Landmark问题的一篇文章，整体还是follow 3DMM的那一套pipeline，只是因为这篇论文出的相对比较早，对于参数的学习是利用一般的回归模型来级联回归，整体在不同Pose下还是有一定效果的。不过和整个pipeline的设计有关速度比较慢。</p>
<p>下图是论文中描述的整体Pipeline：<br><img src="Pose-Invariant-3D-Face-Alignment-屏幕快照 2019-04-24 下午11.31.33.jpg" alt=""><br>整体可以分为这么几个主要部分：<br><strong>第一是3D模型的抽象</strong><br>这一部分和3DMM是一致的，任意人脸的描述被定义成平均脸和delta的和：<br><img src="Pose-Invariant-3D-Face-Alignment-屏幕快照 2019-05-29 下午10.43.41.png" alt=""><br>3D Landmark到2D Landmark的映射被定义为U = MS, 其中M为弱视角投影的参数.所以求解人脸的3D模型就会被转化成参数P = {M,p}的求解<br>那么通常我们对3D/2D人脸的标注只会涉及到Landmark点的坐标,没有办法得到M和p的gt，所以论文中也具体说明了M和p的gt的生成。首先定义具体的目标函数，那么对于理想的gt函数J应该为0，所以只要想办法最小化这个函数就行，具体计算的时候实际上是采用了类似启发式规则的方法，先让p为0，然后去求最优的M，然后再固定M去求最优的P，以此逐步迭代直到前后两次的值之间的delta很小，那么这个时候的M和p就是最后的gt，（V是关键点的可见性标注）:<br><img src="Pose-Invariant-3D-Face-Alignment-屏幕快照 2019-05-29 下午10.44.07.png" alt=""><br><strong>第二部分就是具体的模型训练</strong><br>关于具体参数的学习论文中所提的方式是在cascade的每一个阶段都出两个回归模型来分别回归M和p, 特征的提取是用的HOG特征，其他似乎没有什么特殊的:<br><img src="Pose-Invariant-3D-Face-Alignment-屏幕快照 2019-05-29 下午10.44.42.png" alt=""><br>论文中所提模型同时还出了点的可见性与否，但是实际上在预测的时候点的可见性是算出来的而不是模型直接预测出来的…具体可以看论文中的详细说明<br><img src="Pose-Invariant-3D-Face-Alignment-屏幕快照 2019-05-29 下午10.45.07.png" alt=""><br>综合整个Pipeline的逻辑：<br><img src="Pose-Invariant-3D-Face-Alignment-屏幕快照 2019-05-29 下午10.45.40.png" alt=""><br>至于在Benchmark上的结果，对于不同的Pose效果还是有的，不过在AFW上整体似乎差于TCDCN：<br><img src="Pose-Invariant-3D-Face-Alignment-屏幕快照 2019-05-29 下午10.46.06.png" alt=""><br>这篇论文另一个比较有趣的现象就是当对每一个landmark点进行分别评估NME的时候会发现面部的边界点误差很大，几乎是其他关键点的2倍，我们最近在做的时候也发现有同样的问题，因为面部边界很难有明确的语意定义，所以这个现象很难避免，对于这个问题CVPR2019的一篇semantic alignment论文还是很有参考意义的：<br><img src="Pose-Invariant-3D-Face-Alignment-屏幕快照 2019-05-29 下午10.46.31.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/05/01/Pose-Invariant-3D-Face-Alignment/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/04/20/Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression/">Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1904.07399" target="_blank" rel="noopener">https://arxiv.org/abs/1904.07399</a><br>最新挂出来的关于人脸landmark的论文，可以理解为整合wing loss + look at boundary进行的优化，wing loss在cvpr2018提出的时候是直接应用在回归landmark点坐标，作者想将其应用到heatmap出点的逻辑上因而提出了adaptive wing loss，同时在网络中引入boundary的信息来辅助模型的训练，在benchmark上的表现还是很不错的，部分指标可以和wing loss、lab拉开比较大的差距。</p>
<p>这篇论文的核心是提出了adaptive wing loss的概念，基于heatmap出点的方法作者认为模型需要focus在两个主要的部分，一个是前景区域，另外一个是比较难的背景区域，比较难的背景区域定义为前景区域附近的背景，具体的划分论文中也给了一个简单的示例：<br><img src="Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression-屏幕快照 2019-05-29 下午10.52.16.png" alt=""><br>因此作者在设计adaptive wing loss的时候也考虑了heatmap上不同pixel的重要性，那么为了更好的进行模型的训练，理想中的Loss function可以实现当训练初期gt与dt差距比较大的时候可以快速收敛，梯度可以直观的反馈gt与dt的差距，当gt和dt差距比较小的时候，前景和比较难的的背景pixel需要加大重要性，而其他的背景pixel需要削弱影响因素，所以最终adaptive wing loss具体的形式如下图，其中A = ω(1/(1 + (θ/ε)(α−y)))(α − y)((θ/ε)(α−y−1))(1/ε)，C = (θA−ω ln(1+(θ/ε)α−y))， 实际实验的时候α = 2.1,ω = 14,ε = 1,θ = 0.5<br><img src="Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression-屏幕快照 2019-04-18 下午10.50.38.png" alt=""><br>几个细节需要考虑：loss的第一部分指数为α-y，这个参数就是控制了不同pixel可以表现不同的重要性，比如越接近高斯分布的中心这个指数越小，否则越大，loss的第二部分当差距比较大的时候loss是一个常数可以比较块的收敛。<br>另外为了突出对gt的优化，作者提出了一个weigthed loss map的概念，本质就是对loss加权：<br><img src="Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression-屏幕快照 2019-04-18 下午11.28.31.jpg" alt=""><br><img src="Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression-屏幕快照 2019-04-18 下午11.30.26.png" alt=""><br>此外作者也参考了Look at boundary论文的做法将landmark组成的轮廓引入到网络中，感觉像是辅助监督，论文中是说了用coorconv来encode边缘的信息，具体细节论文也没有给出：<br><img src="Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression-屏幕快照 2019-04-18 下午8.36.11.png" alt=""><br>实验结果还是很高的：<br><img src="Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression-屏幕快照 2019-04-18 下午11.36.21.png" alt=""><br><img src="Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression-屏幕快照 2019-04-18 下午11.36.27.png" alt=""><br>作者对基于heatmap出点的方式所进行的loss设计的思考还是很有参考意义的，最后做出来的点也很高，感觉可以尝试</p>
</div></div><a class="button-hover more" href="../../2019/04/20/Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/04/10/Spatial-Transformer-Networks/">Spatial Transformer Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Classic/">Classic</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1506.02025.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.02025.pdf</a><br>这是一篇NIPS 2015的文章，主要提出了STN网络结构直接的赋予了网络对于各种变换的不变性。<br>STN网络主要分为三个部分：</p>
<ol>
<li>Localisation Network：一个子网络-用来学习变换参数θ ，θ的大小则和具体的变换有关，比如一般的仿射变换就是6维的参数，子网络的形式可以是FC也可以是CNN结构。</li>
<li>Parameterised Sampling Grid：这一部分负责将目标feature map和 源 feature map的像素之间形成映射，主要计算目标feature map的每一个位置在源feature map上对应的位置 TG。<br><img src="Spatial-Transformer-Networks-image001.png" alt=""></li>
<li>Differentiable Image Sampling：这一部分主要根据Parameterised Sampling Grid生成的TG和源feature map信息采样得到目标feature map.<br><img src="Spatial-Transformer-Networks-image002.png" alt=""><br>一些实验结果：<br><img src="Spatial-Transformer-Networks-image003.png" alt=""><br><img src="Spatial-Transformer-Networks-image004.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2019/04/10/Spatial-Transformer-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/04/04/Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection/">Semantic Alignment: Finding Semantically Consistent Ground-truth for Facial Landmark Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1903.10661" target="_blank" rel="noopener">https://arxiv.org/abs/1903.10661</a><br>CVPR2019最新挂出来的一篇关于人脸landmark的论文，论文的出发点是觉得目前landmark定位精度受限于部分标注点”语意”模糊有关,比如说脸部轮廓点或者眼部轮廓点不像眼球、鼻尖这些点有明确的语意定义，因此标注引入的误差就相对影响比较大。所以作者从这方面入手在模型每次迭代的时候去寻找这样一个“真正”的gt来监督网络的训练，此外为了来修正一些偏移比较厉害的点，作者又引入了一个子网络来refine整体的landmark。这篇论文整体个人感觉很有意义。</p>
<p>首先作者是利用4个级联的Hourglass结构网络来进行landmark点定位的，下图是作者可视化语意明确和语意不明确点在输出heatmap上的结果，在2D空间可以发现，语意比较明确的点比如眼球中心点它的分布更加接近高斯分布，在3D空间这些点的分布更加的锐利，而语意不明确的点比如轮廓点在3D空间就会形如一个“flat hat”：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-屏幕快照 2019-04-03 下午11.53.38.png" alt=""><br>同时当网络已经差不多收敛的时候如果继续训练也会发现那些语意不明确的点依然在gt附近来回抖动，这也一定程度上验证了语意不明确导致标注带来的noise。<br>从网络输出的heatmap上出landamrk点可以将landmark点理解为一种数据分布，w为网络权重、x为输入图片、o为landmark点：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-3043ff99afa634151fd3e0f785d7b1dbfbdd0c63.png" alt=""><br>那么既然gt也不是那么的准确，作者不妨就假设目前存在这样一个真正的gt，不会引入任何的语意不确定性，那么上述的公式可以表示为，y为定义的真正的gt, 那么o就可以理解为是y的一个观测值：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-77c089022e93074fcd6ae6a27cff0e58657635b3.png" alt=""><br>为了缩小后续的搜索空间作者做了一个合理的假定：y<sup>k</sup>存在于o<sup>k</sup>的附近，所以可以用高斯相似度来衡量这种先验概率：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-e1fa30cf7bebe4160d61ca0066ab6f041513d19c.png" alt=""><br>而至于公式的后半段似然概率作者认为对于模型输出的heatmap，如果位置(x, y)周围的region越符合高斯分布，那么点(x, y )就更接近y这样的真正的gt, 所以作者就利用两个分布（预测分布，实际分布(y的分布)）之间的相似度来度量这个似然概率，Φ为抠patch的操作，E为真正gt的分布：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-b8ba7247a1f5ce534923f05834a51f56ae2145cd.png" alt=""><br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-9d0211bc90065c7cb20afa4bd494d72283d417cc.png" alt=""><br>那么通过简单的转化就可以把前面的优化目标转换为，N(ok)可以理解为以点ok为中心的一小部分区域，其实也就是y的搜索空间：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-屏幕快照 2019-04-04 下午12.58.56.jpg" alt=""><br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-9774dd8b68edeb975c901d5289f5bed290e42e57.png" alt=""><br>那么在实际做的时候，作者将整个方法的优化分成两部分来进行，第一步是固定模型的参数W，去搜索最好的y，因为一旦w固定除了yk其他都是已知的，所以直接去搜索y<sup>k</sup>,搜索空间实际应该是o<sup>k</sup>为中心的17x17大小的区域，那么第一个iteration，标注结果就是gt，在二个iteration，前一个iteration搜索得到的y就是gt，依次类推。第二步是固定y去训练模型的参数W，然后这一步就是具体的模型训练了，训练目标为：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-d29dc1db9c88f3c357b807b8cf756a1e40ffb5db.png" alt=""><br>因为从模型的输出直接出landmark点没有完全的考虑脸的整个形态，更多的是考虑了单个点的相关信息，所以作者最后加了一个GHCU的模块来refine landmark点：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-屏幕快照 2019-04-04 下午1.10.55.png" alt=""><br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-屏幕快照 2019-04-04 下午1.12.28.png" alt=""><br>300-W上的实验结果：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-屏幕快照 2019-04-04 下午1.13.23.png" alt=""><br>AFLW的实验结果：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-屏幕快照 2019-04-04 下午1.14.02.png" alt=""><br>感觉作者的想法还是很make sense的，目前我们正在用的landmark标注数据也存在这样的语意模糊导致引入标注误差的问题，但是这种标注误差带来的影响还是和实际的任务比较相关，从论文给出的例子来看，预测的landmark点通常在gt附近有一定的抖动，如果这种抖动是贴合轮廓这个影响就相对比较小</p>
</div></div><a class="button-hover more" href="../../2019/04/04/Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/25/Object-Detection-based-on-Region-Decomposition-and-Assembly/">Object Detection based on Region Decomposition and Assembly</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1901.08225" target="_blank" rel="noopener">https://arxiv.org/abs/1901.08225</a><br>AAAI2019的一篇关于检测的论文，论文主要的出发点是想解决遮挡场景下的物体检测问题，整个逻辑基于Faster RCNN的框架来做，主要思路是先把proposal分part分别来提取特征然后再通过一定的方法将其merge到一起来突出可见部分的特征，从而得到更可信的信息。</p>
<p>论文所提方法整体基于Faster RCNN的逻辑，具体分成两个部分MRP和RDA，下面是具体的示意图：<br><img src="Object-Detection-based-on-Region-Decomposition-and-Assembly-图片 1.jpg" alt=""><br><strong>Multi-scale region proposal (MRP) network</strong><br>这一部分做法其实很简单，就是给RPN的输出proposal给一些scale来丰富porposal的覆盖程度，论文中用了[0.5, 0.7, 1, 1.2, 1.5]共5个scale：<br><img src="Object-Detection-based-on-Region-Decomposition-and-Assembly-屏幕快照 2019-03-16 下午5.47.30.jpg" alt=""><br><strong>Region decomposition and assembly (RDA) network</strong><br>这一部分可以理解为整个方法的核心了，主要分成Decomposition 和 Assembly两个部分， Decomposition 部分将经过ROI Pooling之后的feature map x2然后将其等分成上下左右四部分，每一个部分都会通过conv提取依次特征然后分别merge，merge的方法实际就是element wise max，从而可以显著一些可见区域的特征。这样4个part最终还是会merge为一个feature map，megre完的结果最后再跟全图的feature map在做一次同样的操作得到最后的output，整个逻辑的话图示很清楚：<br><img src="Object-Detection-based-on-Region-Decomposition-and-Assembly-屏幕快照 2019-03-16 下午5.52.33.png" alt=""><br><strong>实验结果</strong><br><img src="Object-Detection-based-on-Region-Decomposition-and-Assembly-屏幕快照 2019-03-16 下午5.56.03.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/03/25/Object-Detection-based-on-Region-Decomposition-and-Assembly/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/19/Mask-Scoring-R-CNN/">Mask Scoring R-CNN</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1903.00241" target="_blank" rel="noopener">https://arxiv.org/abs/1903.00241</a><br>论文的出发点很直观，就是为了优化在目前的一些instance segmentation的方法中用classification score来标注一个mask的质量，这个其实很显然和实际应用场景是完全不一致的，比如论文中给出的例子：<br><img src="Mask-Scoring-R-CNN-屏幕快照 2019-03-19 下午10.56.07.png" alt=""></p>
<p>所以为了解决这个问题，作者以mask rcnn为依托在其基础上增加了 MaskIoU 分支用来出mask和gt之间的IoU，而mask的质量分S<sub>mask</sub> = S<sub>cls</sub> * S<sub>mask_IoU</sub>：<br><img src="Mask-Scoring-R-CNN-屏幕快照 2019-03-19 下午11.01.42.png" alt=""><br>具体MaskIoU分支的逻辑图例给的比较清楚，mask分支的输出通过max pooling的作用之后和RoIAlign的结果进行concat作为MaskIoU分支的输入，经过conv和fc之后得到最后的C个iou，maskiou的计算也比较简单，mask分支的输出卡一个阈值0.5就可以实现二值化，二值化后的mask可以和gt可以比较简单的计算出IoU。</p>
<p>针对MaskIoU分支输入的形式作者也做了好几个尝试：<br><img src="Mask-Scoring-R-CNN-屏幕快照 2019-03-22 下午8.54.04.png" alt=""><br>最后显示直接相加效果是最好的：<br><img src="Mask-Scoring-R-CNN-屏幕快照 2019-03-22 下午8.55.55.png" alt=""></p>
<p>最后的点：<br><img src="Mask-Scoring-R-CNN-屏幕快照 2019-03-22 下午8.53.56.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/03/19/Mask-Scoring-R-CNN/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/18/Region-Proposal-by-Guided-Anchoring/">Region Proposal by Guided Anchoring</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1901.03278" target="_blank" rel="noopener">https://arxiv.org/abs/1901.03278</a><br>CVPR2019的一篇对anchor进行优化的论文，主要将原来需要预先定义的anchor改成直接end2end学习anchor位置和size。首先anchor的定义通常为(x, y, w, h) (x, y为中心点)，formulate一下：<br><img src="Region-Proposal-by-Guided-Anchoring-屏幕快照 2019-03-18 下午10.33.58.png" alt=""><br>因此本文所提的guided anchoring利用两个branch分别预测anchor的位置和w、h：<br><img src="Region-Proposal-by-Guided-Anchoring-屏幕快照 2019-03-18 下午10.28.29.png" alt=""></p>
<p>guided anchoring的主要内容有如下几点：<br><strong>Anchor Location Prediction</strong><br>逻辑很简单，利用一个1x1的conv将输入的feature map转换成 W x H x 1的heatmap，通过卡阈值t来得到anchor可能出现的位置，在训练的时候可以通过gt的框来生成heatmap的groudtruth，negtive、positive、ignore的pixel定义论文中有比较详细的介绍。<br><img src="Region-Proposal-by-Guided-Anchoring-屏幕快照 2019-03-18 下午10.46.37.png" alt=""><br><strong>Anchor Shape Prediction</strong><br>这一部分逻辑和上一部分一样，也是通过一个1x1的conv将输入的feature map转换成W x H x 2的heatmap，只是考虑到如果直接回归w和h范围太广会比较不稳定，作者做了一定的转化将预测值约束到[-1,1],实际使用的时候再映射回去，s为feature map的stride，sigma为8：<br><img src="Region-Proposal-by-Guided-Anchoring-屏幕快照 2019-03-18 下午10.41.23.png" alt=""><br>需要注意的是和传统的anchor设置不一样的是，guider anchoring在某一个pixel下只会设置一个anchor。<br>这一部分的训练其实会是比较需要特别注意的地方，论文中使用来IoU loss来监督，但是这样存在一个问题，因为这个分支本身是预测w，h的，所以IoU Loss的计算无法知道match的具体gt，作者提出的方法是sample 9组常见的w、h，这样就可以利用这9组w、h构建9个不同的anchor去和gt匹配，IoU最大的匹配gt就是当前需要去计算IoU Loss的gt，然后直接用heatmap的w、h和这个gt计算IoU Loss即可：<br><img src="Region-Proposal-by-Guided-Anchoring-屏幕快照 2019-03-18 下午10.48.54.png" alt=""><br><strong>Anchor-Guided Feature Adaptation</strong><br>这一个模块主要是针对feature有可能和anchor不一致而提出的，因为对于原先预定义的anchor而言，每一个pixel对应位置的anchor其实都是一样的，所以也就无所谓feature的异同，但是guided anchoring逻辑下不同的pixel有可能anchor的size差别很大，仍然像之前那样直接出cls和reg很显然是不合适的，所以作者就提出了adaptation的模块，利用deformable conv来处理不同形状的anchor对应的feature。</p>
<p>论文的最后作者也提了一下因为GA-RPN可以得到很多高质量的porposal，通过提高阈值可以进一步优化检测的效果。<br>实验结果：<br><img src="Region-Proposal-by-Guided-Anchoring-屏幕快照 2019-03-18 下午11.21.29.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/03/18/Region-Proposal-by-Guided-Anchoring/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/15/Grid-RCNN/">Grid RCNN</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1811.12030" target="_blank" rel="noopener">https://arxiv.org/abs/1811.12030</a><br>CVPR2018的一篇论文，从某种程度上来说是借鉴Bottom Up的方法来优化目前检测方面的一些问题，主要出发点还是希望检测器出的框能尽可能的准，所以相比较一般的检测器直接出四维的坐标信息，Grid RCNN则是出9个点，用9个点的信息来表示一个bbox。<br>具体的PipeLine如下：<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.04.35.png" alt=""></p>
<p><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.04.42.png" alt=""><br>Grid RCNN本身是基于RCNN这一套Two Stage的逻辑来做的，所以相比较Faster RCNN主要就是Fast RCNN那个分支做了一些优化，主要几个方面：<br><strong>Grid Guided Localization</strong><br>用NxN个均匀的点来表示一个框而不再是直接回归两个顶点坐标，这样做相比较FC回归点的好处是Conv保留了物体的一些空间位置信息，有助于物体的定位，而类似的 Grid RCNN相比较CornerNet之类基于脚点的检测模型好处在于CornerNet是直接出两个顶点的信息，但是实际上对于一个框的两个顶点它实际上多数处在一个backbroud上，实际可利用的有价值的信息很有限，因此Grid RCNN以及ExteamNet实际上在一定程度上都缓解了这个问题，那么通过Heatmap得到NxN个点之后(共NxN个Heatmap)就可以通过简单的坐标转换得到在原图中NxN个点的坐标，而将NxN个点转换称框的时候作者也提出了自己的逻辑：<br>（ (Px,Py) is the position of upper left corner of the proposal in input image, wp and hp are width and height of proposal, wo and ho are width and height of output heatmap）<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.11.08.png" alt=""><br>本质是用四条边上N个点坐标的加权平均作为边的坐标：<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.11.11.png" alt=""><br><strong>Grid Points Feature Fusion</strong><br>这一部分可以理解为对Grid RCNN的优化了，作者认为NxN点之间是存在比较强的关联信息的，点与点之间相辅相成可以达到共同促进的作用，所以提出了fusion的逻辑：<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.14.19.png" alt=""><br>做法也很粗暴直接，就是直接将最近点的heatmap通过<strong>3层5x5的conv提取特征</strong>之后直接和当前点的heatmap<strong>取sum</strong>:<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.17.14.png" alt=""><br>那么对于<strong>最近点</strong>的定义论文中也给的很清楚，比如距离为1，那就是相邻的所有点，距离为2，那就是所有距离当前点2个单位长度的点综合来fusion，上面的示意图给的比较清楚，对于当前点的<strong>最近点</strong>被定义为’source point’<br><strong>Extended Region Mapping</strong><br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.19.26.png" alt=""><br>这个优化主要是针对RPN给出的Proposal不够准，导致定义的NxN个点其实并不能包含检测的物体，那么最简单的方法就是把proposal认为放大，但是这样人为放大之后会严重影响检测的效果，尤其是对小物体而言，所以作者认为考虑到整个CNN的运算过程中感受野是足够的，所以就可以把这些个proposal<strong>看作</strong>是4倍于原来Proposal大小,那么我们就需要直接改坐标的映射关系就好：<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.23.47.png" alt=""><br>这个可以和上面提供的原公式做一个简单的化简其实就是在原来的基础上加了一个偏移量来smooth这个操作，其实整体感觉也好理解，虽然proposla给的框比较小，但是因为感受野的原因最后抽取的特征是可以包含object的信息的，所以就可以直接理解为这个点的坐标偏移相比正常的proposal来说更大，所以需要重新计算加一个偏移量。<br>结果上也是不错的：<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.32.52.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/03/15/Grid-RCNN/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/03/Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks/">Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1711.06753" target="_blank" rel="noopener">https://arxiv.org/abs/1711.06753</a><br>CVPR2018一篇关于人脸Landmark的论文，这篇论文主要是关于人脸关键点的定位，因为论文的重点是loss function和data augmentation所以论文所实验的模型结构是比较简单的CNN结构来实验：<br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image002.png" alt=""></p>
<p>论文主要的研究内容：</p>
<ul>
<li><strong>Wing Loss</strong>：论文首先通过实验直观的反映了常见的L1 Loss、L2 Loss、Smooth L1 Loss的优劣，通过分析不同损失函数的走势，作者认为landmark定位任务中需要更加重视中小范围误差的那些样本(small or medium range error)：<br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image003.png" alt=""><br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image004.png" alt=""><br>因此论文提出了wing loss损失函数，利用对数函数来增强小误差那些样本的表现，其中C是个常数 C = W - Wln(1 + W / e)，至于最终两个变量的取值只能一一尝试，论文也给出了具体的尝试，W = 10 , e = 2最终表现最好：<br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image007.png" alt=""><br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image008.png" alt=""></li>
<li><strong>Pose-based data balancing</strong>：PDB主要用来解决大Pose表现不好的问题，作者认为 大 pose表现不好的根本原因是样本数据不均衡，因此提出了PDB的策略。论文首先利用Procrustes Analysis和PCA将数据集中不同的人脸转化到一维向量空间中用来分析样本pose的分布（具体操作逻辑还需要仔细看，论文说的比较少还不是很清楚），比如对于AFLW数据集可以得到下面的分布图，然后根据具体的样本分布对于那些占比比较小的pose类别通过基本的data augmentation方法来增加这类样本的数量(其实就是直接多复制几份这样的数据):<br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image009.png" alt=""></li>
<li><strong>Two-stage landmark localisation</strong>：这一优化比较常见，利用cascade的逻辑讲landmark的定位分到两阶段CNN网络中，第一阶段就是上面提到的CNN-6，第二阶段则是CNN-7，与CNN-6的差别就是输入从64x64x3变到了128x128x3，增加了一层卷积层，卷积核的个数也略有增加，其他没有什么特殊的设计，最后cascade的逻辑和PDB数据增强带来的效果，CNN-6/7就代表two stage的模型：<br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image010.png" alt=""><br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image011.png" alt=""></li>
</ul>
</div></div><a class="button-hover more" href="../../2019/03/03/Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/03/Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression/">Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL：<a href="https://arxiv.org/abs/1902.09630" target="_blank" rel="noopener">https://arxiv.org/abs/1902.09630</a><br>这是CVPR2019的一篇论文。本论文主要提出了GIoU的概念来优化IoU在评估或者IoU Loss在训练中的一些问题<br>这是利用Ln-Loss来优化bbox回归问题的常见bug，不同的overlap程度在Ln-loss看来都一样，但是实际上对于IoU或者GIoU确实不一样的，很显然后者更合理：</p>
<p><img src="Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression-屏幕快照 2019-03-03 上午11.01.57.png" alt=""></p>
<p>然后就是IoU的不足，第一是对于IoU为0的情况也就是两个box没有交集的情况无法处理，IoU Loss在这种情况下没有梯度的回传。第二就是对于IoU的评价指标对于overlap的方式没有什么体现，比如下图中的示例，IoU都是0.33，但是它们的链接方法是不一样的，或者说对于box回归的任务来说我们的接受度也是不一样的，很显然下图是依次递减，恰好GIoU刚好可以做到这一点：<br><img src="Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression-屏幕快照 2019-03-03 上午11.05.36.png" alt=""><br>接下来就是GIoU的计算，C(AUB)可以理解为两框在convex之内的空白部分了,GIoU计算的方法主要claim一点，更整齐的overlap方法会导致空白部分很小所以GIoU更大：<br><img src="Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression-屏幕快照 2019-03-03 上午11.08.11.png" alt=""><br>GIoU Loss：<br><img src="Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression-屏幕快照 2019-03-03 上午11.09.13.png" alt=""><br>不过从结果上来看，GIoU似乎只对YoloV3这样anchor相对比较稀疏的模型比较有效，也算比较符合GIoU的定义吧：<br><img src="Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression-屏幕快照 2019-03-03 上午11.11.37.png" alt=""><br><img src="Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression-屏幕快照 2019-03-03 上午11.11.42.png" alt=""><br><img src="Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression-屏幕快照 2019-03-03 上午11.11.50.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/03/03/Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/02/Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points/">Bottom-up Object Detection by Grouping Extreme and Center Points</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-27</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1901.08043.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.08043.pdf</a><br>Codebase:<a href="https://github.com/xingyizhou/ExtremeNet" target="_blank" rel="noopener">https://github.com/xingyizhou/ExtremeNet</a><br>一篇比较有意义的论文，主要是用bottom up的方法来做检测的问题，整个工作是基于ECCV2018的cornernet来做的，对于一个框ExtremeNet会出5个点，四个边界点和一个中心点，中心点主要是用来做group。下图是标注的示例图：</p>
<p><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-02 下午4.52.39.png" alt=""></p>
<p>论文的整体框架也很简单，主要都是基于CornerNet的code进行改的，对于一张图片出5个点的heatmap和四个offset的heatmap：<br><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-02 下午4.44.43.png" alt=""><br>当拿到最后的5点heatmap和4个offset之后，对于点的instance group方法也很简单甚至比较暴力，首先会设置一个阈值T<sub>p</sub>，那么4个边界点heatmap上大于T<sub>p</sub>的话就会被记为一个candidate，论文中也说了在 coco数据集上一般会有40个左右，那么匹配方法很简单，暴力O(N^4)轮询，然后对于当前的4个点，直接通过取平均的方式得到中心点，然后按这个中心点的位置去中心点的heatmap上取值，如果这个值大于某个阈值T<sub>c</sub>，那么就认为这是一个合法的框：<br><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-02 下午4.45.10.png" alt=""><br>这样利用bottom up进行detection任务的整个框架大体就是如此，那么这种方法也有两个比较明显的问题：</p>
<ul>
<li>Ghost box：假设有三个一样大小、水平或者竖直等距离排布的物体，那么利用extremenet来预测的时候应该会出现4个比较高置信度的框！，因为外围两个框的四个边界点可以组成比较高置信度的框同时中心点还落在中间框的中心点会大概率满足阈值，论文中将这种box称之为ghost box，解决方法比较直接，如果出现一个框内部三个框的score之后大于本身这个大框(‘ghost box’)score的3倍，那么就将这个大框的置信度/2，这样有助于在解析来的NMS中remove掉这个box。</li>
<li>Edge aggregation：假设对于方方正正的物体，比如汽车等，那么由于它的边界点并不是很唯一，它的整条边的点都可以作为边界点，所以后果就是整条边的confidence都不高影响最后的box的生成，因此论文中作者借鉴来cornernet中的pooling的一些想法，对给定的一个extreme point在水平和竖直两个方向以单调递减(heatmap的score)的方法一直遍历直到找到一个局部最小值，过程中遍历的点score的和会作为加权的一部分算到当前extreme point的置信度上：<br>单调递减过程中的点：<br><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-03 上午10.20.37.png" alt=""><br>extreme point score的计算：<br><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-03 上午10.20.32.png" alt=""><br>具体的case：<br><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-03 上午10.22.36.png" alt=""><br>作者同时也把这个方法应用到instance seg的任务中，只是标注略微粗糙利用一个八边形来做mask，具体细节不赘述，具体示例：<br><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-03 上午10.12.15.png" alt=""><br><strong>实验结果还是很不错的：</strong><br><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-03 上午10.12.06.png" alt=""></li>
</ul>
</div></div><a class="button-hover more" href="../../2019/03/02/Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/02/PFLD-A-Practical-Facial-Landmark-Detector/">PFLD: A Practical Facial Landmark Detector</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1902.10859" target="_blank" rel="noopener">https://arxiv.org/abs/1902.10859</a><br>相关主页：<a href="https://sites.google.com/view/xjguo/fld" target="_blank" rel="noopener">https://sites.google.com/view/xjguo/fld</a><br>这两天刚挂出来的关于landmark的论文，论文中对300w数据集报的点是要比LAB、SAN等CVPR2018论文的点是要高的，在845手机上也可以达到140FPS的速度，论文所提方法主要在设计加权的loss，比如考虑人脸的yaw、pitch、roll等信息来加权loss。</p>
<p>论文首先总结了一下目前landmark检测的一些问题，比如局部遮挡、局部光照、人脸Pose、数据不均衡等，同时计算平台对算力的限制也是一个需要关注的点。除了最后的减少模型size是通过尝试不同的backbone网络，其他的论文所提问题可以理解为都集中在Loss的设计：<br><img src="PFLD-A-Practical-Facial-Landmark-Detector-屏幕快照 2019-03-02 上午10.22.31.png" alt=""><br>而整个Loss的设计又可以理解为加权的权重该以什么逻辑加上去，上式是论文中所提的Loss：<br>m是人脸数，n为landmark数，Wnc是当前脸所属的类别c所占总脸数的比例的倒数，theta为具体的角度，k为yaw、roll、pitch，dnm为距离计算的方式比如L1，L2.至此Loss的具体含义就不用赘述了；<br><img src="PFLD-A-Practical-Facial-Landmark-Detector-屏幕快照 2019-03-02 上午10.22.27.png" alt=""><br>这个是论文中所提出的整个网络结构：<br>backbone基于mobilenet v2，为了得到人脸的yaw、pitch、roll等值（默认送进网络的图经过align所以不需要考虑其他的状态信息），作者在backbone的基础上又加了一个分支来专门出这几个值，至于这几个值gt的生成是通过mean face直接计算得到的。</p>
<p>在300w数据集上的表现（ION和IPN是不同的距离norm方式）：<br><img src="PFLD-A-Practical-Facial-Landmark-Detector-屏幕快照 2019-03-02 上午10.22.48.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/03/02/PFLD-A-Practical-Facial-Landmark-Detector/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/02/28/S3FD-Single-Shot-Scale-invariant-Face-Detector/">S3FD: Single Shot Scale-invariant Face Detector</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_S3FD_Single_Shot_ICCV_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_S3FD_Single_Shot_ICCV_2017_paper.pdf</a><br>ICCV2017的一篇论文，主要研究小人脸的检测，作者针对目前基于anchor的检测器对小人脸表现不好的现象分析了几个可能的原因，并针对性的提出了具体的解决方法，比如新的anchor匹配策略、max out backgroud label等方法。</p>
<p>作者首先提出了目前小人脸的难点：<br><img src="S3FD-Single-Shot-Scale-invariant-Face-Detector-屏幕快照 2019-03-02 上午10.08.37.png" alt=""><br>论文中也正是从这四点出发来解决小人脸的检测问题。</p>
<p><img src="S3FD-Single-Shot-Scale-invariant-Face-Detector-屏幕快照 2019-03-02 上午10.07.42.png" alt=""><br>上图是论文中提出的基于VGG16的检测模型，比较特别的地方是Normalization Layer、conv3_3输出结果为Nm+4 而不是一般的2 + 4 以及 anchor设计的策略：</p>
<ul>
<li>Normalization Layer是考虑到Conv3_3 - Conv5_3 feature scale差异比较大，所以对activation做了norm来加速训练。</li>
<li>Conv3_3作为最底层的detection layer，主要负责小人脸的检测，对于小人脸的检测通常需要设置比较多的anchor，这就会导致比较验证的正负样本不均衡的现象，于是作者就提出了max-out backgroud label的逻辑，cls的那个分支会预测N + 1个类别，1就是face，N就是backgroud，然后取最大的backgroud的score去参与计算loss以此来提高cls分支的分类能力，毕竟background归为一类很难去精确分类。<br>max out backgroud逻辑：<br><img src="S3FD-Single-Shot-Scale-invariant-Face-Detector-屏幕快照 2019-03-02 上午10.19.42.png" alt=""></li>
<li>至于anchor的设计策略，可以细看论文中的table：<br><img src="S3FD-Single-Shot-Scale-invariant-Face-Detector-屏幕快照 2019-03-02 上午10.09.36.png" alt=""><br>anchor / stride 恒为4， 这样做的好处就是不同scale保证采样密度一致（两个anchor之间的overlap都是1/4anchor的大小）从而不同的人脸能基本匹配相同数目的anchor。而anchor具体的scale设置也是和一些观察经验有关的，比如论文提到的erf，有效感受野，因为对于图片的不同位置可以理解为权重是不一样的，比如靠近图片的中心他会有很大概率被其他的kernel重复计算，而边缘的pixel则相对被更加稀疏的计算，所以论文提到了ERF的概念，anchor也是针对ERF设计的：<br><img src="S3FD-Single-Shot-Scale-invariant-Face-Detector-屏幕快照 2019-03-02 上午10.09.40.png" alt=""><br>因为anchor的匹配是根据IoU来的，那么对于一些face还是会不可避免的匹配不上anchor，所以论文就提出了一个补充匹配的逻辑来缓解这个问题，方法也很简单，用基本的匹配逻辑匹配完一轮之后，对于剩下没有匹配的那些小脸取IoU 大于0.1的anchor，排序取top N补充进来。这个想法相对比较直观吧。<br>具体的实验结果：<br><img src="S3FD-Single-Shot-Scale-invariant-Face-Detector-屏幕快照 2019-03-02 上午10.14.05.png" alt=""></li>
</ul>
</div></div><a class="button-hover more" href="../../2019/02/28/S3FD-Single-Shot-Scale-invariant-Face-Detector/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/02/22/谈一谈模型量化/">谈一谈模型量化</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Quantization/">Quantization</a></div></div><div class="post-content"><div class="main-content content"><p>说到量化，首先需要弄清楚我们为什么需要量化？为什么可以量化？<br>对于为什么需要量化可以从两个实际的需求来说，第一，对于一般float32存储的模型来说通常硬盘占用相对会比较大，那么在现有的一些端上比如手机端对模型的内存占用是有很明显的限制的，将float32的模型转化为int8的模型就可以带来75%存储的节约，因此在某些场景下模型的量化也是一个必然的选择。第二，从模型的运算效率上来说，int8只有一个字节，float32有四个字节，所以int8参数的获取只需要25%float32的内存带宽，因此可以更好地使用缓存，同时每个时钟周期执行更多操作的SIMD操作。另外DSP等计算单元本身对int8计算很友好，int8模型的使用可以提高速度同时可以降低功耗。<br>至于为什么可以量化，可以这么理解，我们知道CNN网络在对图片进行各种处理的时候比如识别、比如分类、比如检测，都有比较强的鲁棒性，对噪声的兼容性相对比较高。那么落到量化这件事情上来说，可能带来的问题就是精度的损失，那么其实我们可以把这一部分精度的损失理解为外界带给网络的<strong>“噪声”</strong>,所以结合CNN在具体任务的表现我们有理由相信量化操作给网络带来的精度影响不会那么严重。当然了这算是比较理想的状态了，其实在具体的实验过程中对于目前已经做的比较轻量的网络，量化对模型的精度还是会有比较大的影响的。所以量化通常也是根据具体的任务需要做的一个选择。</p>
</div></div><a class="button-hover more" href="../../2019/02/22/谈一谈模型量化/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/02/03/You-Only-Look-Once-Unified-Real-Time-Object-Detection/">You Only Look Once: Unified, Real-Time Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-19</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Classic/">Classic</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" target="_blank" rel="noopener">https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf</a><br><strong>重读经典系列第一篇：YOLO</strong><br>YOLO系列是另一个经典的Single Stage的检测器：<br><img src="You-Only-Look-Once-Unified-Real-Time-Object-Detection-屏幕快照 2019-02-03 下午10.49.49.png" alt=""></p>
<p>YOLO整体的流程是这样的，第一步首先将输入图划分成SxS个Grid,</p>
</div></div><a class="button-hover more" href="../../2019/02/03/You-Only-Look-Once-Unified-Real-Time-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/02/03/SSD-Single-Shot-MultiBox-Detector/">SSD: Single Shot MultiBox Detector</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Classic/">Classic</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1512.02325.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1512.02325.pdf</a><br>Code: <a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">https://github.com/weiliu89/caffe/tree/ssd</a><br><strong>重读经典系列第一篇：SSD</strong><br>SSD是很经典的Single Stage检测网络，至今仍有很多的工作是基于SSD在改进。<br>不同于典型的Two stage检测网络将proposal的生成放在RPN网络来做，然后后续的网络branch基于RPN的结果进行Refine，SSD将Proposal直接放到网络后面的feature map上来做，基本逻辑如下图，实际和FasterRCNN网络生成anchor的逻辑是一模一样的：<br><img src="SSD-Single-Shot-MultiBox-Detector-屏幕快照 2019-02-03 下午10.33.52.png" alt=""></p>
<p>至于proposal具体的scale，论文中也是给出具体的公式的，整体其实就是给定最大和最小scale(s<sub>min</sub>, s<sub>max</sub>)，中间的feature map层均匀变化：<br><img src="SSD-Single-Shot-MultiBox-Detector-屏幕快照 2019-02-03 下午10.38.39.png" alt=""><br>k就是具体的层，m是全部产生proposal的层<br>SSD网络整体的结构目前看来也比较直接，相比较传统的VGG等网络，SSD考虑到multi scale的情况，所以在VGG16 backbone的基础上又引出了N层feature map，每一层feature map的感受野都不一样，作者通过这种方式来实现multi scale的检测，每一个feature map都会出4个offset和一个分类器，回归分支的结果是用中心点坐标（x，y） + w和h来表示一个框：<br><img src="SSD-Single-Shot-MultiBox-Detector-屏幕快照 2019-02-03 下午10.39.31.png" alt=""><br>训练用到的loss，对于回归用的smooth l1:<br><img src="SSD-Single-Shot-MultiBox-Detector-屏幕快照 2019-02-03 下午10.43.17.png" alt=""><br><img src="SSD-Single-Shot-MultiBox-Detector-屏幕快照 2019-02-03 下午10.43.21.png" alt=""><br>一些benchmark上的结果：<br><img src="SSD-Single-Shot-MultiBox-Detector-屏幕快照 2019-02-03 下午10.42.36.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/02/03/SSD-Single-Shot-MultiBox-Detector/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/31/High-Speed-Tracking-by-Detection-Without-Using-Image-Information/">High-Speed Tracking-by-Detection Without Using Image Information</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Tracking/">Tracking</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://ieeexplore.ieee.org/document/8078516/" target="_blank" rel="noopener">http://ieeexplore.ieee.org/document/8078516/</a><br>一篇关于IoU tracker的论文，整个跟踪的逻辑都基于检测框来做，对于当前的帧f，检测模型首先会检测出这一帧上面所有的bounding box，然后利用贪心的逻辑将捡出来的框尝试加入到对应的track中，匹配的逻辑就是根据当前的bbox和track的bbox之间的IoU，然后IoU &gt; IoU<sub>threshold</sub>,就把它加到当前track中否则看整个bbox的score，如果score  &gt;= score<sub>threshold</sub> 并且 track的长度  &gt;= length<sub>threshold</sub>就把当前帧作为这个track的结束帧，否则就直接终端当前的这个track，因为他有很大的概率是一段fp，当然论文中也提到匹配的时候也是可以用最大匹配（IoU最大）的一些算法来做的：</p>
<p><img src="High-Speed-Tracking-by-Detection-Without-Using-Image-Information-image.jpg" alt=""><br><img src="High-Speed-Tracking-by-Detection-Without-Using-Image-Information-屏幕快照 2019-01-29 下午8.01.57.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/31/High-Speed-Tracking-by-Detection-Without-Using-Image-Information/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/28/Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose/">Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-09</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Skeleton/">Skeleton</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1811.12004.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.12004.pdf</a><br>论文主要在探索把OpenPose做的轻量化一点，从模型结构到工程优化都做了不少工作，基本把一些常见的优化操作都尝试了一下，比如把原有的VGG backbone替换成针对手机端的mobilenet、把7x7的conv化成1 x 1 + 3 x 3 + 3 x 3 + residual connection、把PAF和joint两个分支进行部分合并共享参数等等。最后模型的复杂度从61.7GFlops降到9GFlops的同时AP只从43.3%降到42.8%，速度直接的量化结果（NUC支持FP16、CPU支持FP32）：<br><img src="Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose-屏幕快照 2018-12-23 下午11.55.18.png" alt=""></p>
<p><img src="Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose-屏幕快照 2019-01-28 下午4.16.00.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/28/Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/28/Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation/">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1802.02611.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1802.02611.pdf</a><br>这是DeepLab系列的第四篇文章，DeepLab V3+，这篇论文的主要内容主要是在DeepLab V3的基础上接了一个decoder形成一个hourglass的结构结果直接出原图的resolution，这相比前面版本直接插值要更好一点，整个结构中encoder就直接用的DeepLab V3的结构，输出结果concat到一起之后会和低层的网络特征直接concat，低层的特征通过1x1卷积来降维，简单的示意图：<br><img src="Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation-屏幕快照 2019-01-26 上午11.18.27.png" alt=""></p>
<p>论文中另外一个主要内容是depthwise separable convolution的使用，论文中用深度可分离的空洞卷积来替换掉原有xception中的max pooling 层，主要也是出于模型速度的优化，整体的网络结构：<br><img src="Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation-屏幕快照 2019-01-26 上午11.29.11.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/28/Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/28/Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/">Rethinking Atrous Convolution for Semantic Image Segmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/DeepLab/">DeepLab</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1706.05587.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.05587.pdf</a><br>这篇论文是DeepLab系列的第三篇论文DeepLab V3. 相比较DeepLab V1和V2主要在研究如何更好的运用空洞卷积, 同时也去掉了之前的DenseCRF后处理模块。论文中主要涉及了两个方面一个是串行的连接空洞卷积，因为相比较传统的直接用conv层来增加网络的深度会导致抽象到最后一层会丢掉很多的原始信息，利用空洞卷积可以一定程度上优化这个问题。另一方面是在ASPP的基础上优化空洞卷积的使用，作者通过实验发现，如果一味的通过扩大空洞卷积的rate来扩大感受野，有时候并不能得到很好的结果，比如下图就说明当rate足够大的时候其实卷积核有效的参数量只有一个：<br><img src="Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation-屏幕快照 2019-01-26 上午11.08.28.png" alt=""></p>
<p>所以最后作者在ASPP模块中就直接用全图的global average pooling来替代空洞卷积来获取比较大区域的特征：<br><img src="Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation-20181221233024.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/28/Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/28/DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs/">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/DeepLab/">DeepLab</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener">https://arxiv.org/abs/1606.00915</a><br>DeepLab系列的第二篇论文，主要是在DeepLab V1的基础上提出了ASPP的结构来做多尺度的问题。<br>在DeepLab V1中作者其实也提到了multiscale的问题，当时论文中提到的方法就是对输入进行多次rescale，那么这种方法很显然太naive，并且也会带来比较大的运算量。因此DeepLab V2中作者就借鉴SPP的方法提出了ASPP的结构来解决多尺度的问题，ASPP本质就是用不同rate的空洞卷积来实现不同的感受野这样就可以覆盖比较多scale的物体，具体的应用逻辑论文中也给出了具体的结构：<br><img src="DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs-屏幕快照 2019-01-25 下午11.01.55.png" alt=""></p>
<p><img src="DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs-20181220234952.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/28/DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/24/SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS/">SEMANTIC IMAGE SEGMENTATION WITH DEEP CON- VOLUTIONAL NETS AND FULLY CONNECTED CRFS</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/DeepLab/">DeepLab</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1606.00915.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1606.00915.pdf</a><br>Semantic Segmentation比较经典的DeepLab系列的第一篇，DeepLab V1，主要利用FCN和DenseCRF来实现比较出色的segmentation效果，DeepLab V1的整个Pipeline整体可以分成两个部分: FCN得到相对比较粗糙的分割结果，DenseCRF在FCN结果的基础上对边缘进行Refine得到相对比较分明的物体分割轮廓。<br>在FCN部分主要是基于VGG16，将VGG16中FC换成Conv变成全卷积网络，而为了保持相对比较dense的feaure map,从VGG16原来的32倍下采样提高到8倍下采样，这个可以通过改变stride再加上空洞卷积来保持感受野，空洞卷积的具体示意图如下，网上也可以找到比较详细的解释：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午8.53.19.png" alt=""><br>最后在inference的时候可以直接从8倍下采样的结果直接插值回原图resolution.</p>
<p>第一阶段FCN得到的pixel wise的结果无疑是相对比较粗糙的，论文中作者也给了一些对比的图,FCN的结果相比较gt还是有比较大的差距的：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.21.37.png" alt=""><br>论文中提及的FC CRF本身是针对轮廓的处理提出来的，论文中也给出了具体的计算公式，x<sub>i</sub>为pixel的label，I为pixel的颜色轻度，整体逻辑是基于pixel的位置和pixel的颜色强度尽量让“相似”的pixel归为一类，让“不同”的pixel归为另一类，这样就可以在物体的边缘区域有一个比较好的划分：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.29.59.png" alt=""><br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.30.02.png" alt=""></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在VOC2012上的测试结果，MSc代表多尺度的特征融合，论文中是对网络的最后4个mas pooling层进行特征融合，FOV则是和空洞卷积有关：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.38.17.png" alt=""><br>作者也过空洞卷积的参数设置进行了简单的实验：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.40.38.png" alt=""><br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.41.21.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/24/SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/24/PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model/">PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Skeleton/">Skeleton</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1803.08225" target="_blank" rel="noopener">https://arxiv.org/abs/1803.08225</a><br>这篇论文其实和之前读的G-RMI那篇论文是同一个作者，G-RMI是top down的逻辑，而这篇论文是bottom up的逻辑。论文所提方法同时在做pose estimation和instance-level person segmentation两个task。pose estimation主要是通过预测点对之间的向量来做group，seg则是主要借鉴embedding的逻辑通过设计新的loss函数来优化效果。<br>下图是论文所提方法的整个pipeline，两个大的分支，pose + seg：<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-03 下午11.18.23.jpg" alt=""></p>
<p>先说一下论文中对pose estimation的做法，这一部分主要涉及三个内容：</p>
<ul>
<li><strong>heatmap生成</strong>：k个joint， k channel的heatmap，同时针对每一个joint人为围绕这个joint设置一个半径为R的圆（论文中R=32pixel），圆内的点为1，其余为0，构造一个分类任务来监督heatmap的生成。</li>
<li><strong>short-range offset</strong>: 把G-RMI那篇论文中用到的方法拿过来应用，主要是在heatmap的基础上还会预测在joint半径R pixel内的点与当前joint点的offset。然后再整合heatmap和offset（hough voting）得到最后的位置信息具体如上图。</li>
<li><strong>mid-range offset</strong>: 这个内容主要是用来对多人做group的。主要是想通过预测两个joint之间的offset向量在inference的进行instance的划分，但是在整张图中预测两点之间准确的offset是很难的，所以作者就把这个问题转换成两个joint半径R区域内相对应的点之间的offset和圆内点到joint之间的offset的加和来避免这样的问题，相当于用两个相对比较糙的结果来refine 最后的offset：<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-03 下午11.20.40.png" alt=""><br>论文中另外一个主要的部分就是关于seg的内容, 也引出了论文提到的第三个offset，long-range offset：<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-06 下午11.19.49.jpg" alt=""><br>论文所提做seg的方法主要是借鉴了embedding 的逻辑比如associate embedding方法，这些方法的一个主要思想就是设计一个loss使得属于同一个instance的点离的尽量近，不属于同一个instance的点离的尽量远，long-range offset就是作者在设计loss函数（距离函数）时提到的概念。long-range offset就是instance内的点到某一个joint的offset，上图画的比较清楚，具体计算distance的时候是：<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-06 下午11.30.27.png" alt=""><br>其中G(x) = x + L(x), L(x）就是long-range offset.</li>
</ul>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>Pose Estimation:<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-06 下午11.32.43.png" alt=""><br>Segmentation:<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-06 下午11.33.19.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/24/PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/22/Fully-Convolutional-Siamese-Networks-for-Object-Tracking/">Fully-Convolutional Siamese Networks for Object Tracking </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Tracking/">Tracking</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1606.09549" target="_blank" rel="noopener">https://arxiv.org/abs/1606.09549</a><br>Siamese-FC v1 , 利用Siamese网络来做跟踪，论文中把跟踪的任务定义为两个图像patch之间的相似度，通过计算两个图像patch之间的相似度来定位物体，通过多次rescale 输入图片来实现多尺度物体的跟踪。<br>下图是论文中给出的Siamese-FC v1基本的示意图：<br><img src="Fully-Convolutional-Siamese-Networks-for-Object-Tracking-d0c371c53a45ca34e71ad8a64fdc055b53e0f958.png" alt=""></p>
<p>网络的第一个分支为目标object，输入被resize到127x127，经过全卷积网络之后得到6x6x128的输出。<br>网络的第二个分支为搜索的图片，经过同样的全卷积网络之后也会得到相对应的输出比如上图的22x22x128, 这两个分支的输出最后会通过 cross- correlation 操作(第一个分支的输出会作为第二个分支输出的kernel进行卷积操作)得到最后的输出，比如上图的17x17x1，那么直观上来看对于最后输出的score map上的每一个点其实就对应到原图和目标object图像同样大小的区域，而score map上这个点的取值就可以理解为原图中这块区域和目标object之间的相似度，那么最后相似度最高的点就被定位为目标object在当前帧上面的位置。<br>至于具体训练的时候论文中也提到了一些细节，比如正负例的定义，score map中落在中心半径R范围内的点被定义为正例label为1，其余为-1. 输出的score map会利用cosine window来抑制距离中心比较远的点。多尺度物体的检测则是直接通过rescale输入图片来实现的。<br><img src="Fully-Convolutional-Siamese-Networks-for-Object-Tracking-9ec817caf543acb7d9a52ed7008a1a31d9679a8d.jpeg" alt=""><br>一些实验结果,SiamFC-3s代表进行三次scale缩放：<br><img src="Fully-Convolutional-Siamese-Networks-for-Object-Tracking-b23460370758b3eca2601241e9fcda1eec173e47.png" alt=""><br><img src="Fully-Convolutional-Siamese-Networks-for-Object-Tracking-f83536950b93ff0983f49a137ba3b018ba9c7fd1.png" alt=""><br>Siamese-FC因为是从分类的角度来做位置的定位的，所以感觉框的精度会比较不够精确</p>
</div></div><a class="button-hover more" href="../../2019/01/22/Fully-Convolutional-Siamese-Networks-for-Object-Tracking/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/20/IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks/">IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1806.00178" target="_blank" rel="noopener">https://arxiv.org/abs/1806.00178</a><br>IGCV3，BMVC2018，在IGCV2基础上进行的改进，主要引入bottleneck的网络结构来改善IGCV2网络的信息交互，从而进一步提高网络效果。<br>在IGCV2中曾提到过一个互补条件：前面一级group convolution里不同partition的channel在后面一级的group convolution要在同一个partition里面，那么follow这个逻辑设计的IGCV2网络结构的每一个输出channel、输入channel之间将有且仅有一条路径可以连接，示意图如下，这就会导致网络过于稀疏不一定有利于网络的整体性能：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-a0f037095b27476caa3cf49ca30929b2923f2315_1_690x253.jpeg" alt=""></p>
<p>那么在IGCV3网络设计中借助bottleneck结构提出了super-channel的概念，每一个super-channel其实就是一组channel，first group convolution是1x1的卷积，它把网络变宽，second group convolution是3x3的spatial conv，third group convolution 同样是1x1的卷积，它又把网络结构压缩到原来的宽度，整个过程中的permutation和IGCV1、IGCV2的逻辑是一致的，这样设计之后每一个输出和输入的channel之间都会有多条路径可以交互，这样就优化了IGCV2网络设计中的问题，具体逻辑如下图：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-dfcbf9fea9f926a00f49a63e9242ce7a83ec10e8_1_690x316.png" alt=""><br>IGCV1、IGCV2、IGCV3在CIFAR上面的对比：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-2460c1c050b642ec41193bbc0f29770f65933ac2_1_690x242.png" alt=""><br>和MobileNetV2的对比：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-屏幕快照 2019-01-19 上午11.38.42.png" alt=""><br>和其他现有针对移动平台设计的网络对比：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-屏幕快照 2019-01-19 上午11.39.27.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/20/IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks/#more">阅读全文</a></div></div><div id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="../../"><i class="fas fa-angle-left"></i></a><a class="page-number" href="../../">1</a><span class="page-number current">2</span><a class="page-number" href="../3/">3</a><a class="page-number" href="../4/">4</a><a class="extend next" rel="next" href="../3/"><i class="fas fa-angle-right"></i></a></div></div></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fas fa-user"></i></span><span id="busuanzi_value_site_uv"></span><span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fas fa-eye"></i></span><span id="busuanzi_value_site_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Out of Memory</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/copy.js"></script><!--script(src=url)--></body></html>