<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Live and Learn"><meta name="keywords" content=""><meta name="author" content="libanghuai,undefined"><meta name="copyright" content="libanghuai"><title>Live and Learn【Out of Memory】</title><link rel="stylesheet" href="../../css/fan.css"><link rel="stylesheet" href="../../css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="../../favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="../../js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="author-info"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">libanghuai</div><div class="author-info-description">Live and Learn</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/fan-lv" target="_blank">GitHub<i class="icon-dot bg-color2"></i></a><a class="links-button button-hover" href="mailto:15757856604@163.com" target="_blank">E-Mail<i class="icon-dot bg-color3"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1019593584&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color1"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="../../archives"><span class="pull-top">日志</span><span class="pull-bottom">82</span></a><a class="author-info-articles-tags article-meta" href="../../tags"><span class="pull-top">标签</span><span class="pull-bottom">30</span></a><a class="author-info-articles-categories article-meta" href="../../categories"><span class="pull-top">分类</span><span class="pull-bottom">3</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Out of Memory</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><div id="recent-posts"><!-- each post in page.posts.sort('date', -1).limit(10).toArray()--><!-- config中配置按照什么排序--><div class="recent-post-item"><a class="post-title" href="../../2019/01/31/How-Does-Batch-Normalization-Help-Optimization/">How Does Batch Normalization Help Optimization?</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time></div><div class="post-content"><div class="main-content content"></div></div><a class="button-hover more" href="../../2019/01/31/How-Does-Batch-Normalization-Help-Optimization/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/31/High-Speed-Tracking-by-Detection-Without-Using-Image-Information/">High-Speed Tracking-by-Detection Without Using Image Information</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Tracking/">Tracking</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://ieeexplore.ieee.org/document/8078516/" target="_blank" rel="noopener">http://ieeexplore.ieee.org/document/8078516/</a><br>一篇关于IoU tracker的论文，整个跟踪的逻辑都基于检测框来做，对于当前的帧f，检测模型首先会检测出这一帧上面所有的bounding box，然后利用贪心的逻辑将捡出来的框尝试加入到对应的track中，匹配的逻辑就是根据当前的bbox和track的bbox之间的IoU，然后IoU &gt; IoU<sub>threshold</sub>,就把它加到当前track中否则看整个bbox的score，如果score  &gt;= score<sub>threshold</sub> 并且 track的长度  &gt;= length<sub>threshold</sub>就把当前帧作为这个track的结束帧，否则就直接终端当前的这个track，因为他有很大的概率是一段fp，当然论文中也提到匹配的时候也是可以用最大匹配（IoU最大）的一些算法来做的：</p>
<p><img src="High-Speed-Tracking-by-Detection-Without-Using-Image-Information-image.jpg" alt=""><br><img src="High-Speed-Tracking-by-Detection-Without-Using-Image-Information-屏幕快照 2019-01-29 下午8.01.57.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/31/High-Speed-Tracking-by-Detection-Without-Using-Image-Information/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/28/Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose/">Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/KeyPoint/">KeyPoint</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1811.12004.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.12004.pdf</a><br>论文主要在探索把OpenPose做的轻量化一点，从模型结构到工程优化都做了不少工作，基本把一些常见的优化操作都尝试了一下，比如把原有的VGG backbone替换成针对手机端的mobilenet、把7x7的conv化成1 x 1 + 3 x 3 + 3 x 3 + residual connection、把PAF和joint两个分支进行部分合并共享参数等等。最后模型的复杂度从61.7GFlops降到9GFlops的同时AP只从43.3%降到42.8%，速度直接的量化结果（NUC支持FP16、CPU支持FP32）：<br><img src="Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose-屏幕快照 2018-12-23 下午11.55.18.png" alt=""></p>
<p><img src="Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose-屏幕快照 2019-01-28 下午4.16.00.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/28/Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/28/Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation/">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1802.02611.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1802.02611.pdf</a><br>这是DeepLab系列的第四篇文章，DeepLab V3+，这篇论文的主要内容主要是在DeepLab V3的基础上接了一个decoder形成一个hourglass的结构结果直接出原图的resolution，这相比前面版本直接插值要更好一点，整个结构中encoder就直接用的DeepLab V3的结构，输出结果concat到一起之后会和低层的网络特征直接concat，低层的特征通过1x1卷积来降维，简单的示意图：<br><img src="Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation-屏幕快照 2019-01-26 上午11.18.27.png" alt=""></p>
<p>论文中另外一个主要内容是depthwise separable convolution的使用，论文中用深度可分离的空洞卷积来替换掉原有xception中的max pooling 层，主要也是出于模型速度的优化，整体的网络结构：<br><img src="Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation-屏幕快照 2019-01-26 上午11.29.11.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/28/Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/28/Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/">Rethinking Atrous Convolution for Semantic Image Segmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1706.05587.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.05587.pdf</a><br>这篇论文是DeepLab系列的第三篇论文DeepLab V3. 相比较DeepLab V1和V2主要在研究如何更好的运用空洞卷积, 同时也去掉了之前的DenseCRF后处理模块。论文中主要涉及了两个方面一个是串行的连接空洞卷积，因为相比较传统的直接用conv层来增加网络的深度会导致抽象到最后一层会丢掉很多的原始信息，利用空洞卷积可以一定程度上优化这个问题。另一方面是在ASPP的基础上优化空洞卷积的使用，作者通过实验发现，如果一味的通过扩大空洞卷积的rate来扩大感受野，有时候并不能得到很好的结果，比如下图就说明当rate足够大的时候其实卷积核有效的参数量只有一个：<br><img src="Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation-屏幕快照 2019-01-26 上午11.08.28.png" alt=""></p>
<p>所以最后作者在ASPP模块中就直接用全图的global average pooling来替代空洞卷积来获取比较大区域的特征：<br><img src="Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation-20181221233024.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/28/Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/28/DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs/">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener">https://arxiv.org/abs/1606.00915</a><br>DeepLab系列的第二篇论文，主要是在DeepLab V1的基础上提出了ASPP的结构来做多尺度的问题。<br>在DeepLab V1中作者其实也提到了multiscale的问题，当时论文中提到的方法就是对输入进行多次rescale，那么这种方法很显然太naive，并且也会带来比较大的运算量。因此DeepLab V2中作者就借鉴SPP的方法提出了ASPP的结构来解决多尺度的问题，ASPP本质就是用不同rate的空洞卷积来实现不同的感受野这样就可以覆盖比较多scale的物体，具体的应用逻辑论文中也给出了具体的结构：<br><img src="DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs-屏幕快照 2019-01-25 下午11.01.55.png" alt=""></p>
<p><img src="DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs-20181220234952.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/28/DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/24/SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS/">SEMANTIC IMAGE SEGMENTATION WITH DEEP CON- VOLUTIONAL NETS AND FULLY CONNECTED CRFS</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/DeepLab/">DeepLab</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1606.00915.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1606.00915.pdf</a><br>Semantic Segmentation比较经典的DeepLab系列的第一篇，DeepLab V1，主要利用FCN和DenseCRF来实现比较出色的segmentation效果，DeepLab V1的整个Pipeline整体可以分成两个部分: FCN得到相对比较粗糙的分割结果，DenseCRF在FCN结果的基础上对边缘进行Refine得到相对比较分明的物体分割轮廓。<br>在FCN部分主要是基于VGG16，将VGG16中FC换成Conv变成全卷积网络，而为了保持相对比较dense的feaure map,从VGG16原来的32倍下采样提高到8倍下采样，这个可以通过改变stride再加上空洞卷积来保持感受野，空洞卷积的具体示意图如下，网上也可以找到比较详细的解释：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午8.53.19.png" alt=""><br>最后在inference的时候可以直接从8倍下采样的结果直接插值回原图resolution.</p>
<p>第一阶段FCN得到的pixel wise的结果无疑是相对比较粗糙的，论文中作者也给了一些对比的图,FCN的结果相比较gt还是有比较大的差距的：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.21.37.png" alt=""><br>论文中提及的FC CRF本身是针对轮廓的处理提出来的，论文中也给出了具体的计算公式，x<sub>i</sub>为pixel的label，I为pixel的颜色轻度，整体逻辑是基于pixel的位置和pixel的颜色强度尽量让“相似”的pixel归为一类，让“不同”的pixel归为另一类，这样就可以在物体的边缘区域有一个比较好的划分：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.29.59.png" alt=""><br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.30.02.png" alt=""></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在VOC2012上的测试结果，MSc代表多尺度的特征融合，论文中是对网络的最后4个mas pooling层进行特征融合，FOV则是和空洞卷积有关：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.38.17.png" alt=""><br>作者也过空洞卷积的参数设置进行了简单的实验：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.40.38.png" alt=""><br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.41.21.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/24/SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/24/PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model/">PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Pose/">Pose</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1803.08225" target="_blank" rel="noopener">https://arxiv.org/abs/1803.08225</a><br>这篇论文其实和之前读的G-RMI那篇论文是同一个作者，G-RMI是top down的逻辑，而这篇论文是bottom up的逻辑。论文所提方法同时在做pose estimation和instance-level person segmentation两个task。pose estimation主要是通过预测点对之间的向量来做group，seg则是主要借鉴embedding的逻辑通过设计新的loss函数来优化效果。<br>下图是论文所提方法的整个pipeline，两个大的分支，pose + seg：<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-03 下午11.18.23.jpg" alt=""></p>
<p>先说一下论文中对pose estimation的做法，这一部分主要涉及三个内容：</p>
<ul>
<li><strong>heatmap生成</strong>：k个joint， k channel的heatmap，同时针对每一个joint人为围绕这个joint设置一个半径为R的圆（论文中R=32pixel），圆内的点为1，其余为0，构造一个分类任务来监督heatmap的生成。</li>
<li><strong>short-range offset</strong>: 把G-RMI那篇论文中用到的方法拿过来应用，主要是在heatmap的基础上还会预测在joint半径R pixel内的点与当前joint点的offset。然后再整合heatmap和offset（hough voting）得到最后的位置信息具体如上图。</li>
<li><strong>mid-range offset</strong>: 这个内容主要是用来对多人做group的。主要是想通过预测两个joint之间的offset向量在inference的进行instance的划分，但是在整张图中预测两点之间准确的offset是很难的，所以作者就把这个问题转换成两个joint半径R区域内相对应的点之间的offset和圆内点到joint之间的offset的加和来避免这样的问题，相当于用两个相对比较糙的结果来refine 最后的offset：<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-03 下午11.20.40.png" alt=""><br>论文中另外一个主要的部分就是关于seg的内容, 也引出了论文提到的第三个offset，long-range offset：<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-06 下午11.19.49.jpg" alt=""><br>论文所提做seg的方法主要是借鉴了embedding 的逻辑比如associate embedding方法，这些方法的一个主要思想就是设计一个loss使得属于同一个instance的点离的尽量近，不属于同一个instance的点离的尽量远，long-range offset就是作者在设计loss函数（距离函数）时提到的概念。long-range offset就是instance内的点到某一个joint的offset，上图画的比较清楚，具体计算distance的时候是：<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-06 下午11.30.27.png" alt=""><br>其中G(x) = x + L(x), L(x）就是long-range offset.</li>
</ul>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>Pose Estimation:<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-06 下午11.32.43.png" alt=""><br>Segmentation:<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-06 下午11.33.19.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/24/PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/22/Fully-Convolutional-Siamese-Networks-for-Object-Tracking/">Fully-Convolutional Siamese Networks for Object Tracking </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/SOT/">SOT</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1606.09549" target="_blank" rel="noopener">https://arxiv.org/abs/1606.09549</a><br>Siamese-FC v1 , 利用Siamese网络来做跟踪，论文中把跟踪的任务定义为两个图像patch之间的相似度，通过计算两个图像patch之间的相似度来定位物体，通过多次rescale 输入图片来实现多尺度物体的跟踪。<br>下图是论文中给出的Siamese-FC v1基本的示意图：<br><img src="Fully-Convolutional-Siamese-Networks-for-Object-Tracking-d0c371c53a45ca34e71ad8a64fdc055b53e0f958.png" alt=""></p>
<p>网络的第一个分支为目标object，输入被resize到127x127，经过全卷积网络之后得到6x6x128的输出。<br>网络的第二个分支为搜索的图片，经过同样的全卷积网络之后也会得到相对应的输出比如上图的22x22x128, 这两个分支的输出最后会通过 cross- correlation 操作(第一个分支的输出会作为第二个分支输出的kernel进行卷积操作)得到最后的输出，比如上图的17x17x1，那么直观上来看对于最后输出的score map上的每一个点其实就对应到原图和目标object图像同样大小的区域，而score map上这个点的取值就可以理解为原图中这块区域和目标object之间的相似度，那么最后相似度最高的点就被定位为目标object在当前帧上面的位置。<br>至于具体训练的时候论文中也提到了一些细节，比如正负例的定义，score map中落在中心半径R范围内的点被定义为正例label为1，其余为-1. 输出的score map会利用cosine window来抑制距离中心比较远的点。多尺度物体的检测则是直接通过rescale输入图片来实现的。<br><img src="Fully-Convolutional-Siamese-Networks-for-Object-Tracking-9ec817caf543acb7d9a52ed7008a1a31d9679a8d.jpeg" alt=""><br>一些实验结果,SiamFC-3s代表进行三次scale缩放：<br><img src="Fully-Convolutional-Siamese-Networks-for-Object-Tracking-b23460370758b3eca2601241e9fcda1eec173e47.png" alt=""><br><img src="Fully-Convolutional-Siamese-Networks-for-Object-Tracking-f83536950b93ff0983f49a137ba3b018ba9c7fd1.png" alt=""><br>Siamese-FC因为是从分类的角度来做位置的定位的，所以感觉框的精度会比较不够精确</p>
</div></div><a class="button-hover more" href="../../2019/01/22/Fully-Convolutional-Siamese-Networks-for-Object-Tracking/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/20/IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks/">IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1806.00178" target="_blank" rel="noopener">https://arxiv.org/abs/1806.00178</a><br>IGCV3，BMVC2018，在IGCV2基础上进行的改进，主要引入bottleneck的网络结构来改善IGCV2网络的信息交互，从而进一步提高网络效果。<br>在IGCV2中曾提到过一个互补条件：前面一级group convolution里不同partition的channel在后面一级的group convolution要在同一个partition里面，那么follow这个逻辑设计的IGCV2网络结构的每一个输出channel、输入channel之间将有且仅有一条路径可以连接，示意图如下，这就会导致网络过于稀疏不一定有利于网络的整体性能：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-a0f037095b27476caa3cf49ca30929b2923f2315_1_690x253.jpeg" alt=""></p>
<p>那么在IGCV3网络设计中借助bottleneck结构提出了super-channel的概念，每一个super-channel其实就是一组channel，first group convolution是1x1的卷积，它把网络变宽，second group convolution是3x3的spatial conv，third group convolution 同样是1x1的卷积，它又把网络结构压缩到原来的宽度，整个过程中的permutation和IGCV1、IGCV2的逻辑是一致的，这样设计之后每一个输出和输入的channel之间都会有多条路径可以交互，这样就优化了IGCV2网络设计中的问题，具体逻辑如下图：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-dfcbf9fea9f926a00f49a63e9242ce7a83ec10e8_1_690x316.png" alt=""><br>IGCV1、IGCV2、IGCV3在CIFAR上面的对比：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-2460c1c050b642ec41193bbc0f29770f65933ac2_1_690x242.png" alt=""><br>和MobileNetV2的对比：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-屏幕快照 2019-01-19 上午11.38.42.png" alt=""><br>和其他现有针对移动平台设计的网络对比：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-屏幕快照 2019-01-19 上午11.39.27.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/20/IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/20/IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks/">IGCV2: Interleaved Structured Sparse Convolutional Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1804.06202" target="_blank" rel="noopener">https://arxiv.org/pdf/1804.06202</a><br>IGCV2, CVPR2018, 主要把IGCV1中提到的group convolution进行了推广，将卷积操作变的更加稀疏，以此来达到减少冗余的目的。<br>在IGCV1论文中作者把IGC操作抽象成如下的表达式，x为输入，x前面的表达式整体是一个dense convolution kernel：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-f1ed32b809c7dda6f6c1b5a8834e594e6c0b99ce.png" alt=""></p>
<p>本论文中提到的Interleaved Structured Sparse Convolution就是它的一般表达：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-c181db945a7431adc7c182e16398ada2cc287be4.png" alt=""><br>那么为了保证输入x前面的表达式同样是dense convolutional kernel，论文中做了条件约束：前面一级group convolution里不同partition的channel在后面一级的group convolution要在同一个partition里面, 那么follow IGCV1的结构设计第一级group convolution为spatial conv (3x3)其余为1x1:<br>下图是wangjingdong的PPT上对IGCV2结构的描述示意图，比论文里面的示意图要清晰直观很多，其实就是把IGCV1的第二个group convolution再划分成多个1x1的group convolution操作 ：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-6268924f405c5224270e8fe5c80de272c04dbfa2_1_690x286.png" alt=""><br>实验Group Convolution的组数对结果的影响：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-b493611cc06c1996f7187b42c8cceaffa7452a20_1_539x500.png" alt=""><br>和其他一些网络模型的结果比较：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-e6c4c428b992b470ad7c2655ef49528184b321e9_1_608x500.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/20/IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/20/Interleaved-Group-Convolutions-for-Deep-Neural-Networks/">Interleaved Group Convolutions for Deep Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1707.02725" target="_blank" rel="noopener">https://arxiv.org/abs/1707.02725</a><br>下图是IGCv1的整体示意图：<br><img src="Interleaved-Group-Convolutions-for-Deep-Neural-Networks-7d3b43e53abfbc53dc3dfca907e8935d56f3fcf5.jpeg" alt=""></p>
<p>具体做法，首先整个IGC block会被分成两个部分，primary group convolutions 和 secondary group convolutions：<br>primary group convolutions的输入首先会被分成L个partition，每个partition共M个channel， 每个partition内部进行一般的spatial conv操作，最后的输出还会是ML个channel，那么在secondary group convolutions阶段，上一阶段的输出会先被shuffle一下然后再划分为M个partition，每个partition L个channel，对于第i个partition它其中的M个channel分别来自于前一个阶段的每个partition的第i个channel组成，然后每个partition内部进行1x1的卷积操作，最后还是会输出ML个channel，这ML个打乱的channel会再次映射回输入时候的顺序, 整个过程被抽象成如下的卷积操作，P为序列化：<br><img src="Interleaved-Group-Convolutions-for-Deep-Neural-Networks-f1ed32b809c7dda6f6c1b5a8834e594e6c0b99ce.png" alt=""><br>除了IGC主要的逻辑以外论文中也花了比较多的篇幅在论文，IGC比常规的conv操作在同等参数量的情况下宽度更宽，论证比较简单具体可以参考论文，同时作者也通过实验证明了更宽的网络可以得到更好的结果：<br><img src="Interleaved-Group-Convolutions-for-Deep-Neural-Networks-9a13e111aef4563a2957a372c32d4f8136975302.png" alt=""></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>最后的一组实验结果<br><img src="Interleaved-Group-Convolutions-for-Deep-Neural-Networks-e51af6de7667412d2511a7cfb8d8d61c3d045247.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/20/Interleaved-Group-Convolutions-for-Deep-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/15/High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network/">High Performance Visual Tracking with Siamese Region Proposal Network</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Track/">Track</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2951.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2951.pdf</a><br>Siamese-RPN, CVPR2018一篇关于tracking的论文，论文所提方法的整个逻辑还是基于孪生网络来做，整体也可以理解为对Siamese-FC结构的改进，下图是Siamese-RPN的整个Pipeline：<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-15 下午11.31.31.png" alt=""></p>
<p>Siamese-RPN网络的整个逻辑还是分成两个分支, 上面一个分支针对关键帧，下面一个分支针对检测帧，两个分支本身的参数是共享的，Siamese网络完成对两张输入图片的特征提取，两个分支的输出还会经过上图的Region Proposal Network网络来整合两者的信息.<br>具体做法和RPN网络类似，前一层Siamese网络的每一个分支都会接入两层的conv然后分成两个分支，一支为cls，另一支为reg，关键帧的输出如上图的4×4×(2k×256) 和 4×4×(4k×256)，这两个输出会分别作为对应的检测帧输出的kernel来进行卷积运算（上图中的*号操作）。这两者最后的输出就是检测帧最后的检测结果。<br>那么在得到最后预测的结果时针对tracking的应用场景，作者也提出了一些选框的策略，一是默认物体的移动速度不快，所以只选取靠近中心的anchor：<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.43.46.png" alt=""><br>另外一个做法就是对当前得到的proposal进行重新排名，比如利用cosine window来对距离重点点比较远的候选框进行抑制或者同样基于物体移动速度不快的假设可以默认物体的size变化也不大所以对scale进行penalty，形变比较大的得分会被抑制，具体打分如下，k为超参数，r为proposal的ratio，r’为上一帧的ratio，s和s’则是对应的整体的ratio（输入图片多尺度scale），论文中应该采取的是后者：<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.48.02.png" alt=""><br><strong>实验结果</strong><br>VOT2015:<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.53.58.png" alt=""><br>VOT2016:<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.55.21.png" alt=""><br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.55.25.png" alt=""><br>SiamRPN网络相比SiamFC直接出了框的具体坐标位置，感觉对于跟踪的任务更加合理，也避免了SiamFC相对比较繁琐的后处理，在测试集上的效果也是要比SiamFC好一些。</p>
</div></div><a class="button-hover more" href="../../2019/01/15/High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/14/Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks/">Learning to Track at 100 FPS with Deep Regression Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Track/">Track</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1604.01802" target="_blank" rel="noopener">https://arxiv.org/abs/1604.01802</a><br>论文主要提出了GOTURN的tracking框架，整体还是比较naive的一些tracking逻辑，下图是GOTURN整个的pipeline:<br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.26.02.png" alt=""></p>
<p>GOTURN整个框架是针对连续的两帧来做的，整个框架因此也主要分成两个分支，分别针对当前帧和前一帧，给定当前帧(t)和前一帧(t-1),t-1帧的物体是已知的，比如物体框中心为center(c<sub>x</sub>, c<sub>y</sub>),长宽分别是w、h。那么对于下面一个分支首先会以center为中心，长宽分别是k1w, k1h去抠图，k1这个系数用于控制context信息的量。那么对于上面一个分支，会从同样的位置center作为中心，长宽分别是k2w,k2h去抠图作为当前帧的搜索区域。论文中也提及了一下通常k2 = 2。这两个分支的输出最后会被concat到一起送入一个3层的FC网络，每层FC有4096个神经元，每个分支本身的结构是CaffeNet的前五层。</p>
<p>那么在具体训练的时候，论文中主要提及了一些数据增强的细节，比如论文就直接说明GOTURN本身在训练的时候就是针对低速运动的物体进行的设计，对于高速运动的物体并不能cover。至于GOTURN对低速运行物体的跟踪也跟论文中提到的数据增强方法有关，模型在具体训练的时候会构造这样的pair对：<br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.43.16.png" alt=""><br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.43.23.png" alt=""><br>其中控制偏移幅度的变量delta服从Laplace分布。</p>
<p><strong>具体的实验结果</strong><br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.46.49.png" alt=""><br>Ablation Study:<br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.47.31.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/14/Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/24/Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Pose/">Pose</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<br>论文所提的模型结构整体是一个bottom-up的结构，F是通过backbone网络（论文中用的是vgg）提取的特征，整个结构分成两个分支，一个分支用来预测joint的heatmap，另一个分支可以理解为用来预测joint之间的联系（Part Affinity Field），实际上是joint对应的向量：</p>
</div></div><a class="button-hover more" href="../../2018/12/24/Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/24/Towards-Accurate-Multi-person-Pose-Estimation-in-the-Wild/">Towards Accurate Multi-person Pose Estimation in the Wild</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time></div><div class="post-content"><div class="main-content content"></div></div><a class="button-hover more" href="../../2018/12/24/Towards-Accurate-Multi-person-Pose-Estimation-in-the-Wild/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/24/AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones/">AI Benchmark: Running Deep Neural Networks on Android Smartphones </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Mobile/">Mobile</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Platform/">Platform</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Hardware/">Hardware</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1810.01109" target="_blank" rel="noopener">https://arxiv.org/abs/1810.01109</a><br>关于手机各种硬件平台一篇比较好的科普论文，论文主要提出了一个AI Benchmark来客观评估各个手机平台包括华为海思、高通、联发科、三星、谷歌Pixel等在标准CV任务比如识别、分割上面的具体表现。</p>
<p>论文主要介绍了华为海思、高通、联发科、三星、谷歌Pixel等手机平台针对AI任务所做的具体优化：<br><img src="AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones-9992f821aff8fcd693990d97c8aa896813ce65b8.png" alt=""></p>
<ul>
<li>高通：SNPE(Snapdragon Neu- ral Processing Engine)，支持大多数主流的框架比如Caffe、Tensorflow等，需要INT8量化。</li>
<li>华为海思：NPU(Neural Processing Unit), 当前总共有两款NPU，970和980。目前只支持Caffe和Tensorflow平台，论文中也提到了目前应该只支持16-bit float。</li>
<li>联发科：APU(AI Processing Unit), 需要INT8量化，论文上写只有P60，现在应该还有其他的APU发布。框架的话支持Caffe、Tensorflow、ONNX。</li>
<li>三星：VPU(Vision Processing Unit), 主要用在手机摄像头，目前没有提出对AI任务的针对性支持</li>
<li>Google：IPU( Image Processing Unit ),支持16-bit int和8-bit int，目前也没有提出对AI任务的针对性支持。</li>
</ul>
<p>论文另外一个主要内容就是介绍了其”AI Benchmark”, 至于具体的任务和具体的模型下表给了一个相对比较仔细的介绍，总共是下表的8个任务外加一个Memory Limitation测试共9个测试任务:<br><img src="AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones-b0846738e8d03725f22af52704abcfd5224cbde1.png" alt=""></p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones-9d9e02feef163a08de25b0dfa8efbc42d96c5207.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/12/24/AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/24/Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint/">Bottom-up Pose Estimation of Multiple Person with Bounding Box Constraint </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Pose/">Pose</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1807.09972" target="_blank" rel="noopener">https://arxiv.org/abs/1807.09972</a><br>一篇基于Bottom Up逻辑的pose estimation论文，但是实际上和刚读的PRN那篇论文比较类似可以理解为Top Down + Bottom Up结合的方法，论文所提的方法主要是OpenPose再结合人体框的检测来做多人的pose estimation。</p>
<p>论文所提方法的整个逻辑如下图,整体可以理解为OpenPose再加上人体BBox约束来提升group的效果：<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-ee790656fab70c4c1822e95038c1f8b7543c7177_1_690x279.jpg" alt=""></p>
<ul>
<li><strong>CNN Regression</strong>: 网络这个分支可以直接理解为OpenPose的逻辑，只是作者再OpenPose的基础上做了一些简单的修改比如更换backbone等，整体结构和逻辑和OpenPose基本是一致的：<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-c76f41b54234100ad9b73127621a3323fcb82b02.png" alt=""><br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-5071f6a4a4983012e8408382a76ed443faa1d773.png" alt=""></li>
<li><strong>Pose Parsing</strong>: 主要是利用CNN Regression分支的结果来解析pose，本论文所提方法在做pose parsing 的时候是每个人体框分别来做，不同于PRN，本文所提方法对于一个框是可以出多个人的结果的，具体贪心逻辑如下：<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-34dfefcffcd927265e9239e373f68117aea2cac8.png" alt=""></li>
<li><strong>Pose NMS</strong>：主要是制定了一些规则来定义Pose的Confidence、Pose的距离：<br>Confidence:s1是pose所有点confidence均值，s2是pose所有connection confidence的均值，B<sup>‘</sup>是pose最小外接矩形面积，B是bbox面积<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-ce3bf61db73b6e97c08606efbe034d5ae04912c7.png" alt=""><br>distance：<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-63ebd7be02c976cf5488aa5104f10dc6e99b7fde.png" alt=""><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2>作者只和OpenPose、CPM做了比较。。。。<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-8c8f1ddc7edf786ac0fc6c02cefe7f968e295bd0.png" alt=""><br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-35821768d390b09996b5101a7f1b748ccd7366fa.png" alt=""><br>Top Down和Bottom Up分别都有各自比较明显的缺点和优点，目前也陆续有两者结合的研究工作出现，感觉是一个可以去研究的内容。</li>
</ul>
</div></div><a class="button-hover more" href="../../2018/12/24/Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/24/MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network/">MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Pose/">Pose</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1807.04067" target="_blank" rel="noopener">https://arxiv.org/abs/1807.04067</a><br>ECCV2018一篇利用bottom up方法做pose estimation的论文。论文所提出的方法主要是分别利用两个分支一个分支用来检测人体框，另一个分支用来出关键点的heatmap。然后再利用一个网络来merge 两个分支出的结果，将关键点分别映射到对应的人体框上实现多人的姿态估计。</p>
<p>下图是本文所提方法的大体Pipeline，整个pipeline可以分成三个部分，人体关键点检测、人体框检测和关键点的聚类（PRN）：<br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-b80970b3e1b91db9992796fdada5d5cacddc1531.jpg" alt=""></p>
<ul>
<li><strong>Backbone</strong>：Resnet + FPN</li>
<li><strong>关键点检测</strong>：整个分支结构如下图，比较直观，L2 loss，总共出K + 1 个heatmap，+1为seg的结果：<br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-a42ffc7c2d336f7a095fc898d260ef90c728a7f6.png" alt=""></li>
<li><strong>人体框检测</strong>：就是直接上的RetinaNet<br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-96f37d11a64105bce2440ed6b6955231dd1a6949.png" alt=""></li>
<li><strong>Pose Residual Network (PRN)</strong>：这一部分内容是论文的核心，主要是利用人体框将关键点检测的结果映射到对应的instance上。具体做法是在关键点检测的结果上利用对应人体框的位置crop出一样大小的patch，这样就可以得到K x W x H大小的feature map，K为关键点个数，W、H为对应人体框的size，然后把这个feature map resize到固定大小(36 x 56)作为PRN的输入。那么对于上图的c、d这种同一个框有多个人关键点overlap的情况，主要是利用一个多层感知机（residual multilayer perceptron）来把人体框对应instance的关键点提取出来，具体逻辑如下图：<br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-c340c5ea72c9c75d031dbcba3c8b569d4e393023.jpg" alt=""><br><strong>结果：</strong><br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-9255a515da09097264bb0c17a00cd420a3a16291.png" alt=""><br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-d87795e947cb3e80cf5311605ec05df40db408f6.png" alt=""></li>
</ul>
<p>这篇论文另一个比较重要的点是虽然整个网络的pipeline比较多但是inference的速度还是比较快的，对于典型的COCO图片（～3人）可以达到23FPS的速度，用来merge的PRN网络输入比较小层数也不深。</p>
</div></div><a class="button-hover more" href="../../2018/12/24/MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/19/Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment/">Two-Stream Transformer Networks for Video-based Face Alignment</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Face/">Face</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://ieeexplore.ieee.org/document/7999169" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/7999169</a><br>这是一篇TPAMI2018的论文，论文主要的研究内容是视频场景下的facial landmark 定位的问题。论文的motivation也比较直观，和之前看过的RED-Net很类似，就是想借助于视频流提供的temporal信息再加上静态图片的spatial信息来优化视频场景下的facial landmark问题，比如pose、遮挡等。</p>
<p>下面这幅图是论文中给出的整个框架的示意图，画的比较直观论文中所谓的Two-Stream就是Spatial stream 和 Temporal stream这两个branch，两个分支的输出最后通过不同的权重整合到一起作为最后的输出：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-f1e5f5d479db4e5b4c7d609bcd2acce9a43a4002_1_690x241.jpg" alt=""></p>
<ul>
<li>Spatial Stream: 这个分支通常是和一般的静态图片处理方式是一致的，论文中所提方法分为两个部分，sampler和regression， sampler是从原图中进行采样local patch的过程，local path就是每个landmark点周围的dxd的区域，论文中取d=26。regression则是一个标准的CNN网络，接受local patch为输入回归具体的landmark坐标（实际上回归的是offset），为了达到比较高的精度论文在实际做的时候其实做了两阶段的cascade：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-6c1b39b23cda6d28051df18db474abb7ab3e5c33_1_690x83.png" alt=""></li>
<li>Temporal Stream:这一个分支通常是视频场景下facial landmark定位特有的处理过程, 为了对视频的时序信息进行处理，不少的论文都是采用RNN模型来进行处理，本文也不例外，temporal stream分支的输入是一段连续的帧，这些帧首先会经过encoder进行处理提取图片的一些context的信息，然后会根据时序依次输入到两层RNN中，第一层RNN layer通常来编码一些整体的特征信息，第二层RNN layer则用来编码一些变化的时序信息例如pose等，最后RNN的输出会经过decoder映射回输入的size保持原有的spatial 信息然后再经过比较小的回归网络得到最后的landmark输出：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-e3ebddb40d960442b71b63296cf3ecd654878f7a_1_376x500.jpg" alt=""></li>
</ul>
<p>论文主要在300-VM和TF（Talking Face）两个数据集进行了实验，相比较之前的REDN、TCDCN模型都有提升：<br>300-VW上的表现：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-d371c0da62673b1d319fc818b8ca472efe4cf8c1_1_690x187.png" alt=""><br>视频场景下的facial landmark定位相比静态图片的facial landmark定位增加了时序信息可以利用，目前的研究所用方法也比较类似，之前看过的REDNet和这篇TSTN都是通过CNN+RNN的逻辑来整合spatial信息和temporal信息，感觉这种信息融合的方法还需要仔细的去研究。</p>
</div></div><a class="button-hover more" href="../../2018/12/19/Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/19/DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks/">DropFilter: A Novel Regularization Method for Learning Convolutional Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Regularization/">Regularization</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1811.06783" target="_blank" rel="noopener">https://arxiv.org/abs/1811.06783</a><br>论文主要提出了一种新的Regularization方法DropFilter，主要是对卷积核进行抑制，具体实现的时候也比较简单，只要对卷积核和bias生成对应的0 - 1mask(Bernoulli分布),然后对应和kernel相乘即可：</p>
<p><img src="DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks-6cc74f13d759b1cbb0c4a864d637b520623e0053_1_690x465.png" alt=""><br>在DropFilter的基础上作者还提出了DropFilter-PLUS方法，主要是针对卷积操作过程中的每一个位置都进行一次独立的DropFilter操作，那么对于一个224x224的输入需要计算222x222次mask操作(3x3conv)，计算量比较大，作者在论文中分析具体的conv操作(论文中提及的alx,y)最后通过对卷积的输出进行mask来等价，这就大大加速了DropFilter-PLUS的操作：<br><img src="DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks-9fa3f026fff5f15f159490c18a254286219fda82_1_585x500.png" alt=""><br>DropFilter和Dropfilter-PLUS都有一定的道理，并且简单易实现</p>
</div></div><a class="button-hover more" href="../../2018/12/19/DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/19/Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images/">Robust Face Detection via Learning Small Faces on Hard Images</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Face/">Face</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1811.11662.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.11662.pdf</a><br>论文主要是想解决人脸检测中的小脸问题，论文的motivation其实和SNIP很像，让网络去学习一个相对固定的scale，比如在本论文中anchor大小被设置为固定的16x16，32x32，64x64三个。论文主要的内容有两个部分，一部分是提出来的检测网络，另一部分就是hard image mining。</p>
<p>论文所提的检测网络backbone采用VGG16，在接入cls和reg前经过dilation conv来获得不同的感受野，其他没有特别的内容:<br><img src="Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images-9add084a4879a4cde2af6ba16849afa33dc4a245_1_690x283.png" alt=""><br><img src="Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images-e9d7b5e89400c3d6572e883406879031b064fcd8_1_482x500.png" alt=""><br>而hard image mining是image level的，作者针对图片的难易程度定义了一个变量WPAS, 简言之是综合proposal的IOU以及score信息来量化图片的难易程度，WPAS&gt;0.85被定义为简单的图片：<br><img src="Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images-5d2afb9dc7cdd4a787fc08fb6b82bda9f305572f_1_690x106.png" alt=""><br>最后在训练的时候对于当前epoch会借助前一个epoch的信息把训练数据都划分成easy和hard两类，再从easy中以0.7的概率剔除掉一些图片，剩下的图集作为当前epoch的训练数据来训练模型。</p>
</div></div><a class="button-hover more" href="../../2018/12/19/Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/17/Deep-Layer-Aggregation/">Deep Layer Aggregation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1707.06484.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1707.06484.pdf</a><br>CVPR2018的一篇关于layer aggregation的论文，论文的motivation是作者觉得目前常见的aggregation方式（FPN、U-Net…）比较shallow，作者希望利用更加deeper的连接方式来更好的融合特征。论文中作者分别提出了IDA和HDA两种连接方式。IDA应用在stage之间，HDA应用在block之间。</p>
<p>下图是论文中对IDA和HDA给出的直观图示，下图的c是IDA（Iterative Deep Aggregation），整体和FPN的连接方式比较类似，只是方向相反，浅层的特征被不断的refine与高层特征相融合。下图的d，e，f对应着HDA（Hierarchical Deep Aggregation），d是HDA最原始的表达，整体是一个树状结构，跨越不同层级的特征分层次进行特征融合，e则是在d的基础上将前面节点的父亲节点与当前节点一同考虑进行特征融合。f则是作者出于降低模型复杂度的角度将e同一层级的节点进行merge：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.41.22.png" alt=""><br>下面这幅图是论文中将IDA和HDA合并到一起具体的模型应用，也就是论文标题的Deep Layer Aggregation模型，橙色的连线是IDA的逻辑，红色框内部的连接是HDA的逻辑：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.41.53.png" alt=""><br>而对于一些比如seg的‘image-to-image’任务，只需通过增加简单的插值操作就可以实现：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.42.30.png" alt=""><br>为了验证DLA的有效性作者还是做了比较多的实验的，作者分别在Classification、Fine-grained Recognition、Seg、Boundary Detection几个主流的cv task上都做了实验，都有不同程度的提升：<br>分类的实验：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.48.56.png" alt=""><br>识别的实验：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.49.01.png" alt=""><br>论文中作者提出的IDA和HDA感觉还是很合理的，作者同时还提及residual connection在实验时对比较深的网络是有效的，对于比较浅的网络是有负面作用的。</p>
</div></div><a class="button-hover more" href="../../2018/12/17/Deep-Layer-Aggregation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/15/Deep-Mutual-Learning/">Deep Mutual Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Mutual-learning/">Mutual learning</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Classification/">Classification</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://cn.arxiv.org/pdf/1706.00384v1" target="_blank" rel="noopener">http://cn.arxiv.org/pdf/1706.00384v1</a><br>论文主要在讲mutual learning相互学习，deep mutual learning整体感觉和model distillation还是比较像的,只是不是用训练好的大网络来带小网络而是用一些网络相互同步学习来提高模型整体的效果:<br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.49.05.png" alt=""></p>
<p>对于其中的单个网络除了本身分类的cross entropy loss外还会涉及到KL散度来度量两个网络预测结果之间的loss（K是网络个数）：<br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.49.47.png" alt=""><br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.50.11.png" alt=""><br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.50.24.png" alt=""><br>补充一些作者针对mutual learning的有效性做的一些探究：</p>
<ul>
<li>作者认为DML相对独立的模型得到了wider minima，因此更加鲁棒，作者设计了一组实验对网络的参数增加了一些噪声，下图是增加噪声前后的结果，正常比较的时候两者相似，增加噪声扰动之后DML效果相对更好：<br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.52.25.png" alt=""></li>
<li>K个模型的mutual learning通常可以有两种表现形式，一个就是当前这个模型和其余的K-1个模型分别进行KL distance计算，另一个就是当前模型只要和其余K-1个模型的预测结果均值来进行KL distance计算，作者实验发现后者其实效果会更差，作者认为是多模型结果的融合影响了一些网络细节的teaching signal:<br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.51.58.png" alt=""></li>
</ul>
</div></div><a class="button-hover more" href="../../2018/12/15/Deep-Mutual-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/14/A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition/">Center Loss - A Discriminative Feature Learning Approach for Deep Face Recognition</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Loss/">Loss</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://ydwen.github.io/papers/WenECCV16.pdf" target="_blank" rel="noopener">http://ydwen.github.io/papers/WenECCV16.pdf</a><br>这篇论文主要的贡献就是提出了Center Loss的损失函数，利用Softmax Loss和Center Loss联合来监督训练，在扩大类间差异的同时缩写类内差异，提升模型的鲁棒性。</p>
<p><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image002.png" alt=""><br>为了直观的说明softmax loss的影响，作者在对LeNet做了简单修改，把最后一个隐藏层输出维度改为2，然后将特征在二维平面可视化，下面两张图分别是MNIDST的train集和test集，可以发现类间差异比较明显，但是类内的差异也比较明显。<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image003.png" alt=""><br>为了减小类内差异论文提出了Center Loss：<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image004.png" alt=""><br>C<sub>yi</sub>就是类的中心点特征，Cyi的计算方法就是yi类样本特征的均值，为了让center loss在神经网络训练过程中切实可行，C<sub>yi</sub>的计算是对于每一个mini-batch而言，因此结合Softmax Loss，整个网络的损失函数就变成了， λ用来平衡这两个Loss：<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image005.png" alt=""><br>用同样的网路结构只是将Softmax Loss替换成Center Loss作者在MNIST数据集上做了同样的实验，对于不同的λ值得到了如下可视化结果可以发现Center Loss还是比较明显的减小了类内差异同时类间差异也比较突出。<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image006.png" alt=""><br>在公开数据集上的表现：<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image007.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/12/14/A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/14/CRAFT-Objects-from-Images/">CRAFT Objects from Images</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1604.03239" target="_blank" rel="noopener">https://arxiv.org/abs/1604.03239</a><br>CVPR2016的一篇论文。首先CRAFT代表 Cascade Region proposal network And FasT-rcnn，本论文主要想解决RPN网络生成的区域不太精确的问题，比如对于一些外观复杂度较低的事物如树木，会因为RPN网络产生的背景区域的存在导致比较难检测或者产生FP，因此作者尝试利用cascade的方式来解决这样的问题。<br>CRAFT模型可以分成两个部分：</p>
<ol>
<li>Cascade Proposal Generation：这一部分同样可以分成两个部分，RPN和FRCN，FRCN其实就是一个Fast R-CNN网络，<br>两个网络分别训练，RPN网络生成相对粗糙的区域，FRCN用RPN的输出作为输入对RPN的结果进行进一步的refine，<br>提高了候选区域的质量也减少了背景框：<br><img src="CRAFT-Objects-from-Images-image002.png" alt=""></li>
<li>Cascade Object Classification：Cascade Object Classification部分由两个FRCN网络级联而成，Cascade Proposal Generation部分的输出会作为FRCN1<br>的输入，FRCN1的输出摒弃掉“背景”类作为FRCN2的输入，实现细节上两个模型复用参数。这里需要注意的是，作者认为为了学习类间的细微差别，<br>模型复用了被Fast RCNN抛弃的 One-vs-Rest的分类方式，用N个二分类的交叉熵损失函数的和作为最终的loss。<br><img src="CRAFT-Objects-from-Images-image003.png" alt=""><br>最终在VOC上面的表现效果：<br><img src="CRAFT-Objects-from-Images-image004.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2018/12/14/CRAFT-Objects-from-Images/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/14/Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network/">Weighted Channel Dropout for Regularization of Deep Convolutional Neural Network</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Regularization/">Regularization</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://home.ustc.edu.cn/~saihui/papers/aaai2019_weighted.pdf" target="_blank" rel="noopener">http://home.ustc.edu.cn/~saihui/papers/aaai2019_weighted.pdf</a><br><strong>【Summary】</strong>AAAI2019的一篇关于Regularization的论文，整体感觉可以理解为SENet思想在Regularization中的应用。论文中作者提出了Weighted Channel Dropout(WCD)的逻辑（为每一个channel计算权重、构造取舍的概率…）来对channel进行选择性的DropOut。</p>
<p><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.03.34.png" alt=""><br>上面这张图是WCD的整个Pipeline，整个结构主要可以分成三个部分：</p>
<ul>
<li><strong>Rating Channels</strong>：这个模块主要是为每一个channel构造一个全权重，逻辑很简单就是Global Average Pooling：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.18.15.png" alt=""></li>
<li><strong>Weighted Random Selection (WRS)</strong>: 这一部分逻辑其实也很简单，作者定义了每个channel被选择的概率为：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.20.57.png" alt=""><br>那么在具体计算的时候是这样做的,r<sub>i</sub>是一个（0，1）之间的随机数，对于这两个计算方式的等价性，论文中作者给出了引用论文可以具体参考：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.21.58.png" alt=""><br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.23.52.png" alt=""></li>
<li><strong>Random Number Generation (RNG)</strong>: 这个可以理解为在WRS基础上的一个补充吧，在实际应用中有时间可用的数据量比较小，那么这种情况下通常会用pretrain的模型来初始化网络，那么作者认为这种情况下channel之间的差距会更大，可能只有少部分的channel会有比较大的响应，那么根据WRS选出来的channel可能也很像，所以针对WRS选中的channel依然会有1 - q的概率被抛弃,那么最终每一个channel的状态就和WRS、RNG都有关：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.30.54.png" alt=""><br>论文中取alpha为：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.31.35.png" alt=""><br><strong>一些实验结果</strong></li>
</ul>
<p><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.32.15.png" alt=""><br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.32.21.png" alt=""><br>作者还贴了一个和SENet比较的结果：</p>
<p><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.44.02.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/12/14/Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/14/Caffe-配置/">Caffe 配置</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Engineering/">Engineering</a></div></div><div class="post-content"><div class="main-content content"></div></div><a class="button-hover more" href="../../2018/12/14/Caffe-配置/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/11/Parallel-Feature-Pyramid-Network-for-Object-Detection/">Parallel Feature Pyramid Network for Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener">https://arxiv.org/abs/1612.03144</a><br>ECCV2018的一篇论文,这篇论文从某种程度上来说是为了解决小物体检测的问题，作者从feature map特征表示好坏的角度来分析目前常用检测模型的一些不足。论文分别可视化SSD、FPN、PFPNET（本文所提模型）对同样输入图片的feature map，从图中可以看出来SSD对物体的轮廓细节描述比较差，FPN对于一些遮挡物体的特征表示比较差。PFPNET则相对好一些，至于为什么好，以及这个结构设计的理由论文貌似并没有解释。</p>
<p><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image002.png" alt=""><br>论文的主要内容：</p>
<ol>
<li>Parallel Feature Pyramid Network for Object Detection （PFPNET）模型结构：整体结构和RetinaNet比较像，不同的是PFPNET的multi-scale是一个并行的结构，和RetinaNet中的feature layer之间是一个串的结构是不一样的。模型首先利用Base Model（论文中用的是VGGNET-16）得到PFPNET结构的输入（DxWxH），然后利用SPP产生不同scale的feature map（FP Pool），不同scale的feature map长宽分别以2倍大小减少，对于每一个scale分别再利用Bottleneck Layer完成特征的转化。最后就是特征的融合（MSCA），模型基本结构图：<br><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image003.png" alt=""></li>
<li>Multi-scale context aggregation（MSCA）：本文用的特征融合还是常见的concatenate方式，对于第n个分支最后的融合的特征Pn, 它有第一层FP Pool中与之对应的feature map以及第二层FP Pool中其他的N-1个feature map相融合而得到。然后对于最后的n个feature map分别进行cls + reg的任务即可。<br><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image004.png" alt=""></li>
<li>Some details: 论文主要用refinedet和ssd作为baseline来比较，PFPNet-S代表和SSD相同的anchor设置，PFPNet-R代表和RefineDet相同的anchor设置，300和512则代表具体输入图片的size。</li>
</ol>
<p>实验结果，论文主要对比的对象是SSD和RefineDett，其中从整体上来看在COCO数据集上PFPNET的表现貌似并不是最优的，作者则具体分析了在小物体等场景下的优势：<br><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image006.png" alt=""><br><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image005.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/12/11/Parallel-Feature-Pyramid-Network-for-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/04/An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/">An Analysis of Scale Invariance in Object Detection – SNIP</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1711.08189" target="_blank" rel="noopener">https://arxiv.org/abs/1711.08189</a><br><strong>【Summary】</strong>CVPR2018的一篇Oral，主要在研究scale invariance或者说是domain shift的问题，论文所提出的SNIP方法不同于multi scale的逻辑，可以理解为把网络输入的物体norm到一个相对固定的scale，inference的时候也做同样的策略，这样可以避免训练和测试数据集的scale invariance的问题。</p>
<p>论文在分析scale invariance问题的时候做了几组实验：</p>
<ul>
<li><strong>实验一</strong>：训练三个模型CNN-B(224x224作为输入的图)、CNN-S（和CNN-B类似的图片只是针对输入图片的尺寸修改了第一层conv的stride，可以理解为针对输入图的尺寸做的Resolution Specific Classifier）以及CNN-B-FT（224x224输入图预训练模型 + 用低分辨率图片upsample搭配224x224来finetune）。CNN-B在48x48~224x224范围的输入图上效果如下图a，可见效果随分辨率大小逐渐变好，224x224最好。CNN-B-FT的效果也有一定提升。从结果中也可以发现domain shift对模型的影响：<br><img src="./An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/a.png" alt="51|690x198"></li>
<li><strong>实验二</strong>:验证集固定为1400x2000的分辨率</li>
</ul>
<ol>
<li>分别训练800x1400和1400x2000的检测模型，1400x2000效果最好，但是提升有限，作者认为是因为提升分辨率有助于小物体的检测但是对于大物体的检测是有坏处的</li>
<li>训练1400x2000的模型忽略特别大的物体，最后的结果比800x1400的效果更差，因为抛弃了比较多的数据，丢失了variation in appearance and pose</li>
<li>Multi-Scale Training最后的结果和800x1400的效果差不多。作者认为过程中同样出现了过大和过小的物体。<br><img src="./An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/屏幕快照 2018-12-04 下午9.34.43.png" alt="43|508x178"><br>针对上面实验的结果论文所提出的SNIP方法想法很直接，可以理解为是Image Pyramid的改进。SNIP通过在训练和inference的时候控制物体到一个固定的scale来保证检测的效果。实验一和实验二的结果也支持来这一点。具体实现的时候针对不同scale的输入，论文中都分别定义了一些proposal的面积范围，网络训练的时候只有落在给定范围内的proposal才会回传梯度。这也是SNIP中Scale Normalization的实际含义：<br><img src="./An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/屏幕快照 2018-12-04 下午8.39.47.png" alt="33|690x302"><br>一些实验结果：<br><img src="./An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/屏幕快照 2018-12-04 下午9.59.10.png" alt="10|690x169"><br><img src="./An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/屏幕快照 2018-12-04 下午9.59.16.png" alt="16|690x259"></li>
</ol>
</div></div><a class="button-hover more" href="../../2018/12/04/An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/#more">阅读全文</a></div></div><div id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="../../"><i class="fas fa-angle-left"></i></a><a class="page-number" href="../../">1</a><span class="page-number current">2</span><a class="page-number" href="../3/">3</a><a class="extend next" rel="next" href="../3/"><i class="fas fa-angle-right"></i></a></div></div></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fas fa-user"></i></span><span id="busuanzi_value_site_uv"></span><span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fas fa-eye"></i></span><span id="busuanzi_value_site_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2019 By libanghuai</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/copy.js"></script><!--script(src=url)--></body></html>