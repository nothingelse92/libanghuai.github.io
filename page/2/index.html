<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Live and Learn"><title>Out of Memory | Live and Learn</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Out of Memory</h1><a id="logo" href="/.">Out of Memory</a><p class="description">Live and Learn</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title"><a href="/2019/01/31/High-Speed-Tracking-by-Detection-Without-Using-Image-Information/">High-Speed Tracking-by-Detection Without Using Image Information</a></h1><div class="post-meta">2019-01-31</div><div class="post-content"><p>URL: <a href="http://ieeexplore.ieee.org/document/8078516/" target="_blank" rel="noopener">http://ieeexplore.ieee.org/document/8078516/</a><br>一篇关于IoU tracker的论文，整个跟踪的逻辑都基于检测框来做，对于当前的帧f，检测模型首先会检测出这一帧上面所有的bounding box，然后利用贪心的逻辑将捡出来的框尝试加入到对应的track中，匹配的逻辑就是根据当前的bbox和track的bbox之间的IoU，然后IoU &gt; IoU<sub>threshold</sub>,就把它加到当前track中否则看整个bbox的score，如果score  &gt;= score<sub>threshold</sub> 并且 track的长度  &gt;= length<sub>threshold</sub>就把当前帧作为这个track的结束帧，否则就直接终端当前的这个track，因为他有很大的概率是一段fp，当然论文中也提到匹配的时候也是可以用最大匹配（IoU最大）的一些算法来做的：</p></div><p class="readmore"><a href="/2019/01/31/High-Speed-Tracking-by-Detection-Without-Using-Image-Information/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/28/Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose/">Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose</a></h1><div class="post-meta">2019-01-28</div><div class="post-content"><p>URL: <a href="https://arxiv.org/pdf/1811.12004.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.12004.pdf</a><br>论文主要在探索把OpenPose做的轻量化一点，从模型结构到工程优化都做了不少工作，基本把一些常见的优化操作都尝试了一下，比如把原有的VGG backbone替换成针对手机端的mobilenet、把7x7的conv化成1 x 1 + 3 x 3 + 3 x 3 + residual connection、把PAF和joint两个分支进行部分合并共享参数等等。最后模型的复杂度从61.7GFlops降到9GFlops的同时AP只从43.3%降到42.8%，速度直接的量化结果（NUC支持FP16、CPU支持FP32）：<br><img src="Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose-屏幕快照 2018-12-23 下午11.55.18.png" alt></p></div><p class="readmore"><a href="/2019/01/28/Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/28/Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation/">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></h1><div class="post-meta">2019-01-28</div><div class="post-content"><p>URL: <a href="https://arxiv.org/pdf/1802.02611.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1802.02611.pdf</a><br>这是DeepLab系列的第四篇文章，DeepLab V3+，这篇论文的主要内容主要是在DeepLab V3的基础上接了一个decoder形成一个hourglass的结构结果直接出原图的resolution，这相比前面版本直接插值要更好一点，整个结构中encoder就直接用的DeepLab V3的结构，输出结果concat到一起之后会和低层的网络特征直接concat，低层的特征通过1x1卷积来降维，简单的示意图：<br><img src="Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation-屏幕快照 2019-01-26 上午11.18.27.png" alt></p></div><p class="readmore"><a href="/2019/01/28/Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/28/Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/">Rethinking Atrous Convolution for Semantic Image Segmentation</a></h1><div class="post-meta">2019-01-28</div><div class="post-content"><p>URL: <a href="https://arxiv.org/pdf/1706.05587.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.05587.pdf</a><br>这篇论文是DeepLab系列的第三篇论文DeepLab V3. 相比较DeepLab V1和V2主要在研究如何更好的运用空洞卷积, 同时也去掉了之前的DenseCRF后处理模块。论文中主要涉及了两个方面一个是串行的连接空洞卷积，因为相比较传统的直接用conv层来增加网络的深度会导致抽象到最后一层会丢掉很多的原始信息，利用空洞卷积可以一定程度上优化这个问题。另一方面是在ASPP的基础上优化空洞卷积的使用，作者通过实验发现，如果一味的通过扩大空洞卷积的rate来扩大感受野，有时候并不能得到很好的结果，比如下图就说明当rate足够大的时候其实卷积核有效的参数量只有一个：<br><img src="Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation-屏幕快照 2019-01-26 上午11.08.28.png" alt></p></div><p class="readmore"><a href="/2019/01/28/Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/28/DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs/">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></h1><div class="post-meta">2019-01-28</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener">https://arxiv.org/abs/1606.00915</a><br>DeepLab系列的第二篇论文，主要是在DeepLab V1的基础上提出了ASPP的结构来做多尺度的问题。<br>在DeepLab V1中作者其实也提到了multiscale的问题，当时论文中提到的方法就是对输入进行多次rescale，那么这种方法很显然太naive，并且也会带来比较大的运算量。因此DeepLab V2中作者就借鉴SPP的方法提出了ASPP的结构来解决多尺度的问题，ASPP本质就是用不同rate的空洞卷积来实现不同的感受野这样就可以覆盖比较多scale的物体，具体的应用逻辑论文中也给出了具体的结构：<br><img src="DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs-屏幕快照 2019-01-25 下午11.01.55.png" alt></p></div><p class="readmore"><a href="/2019/01/28/DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/24/SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS/">SEMANTIC IMAGE SEGMENTATION WITH DEEP CON- VOLUTIONAL NETS AND FULLY CONNECTED CRFS</a></h1><div class="post-meta">2019-01-24</div><div class="post-content"><p>URL:<a href="https://arxiv.org/pdf/1606.00915.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1606.00915.pdf</a><br>Semantic Segmentation比较经典的DeepLab系列的第一篇，DeepLab V1，主要利用FCN和DenseCRF来实现比较出色的segmentation效果，DeepLab V1的整个Pipeline整体可以分成两个部分: FCN得到相对比较粗糙的分割结果，DenseCRF在FCN结果的基础上对边缘进行Refine得到相对比较分明的物体分割轮廓。<br>在FCN部分主要是基于VGG16，将VGG16中FC换成Conv变成全卷积网络，而为了保持相对比较dense的feaure map,从VGG16原来的32倍下采样提高到8倍下采样，这个可以通过改变stride再加上空洞卷积来保持感受野，空洞卷积的具体示意图如下，网上也可以找到比较详细的解释：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午8.53.19.png" alt><br>最后在inference的时候可以直接从8倍下采样的结果直接插值回原图resolution.</p></div><p class="readmore"><a href="/2019/01/24/SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/24/PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model/">PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model</a></h1><div class="post-meta">2019-01-24</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1803.08225" target="_blank" rel="noopener">https://arxiv.org/abs/1803.08225</a><br>这篇论文其实和之前读的G-RMI那篇论文是同一个作者，G-RMI是top down的逻辑，而这篇论文是bottom up的逻辑。论文所提方法同时在做pose estimation和instance-level person segmentation两个task。pose estimation主要是通过预测点对之间的向量来做group，seg则是主要借鉴embedding的逻辑通过设计新的loss函数来优化效果。<br>下图是论文所提方法的整个pipeline，两个大的分支，pose + seg：<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-03 下午11.18.23.jpg" alt></p></div><p class="readmore"><a href="/2019/01/24/PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/22/Fully-Convolutional-Siamese-Networks-for-Object-Tracking/">Fully-Convolutional Siamese Networks for Object Tracking </a></h1><div class="post-meta">2019-01-22</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1606.09549" target="_blank" rel="noopener">https://arxiv.org/abs/1606.09549</a><br>Siamese-FC v1 , 利用Siamese网络来做跟踪，论文中把跟踪的任务定义为两个图像patch之间的相似度，通过计算两个图像patch之间的相似度来定位物体，通过多次rescale 输入图片来实现多尺度物体的跟踪。<br>下图是论文中给出的Siamese-FC v1基本的示意图：<br><img src="Fully-Convolutional-Siamese-Networks-for-Object-Tracking-d0c371c53a45ca34e71ad8a64fdc055b53e0f958.png" alt></p></div><p class="readmore"><a href="/2019/01/22/Fully-Convolutional-Siamese-Networks-for-Object-Tracking/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/20/IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks/">IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks</a></h1><div class="post-meta">2019-01-20</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1806.00178" target="_blank" rel="noopener">https://arxiv.org/abs/1806.00178</a><br>IGCV3，BMVC2018，在IGCV2基础上进行的改进，主要引入bottleneck的网络结构来改善IGCV2网络的信息交互，从而进一步提高网络效果。<br>在IGCV2中曾提到过一个互补条件：前面一级group convolution里不同partition的channel在后面一级的group convolution要在同一个partition里面，那么follow这个逻辑设计的IGCV2网络结构的每一个输出channel、输入channel之间将有且仅有一条路径可以连接，示意图如下，这就会导致网络过于稀疏不一定有利于网络的整体性能：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-a0f037095b27476caa3cf49ca30929b2923f2315_1_690x253.jpeg" alt></p></div><p class="readmore"><a href="/2019/01/20/IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/20/IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks/">IGCV2: Interleaved Structured Sparse Convolutional Neural Networks</a></h1><div class="post-meta">2019-01-20</div><div class="post-content"><p>URL: <a href="https://arxiv.org/pdf/1804.06202" target="_blank" rel="noopener">https://arxiv.org/pdf/1804.06202</a><br>IGCV2, CVPR2018, 主要把IGCV1中提到的group convolution进行了推广，将卷积操作变的更加稀疏，以此来达到减少冗余的目的。<br>在IGCV1论文中作者把IGC操作抽象成如下的表达式，x为输入，x前面的表达式整体是一个dense convolution kernel：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-f1ed32b809c7dda6f6c1b5a8834e594e6c0b99ce.png" alt></p></div><p class="readmore"><a href="/2019/01/20/IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/20/Interleaved-Group-Convolutions-for-Deep-Neural-Networks/">Interleaved Group Convolutions for Deep Neural Networks</a></h1><div class="post-meta">2019-01-20</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1707.02725" target="_blank" rel="noopener">https://arxiv.org/abs/1707.02725</a><br>下图是IGCv1的整体示意图：<br><img src="Interleaved-Group-Convolutions-for-Deep-Neural-Networks-7d3b43e53abfbc53dc3dfca907e8935d56f3fcf5.jpeg" alt></p></div><p class="readmore"><a href="/2019/01/20/Interleaved-Group-Convolutions-for-Deep-Neural-Networks/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/15/High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network/">High Performance Visual Tracking with Siamese Region Proposal Network</a></h1><div class="post-meta">2019-01-15</div><div class="post-content"><p>URL: <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2951.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2951.pdf</a><br>Siamese-RPN, CVPR2018一篇关于tracking的论文，论文所提方法的整个逻辑还是基于孪生网络来做，整体也可以理解为对Siamese-FC结构的改进，下图是Siamese-RPN的整个Pipeline：<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-15 下午11.31.31.png" alt></p></div><p class="readmore"><a href="/2019/01/15/High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/01/14/Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks/">Learning to Track at 100 FPS with Deep Regression Networks</a></h1><div class="post-meta">2019-01-14</div><div class="post-content"><p>URL:<a href="https://arxiv.org/abs/1604.01802" target="_blank" rel="noopener">https://arxiv.org/abs/1604.01802</a><br>论文主要提出了GOTURN的tracking框架，整体还是比较naive的一些tracking逻辑，下图是GOTURN整个的pipeline:<br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.26.02.png" alt></p></div><p class="readmore"><a href="/2019/01/14/Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/24/Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</a></h1><div class="post-meta">2018-12-24</div><div class="post-content"><p>URL:<br>论文所提的模型结构整体是一个bottom-up的结构，F是通过backbone网络（论文中用的是vgg）提取的特征，整个结构分成两个分支，一个分支用来预测joint的heatmap，另一个分支可以理解为用来预测joint之间的联系（Part Affinity Field），实际上是joint对应的向量：</p></div><p class="readmore"><a href="/2018/12/24/Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/24/Towards-Accurate-Multi-person-Pose-Estimation-in-the-Wild/">Towards Accurate Multi-person Pose Estimation in the Wild</a></h1><div class="post-meta">2018-12-24</div><p class="readmore"><a href="/2018/12/24/Towards-Accurate-Multi-person-Pose-Estimation-in-the-Wild/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/24/AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones/">AI Benchmark: Running Deep Neural Networks on Android Smartphones </a></h1><div class="post-meta">2018-12-24</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1810.01109" target="_blank" rel="noopener">https://arxiv.org/abs/1810.01109</a><br>关于手机各种硬件平台一篇比较好的科普论文，论文主要提出了一个AI Benchmark来客观评估各个手机平台包括华为海思、高通、联发科、三星、谷歌Pixel等在标准CV任务比如识别、分割上面的具体表现。</p></div><p class="readmore"><a href="/2018/12/24/AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/24/Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint/">Bottom-up Pose Estimation of Multiple Person with Bounding Box Constraint </a></h1><div class="post-meta">2018-12-24</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1807.09972" target="_blank" rel="noopener">https://arxiv.org/abs/1807.09972</a><br>一篇基于Bottom Up逻辑的pose estimation论文，但是实际上和刚读的PRN那篇论文比较类似可以理解为Top Down + Bottom Up结合的方法，论文所提的方法主要是OpenPose再结合人体框的检测来做多人的pose estimation。</p></div><p class="readmore"><a href="/2018/12/24/Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/24/MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network/">MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network</a></h1><div class="post-meta">2018-12-24</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1807.04067" target="_blank" rel="noopener">https://arxiv.org/abs/1807.04067</a><br>ECCV2018一篇利用bottom up方法做pose estimation的论文。论文所提出的方法主要是分别利用两个分支一个分支用来检测人体框，另一个分支用来出关键点的heatmap。然后再利用一个网络来merge 两个分支出的结果，将关键点分别映射到对应的人体框上实现多人的姿态估计。</p></div><p class="readmore"><a href="/2018/12/24/MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/19/Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment/">Two-Stream Transformer Networks for Video-based Face Alignment</a></h1><div class="post-meta">2018-12-19</div><div class="post-content"><p>URL: <a href="https://ieeexplore.ieee.org/document/7999169" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/7999169</a><br>这是一篇TPAMI2018的论文，论文主要的研究内容是视频场景下的facial landmark 定位的问题。论文的motivation也比较直观，和之前看过的RED-Net很类似，就是想借助于视频流提供的temporal信息再加上静态图片的spatial信息来优化视频场景下的facial landmark问题，比如pose、遮挡等。</p></div><p class="readmore"><a href="/2018/12/19/Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/19/DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks/">DropFilter: A Novel Regularization Method for Learning Convolutional Neural Networks</a></h1><div class="post-meta">2018-12-19</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1811.06783" target="_blank" rel="noopener">https://arxiv.org/abs/1811.06783</a><br>论文主要提出了一种新的Regularization方法DropFilter，主要是对卷积核进行抑制，具体实现的时候也比较简单，只要对卷积核和bias生成对应的0 - 1mask(Bernoulli分布),然后对应和kernel相乘即可：</p></div><p class="readmore"><a href="/2018/12/19/DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/19/Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images/">Robust Face Detection via Learning Small Faces on Hard Images</a></h1><div class="post-meta">2018-12-19</div><div class="post-content"><p>URL: <a href="https://arxiv.org/pdf/1811.11662.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.11662.pdf</a><br>论文主要是想解决人脸检测中的小脸问题，论文的motivation其实和SNIP很像，让网络去学习一个相对固定的scale，比如在本论文中anchor大小被设置为固定的16x16，32x32，64x64三个。论文主要的内容有两个部分，一部分是提出来的检测网络，另一部分就是hard image mining。</p></div><p class="readmore"><a href="/2018/12/19/Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/17/Deep-Layer-Aggregation/">Deep Layer Aggregation</a></h1><div class="post-meta">2018-12-17</div><div class="post-content"><p>URL: <a href="https://arxiv.org/pdf/1707.06484.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1707.06484.pdf</a><br>CVPR2018的一篇关于layer aggregation的论文，论文的motivation是作者觉得目前常见的aggregation方式（FPN、U-Net…）比较shallow，作者希望利用更加deeper的连接方式来更好的融合特征。论文中作者分别提出了IDA和HDA两种连接方式。IDA应用在stage之间，HDA应用在block之间。</p></div><p class="readmore"><a href="/2018/12/17/Deep-Layer-Aggregation/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/15/Deep-Mutual-Learning/">Deep Mutual Learning</a></h1><div class="post-meta">2018-12-15</div><div class="post-content"><p>URL: <a href="http://cn.arxiv.org/pdf/1706.00384v1" target="_blank" rel="noopener">http://cn.arxiv.org/pdf/1706.00384v1</a><br>论文主要在讲mutual learning相互学习，deep mutual learning整体感觉和model distillation还是比较像的,只是不是用训练好的大网络来带小网络而是用一些网络相互同步学习来提高模型整体的效果:<br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.49.05.png" alt></p></div><p class="readmore"><a href="/2018/12/15/Deep-Mutual-Learning/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/14/A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition/">Center Loss - A Discriminative Feature Learning Approach for Deep Face Recognition</a></h1><div class="post-meta">2018-12-14</div><div class="post-content"><p>URL:<a href="http://ydwen.github.io/papers/WenECCV16.pdf" target="_blank" rel="noopener">http://ydwen.github.io/papers/WenECCV16.pdf</a><br>这篇论文主要的贡献就是提出了Center Loss的损失函数，利用Softmax Loss和Center Loss联合来监督训练，在扩大类间差异的同时缩写类内差异，提升模型的鲁棒性。</p></div><p class="readmore"><a href="/2018/12/14/A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/14/CRAFT-Objects-from-Images/">CRAFT Objects from Images</a></h1><div class="post-meta">2018-12-14</div><div class="post-content"><p>URL:<a href="https://arxiv.org/abs/1604.03239" target="_blank" rel="noopener">https://arxiv.org/abs/1604.03239</a><br>CVPR2016的一篇论文。首先CRAFT代表 Cascade Region proposal network And FasT-rcnn，本论文主要想解决RPN网络生成的区域不太精确的问题，比如对于一些外观复杂度较低的事物如树木，会因为RPN网络产生的背景区域的存在导致比较难检测或者产生FP，因此作者尝试利用cascade的方式来解决这样的问题。<br>CRAFT模型可以分成两个部分：</p></div><p class="readmore"><a href="/2018/12/14/CRAFT-Objects-from-Images/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/14/Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network/">Weighted Channel Dropout for Regularization of Deep Convolutional Neural Network</a></h1><div class="post-meta">2018-12-14</div><div class="post-content"><p>URL:<a href="http://home.ustc.edu.cn/~saihui/papers/aaai2019_weighted.pdf" target="_blank" rel="noopener">http://home.ustc.edu.cn/~saihui/papers/aaai2019_weighted.pdf</a><br><strong>【Summary】</strong>AAAI2019的一篇关于Regularization的论文，整体感觉可以理解为SENet思想在Regularization中的应用。论文中作者提出了Weighted Channel Dropout(WCD)的逻辑（为每一个channel计算权重、构造取舍的概率…）来对channel进行选择性的DropOut。</p></div><p class="readmore"><a href="/2018/12/14/Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/14/Caffe-配置/">Caffe 配置</a></h1><div class="post-meta">2018-12-14</div><p class="readmore"><a href="/2018/12/14/Caffe-配置/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/11/Parallel-Feature-Pyramid-Network-for-Object-Detection/">Parallel Feature Pyramid Network for Object Detection</a></h1><div class="post-meta">2018-12-11</div><div class="post-content"><p>URL:<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener">https://arxiv.org/abs/1612.03144</a><br>ECCV2018的一篇论文,这篇论文从某种程度上来说是为了解决小物体检测的问题，作者从feature map特征表示好坏的角度来分析目前常用检测模型的一些不足。论文分别可视化SSD、FPN、PFPNET（本文所提模型）对同样输入图片的feature map，从图中可以看出来SSD对物体的轮廓细节描述比较差，FPN对于一些遮挡物体的特征表示比较差。PFPNET则相对好一些，至于为什么好，以及这个结构设计的理由论文貌似并没有解释。</p></div><p class="readmore"><a href="/2018/12/11/Parallel-Feature-Pyramid-Network-for-Object-Detection/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/12/04/An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/">An Analysis of Scale Invariance in Object Detection – SNIP</a></h1><div class="post-meta">2018-12-04</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1711.08189" target="_blank" rel="noopener">https://arxiv.org/abs/1711.08189</a><br><strong>【Summary】</strong>CVPR2018的一篇Oral，主要在研究scale invariance或者说是domain shift的问题，论文所提出的SNIP方法不同于multi scale的逻辑，可以理解为把网络输入的物体norm到一个相对固定的scale，inference的时候也做同样的策略，这样可以避免训练和测试数据集的scale invariance的问题。</p></div><p class="readmore"><a href="/2018/12/04/An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/11/24/SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts/">SGDR: Stochastic Gradient Descent with Warm Restarts</a></h1><div class="post-meta">2018-11-24</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1608.03983" target="_blank" rel="noopener">https://arxiv.org/abs/1608.03983</a> Github: <a href="https://github.com/loshchil/SGDR" target="_blank" rel="noopener">https://github.com/loshchil/SGDR</a><br><strong>【Summary】</strong>ICLR2017一篇关于学习率的论文，论文的核心比较直接就是提出了基于cosine的学习率 warm restart逻辑，然后论文的大篇幅都是围绕这个learing rate进行了比较多的实验。论文所提的SGDR通常只需要原有模型1/2-1/4的训练epoch就可以得到差不多甚至更好的效果。</p></div><p class="readmore"><a href="/2018/11/24/SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts/">阅读全文</a></p></div><nav class="page-navigator"><a class="extend prev" rel="prev" href="/">上一页</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">下一页</a></nav></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://yoursite.com"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Engineering/">Engineering</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper-Reading/">Paper Reading</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Loss/" style="font-size: 15px;">Loss</a> <a href="/tags/Mobile/" style="font-size: 15px;">Mobile</a> <a href="/tags/Platform/" style="font-size: 15px;">Platform</a> <a href="/tags/Hardware/" style="font-size: 15px;">Hardware</a> <a href="/tags/Detection/" style="font-size: 15px;">Detection</a> <a href="/tags/Landmark/" style="font-size: 15px;">Landmark</a> <a href="/tags/KeyPoint/" style="font-size: 15px;">KeyPoint</a> <a href="/tags/BottomUp/" style="font-size: 15px;">BottomUp</a> <a href="/tags/Bottom-Up/" style="font-size: 15px;">Bottom_Up</a> <a href="/tags/Pose/" style="font-size: 15px;">Pose</a> <a href="/tags/Tools/" style="font-size: 15px;">Tools</a> <a href="/tags/Engineering/" style="font-size: 15px;">Engineering</a> <a href="/tags/Mutual-learning/" style="font-size: 15px;">Mutual learning</a> <a href="/tags/Classification/" style="font-size: 15px;">Classification</a> <a href="/tags/Segmentation/" style="font-size: 15px;">Segmentation</a> <a href="/tags/Regularization/" style="font-size: 15px;">Regularization</a> <a href="/tags/VID/" style="font-size: 15px;">VID</a> <a href="/tags/Face/" style="font-size: 15px;">Face</a> <a href="/tags/SOT/" style="font-size: 15px;">SOT</a> <a href="/tags/Tracking/" style="font-size: 15px;">Tracking</a> <a href="/tags/Track/" style="font-size: 15px;">Track</a> <a href="/tags/Mimick/" style="font-size: 15px;">Mimick</a> <a href="/tags/3D/" style="font-size: 15px;">3D</a> <a href="/tags/Template/" style="font-size: 15px;">Template</a> <a href="/tags/Bottom-UP/" style="font-size: 15px;">Bottom UP</a> <a href="/tags/DeepLab/" style="font-size: 15px;">DeepLab</a> <a href="/tags/Learning-Strategy/" style="font-size: 15px;">Learning Strategy</a> <a href="/tags/Classic/" style="font-size: 15px;">Classic</a> <a href="/tags/Basic/" style="font-size: 15px;">Basic</a> <a href="/tags/Quantization/" style="font-size: 15px;">Quantization</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/06/28/Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm/">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/28/Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection/">Quantization Mimic: Towards Very Tiny CNN for Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/24/Mimicking-Very-Efficient-Network-for-Object-Detection/">Mimicking Very Efficient Network for Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/20/Tone-Mapping/">Tone Mapping</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/15/Lightweight-Real-time-Makeup-Try-on-in-Mobile-Browsers-with-Tiny-CNN-Models-for-Facial-Tracking/">Lightweight Real-time Makeup Try-on in Mobile Browsers with Tiny CNN Models for Facial Tracking</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/10/Improving-Landmark-Localization-with-Semi-Supervised-Learning/">Improving Landmark Localization with Semi-Supervised Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/05/Seeing-Small-Faces-from-Robust-Anchor’s-Perspective/">Seeing Small Faces from Robust Anchor’s Perspective</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/01/Style-Aggregated-Network-for-Facial-Landmark-Detection/">Style Aggregated Network for Facial Landmark Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/25/Deep-Regionlets-for-Object-Detection/">Deep Regionlets for Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/21/Regionlets-for-Generic-Object-Detection/">Regionlets for Generic Object Detection</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/libanghuai" title="Github" target="_blank">Github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Out of Memory.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script type="text/javascript" color="0,0,0" opacity="0.5" zindex="-2" count="50" src="//lib.baomitu.com/canvas-nest.js/2.0.3/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>