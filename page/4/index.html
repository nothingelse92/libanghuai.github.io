<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Live and Learn"><meta name="keywords" content=""><meta name="author" content="Out of Memory,undefined"><meta name="copyright" content="Out of Memory"><title>Live and Learn【Out of Memory】</title><link rel="stylesheet" href="../../css/fan.css"><link rel="stylesheet" href="../../css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="../../favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="../../js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="author-info"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Out of Memory</div><div class="author-info-description">Live and Learn</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/libanghuai" target="_blank">GitHub<i class="icon-dot bg-color1"></i></a><a class="links-button button-hover" href="mailto:libanghuai@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color0"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1185719433&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color8"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="../../archives"><span class="pull-top">日志</span><span class="pull-bottom">124</span></a><a class="author-info-articles-tags article-meta" href="../../tags"><span class="pull-top">标签</span><span class="pull-bottom">39</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Out of Memory</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><div id="recent-posts"><!-- each post in page.posts.sort('date', -1).limit(10).toArray()--><!-- config中配置按照什么排序--><div class="recent-post-item"><a class="post-title" href="../../2019/01/20/IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks/">IGCV2: Interleaved Structured Sparse Convolutional Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1804.06202" target="_blank" rel="noopener">https://arxiv.org/pdf/1804.06202</a><br>IGCV2, CVPR2018, 主要把IGCV1中提到的group convolution进行了推广，将卷积操作变的更加稀疏，以此来达到减少冗余的目的。<br>在IGCV1论文中作者把IGC操作抽象成如下的表达式，x为输入，x前面的表达式整体是一个dense convolution kernel：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-f1ed32b809c7dda6f6c1b5a8834e594e6c0b99ce.png" alt=""></p>
<p>本论文中提到的Interleaved Structured Sparse Convolution就是它的一般表达：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-c181db945a7431adc7c182e16398ada2cc287be4.png" alt=""><br>那么为了保证输入x前面的表达式同样是dense convolutional kernel，论文中做了条件约束：前面一级group convolution里不同partition的channel在后面一级的group convolution要在同一个partition里面, 那么follow IGCV1的结构设计第一级group convolution为spatial conv (3x3)其余为1x1:<br>下图是wangjingdong的PPT上对IGCV2结构的描述示意图，比论文里面的示意图要清晰直观很多，其实就是把IGCV1的第二个group convolution再划分成多个1x1的group convolution操作 ：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-6268924f405c5224270e8fe5c80de272c04dbfa2_1_690x286.png" alt=""><br>实验Group Convolution的组数对结果的影响：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-b493611cc06c1996f7187b42c8cceaffa7452a20_1_539x500.png" alt=""><br>和其他一些网络模型的结果比较：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-e6c4c428b992b470ad7c2655ef49528184b321e9_1_608x500.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/20/IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/20/Interleaved-Group-Convolutions-for-Deep-Neural-Networks/">Interleaved Group Convolutions for Deep Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-10</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1707.02725" target="_blank" rel="noopener">https://arxiv.org/abs/1707.02725</a><br>下图是IGCv1的整体示意图：<br><img src="Interleaved-Group-Convolutions-for-Deep-Neural-Networks-7d3b43e53abfbc53dc3dfca907e8935d56f3fcf5.jpeg" alt=""></p>
<p>具体做法，首先整个IGC block会被分成两个部分，primary group convolutions 和 secondary group convolutions：<br>primary group convolutions的输入首先会被分成L个partition，每个partition共M个channel， 每个partition内部进行一般的spatial conv操作，最后的输出还会是ML个channel，那么在secondary group convolutions阶段，上一阶段的输出会先被shuffle一下然后再划分为M个partition，每个partition L个channel，对于第i个partition它其中的M个channel分别来自于前一个阶段的每个partition的第i个channel组成，然后每个partition内部进行1x1的卷积操作，最后还是会输出ML个channel，这ML个打乱的channel会再次映射回输入时候的顺序, 整个过程被抽象成如下的卷积操作，P为序列化：<br><img src="Interleaved-Group-Convolutions-for-Deep-Neural-Networks-f1ed32b809c7dda6f6c1b5a8834e594e6c0b99ce.png" alt=""><br>除了IGC主要的逻辑以外论文中也花了比较多的篇幅在论文，IGC比常规的conv操作在同等参数量的情况下宽度更宽，论证比较简单具体可以参考论文，同时作者也通过实验证明了更宽的网络可以得到更好的结果：<br><img src="Interleaved-Group-Convolutions-for-Deep-Neural-Networks-9a13e111aef4563a2957a372c32d4f8136975302.png" alt=""></p>
<p>最后的一组实验结果<br><img src="Interleaved-Group-Convolutions-for-Deep-Neural-Networks-e51af6de7667412d2511a7cfb8d8d61c3d045247.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/20/Interleaved-Group-Convolutions-for-Deep-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/15/High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network/">High Performance Visual Tracking with Siamese Region Proposal Network</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Tracking/">Tracking</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2951.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2951.pdf</a><br>Siamese-RPN, CVPR2018一篇关于tracking的论文，论文所提方法的整个逻辑还是基于孪生网络来做，整体也可以理解为对Siamese-FC结构的改进，下图是Siamese-RPN的整个Pipeline：<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-15 下午11.31.31.png" alt=""></p>
<p>Siamese-RPN网络的整个逻辑还是分成两个分支, 上面一个分支针对关键帧，下面一个分支针对检测帧，两个分支本身的参数是共享的，Siamese网络完成对两张输入图片的特征提取，两个分支的输出还会经过上图的Region Proposal Network网络来整合两者的信息.<br>具体做法和RPN网络类似，前一层Siamese网络的每一个分支都会接入两层的conv然后分成两个分支，一支为cls，另一支为reg，关键帧的输出如上图的4×4×(2k×256) 和 4×4×(4k×256)，这两个输出会分别作为对应的检测帧输出的kernel来进行卷积运算（上图中的*号操作）。这两者最后的输出就是检测帧最后的检测结果。<br>那么在得到最后预测的结果时针对tracking的应用场景，作者也提出了一些选框的策略，一是默认物体的移动速度不快，所以只选取靠近中心的anchor：<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.43.46.png" alt=""><br>另外一个做法就是对当前得到的proposal进行重新排名，比如利用cosine window来对距离重点点比较远的候选框进行抑制或者同样基于物体移动速度不快的假设可以默认物体的size变化也不大所以对scale进行penalty，形变比较大的得分会被抑制，具体打分如下，k为超参数，r为proposal的ratio，r’为上一帧的ratio，s和s’则是对应的整体的ratio（输入图片多尺度scale），论文中应该采取的是后者：<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.48.02.png" alt=""><br><strong>实验结果</strong><br>VOT2015:<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.53.58.png" alt=""><br>VOT2016:<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.55.21.png" alt=""><br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.55.25.png" alt=""><br>SiamRPN网络相比SiamFC直接出了框的具体坐标位置，感觉对于跟踪的任务更加合理，也避免了SiamFC相对比较繁琐的后处理，在测试集上的效果也是要比SiamFC好一些。</p>
</div></div><a class="button-hover more" href="../../2019/01/15/High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/14/Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks/">Learning to Track at 100 FPS with Deep Regression Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Tracking/">Tracking</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1604.01802" target="_blank" rel="noopener">https://arxiv.org/abs/1604.01802</a><br>论文主要提出了GOTURN的tracking框架，整体还是比较naive的一些tracking逻辑，下图是GOTURN整个的pipeline:<br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.26.02.png" alt=""></p>
<p>GOTURN整个框架是针对连续的两帧来做的，整个框架因此也主要分成两个分支，分别针对当前帧和前一帧，给定当前帧(t)和前一帧(t-1),t-1帧的物体是已知的，比如物体框中心为center(c<sub>x</sub>, c<sub>y</sub>),长宽分别是w、h。那么对于下面一个分支首先会以center为中心，长宽分别是k1w, k1h去抠图，k1这个系数用于控制context信息的量。那么对于上面一个分支，会从同样的位置center作为中心，长宽分别是k2w,k2h去抠图作为当前帧的搜索区域。论文中也提及了一下通常k2 = 2。这两个分支的输出最后会被concat到一起送入一个3层的FC网络，每层FC有4096个神经元，每个分支本身的结构是CaffeNet的前五层。</p>
<p>那么在具体训练的时候，论文中主要提及了一些数据增强的细节，比如论文就直接说明GOTURN本身在训练的时候就是针对低速运动的物体进行的设计，对于高速运动的物体并不能cover。至于GOTURN对低速运行物体的跟踪也跟论文中提到的数据增强方法有关，模型在具体训练的时候会构造这样的pair对：<br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.43.16.png" alt=""><br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.43.23.png" alt=""><br>其中控制偏移幅度的变量delta服从Laplace分布。</p>
<p><strong>具体的实验结果</strong><br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.46.49.png" alt=""><br>Ablation Study:<br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.47.31.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/14/Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/24/AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones/">AI Benchmark: Running Deep Neural Networks on Android Smartphones </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Mobile/">Mobile</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Platform/">Platform</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Hardware/">Hardware</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1810.01109" target="_blank" rel="noopener">https://arxiv.org/abs/1810.01109</a><br>关于手机各种硬件平台一篇比较好的科普论文，论文主要提出了一个AI Benchmark来客观评估各个手机平台包括华为海思、高通、联发科、三星、谷歌Pixel等在标准CV任务比如识别、分割上面的具体表现。</p>
<p>论文主要介绍了华为海思、高通、联发科、三星、谷歌Pixel等手机平台针对AI任务所做的具体优化：<br><img src="AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones-9992f821aff8fcd693990d97c8aa896813ce65b8.png" alt=""></p>
<ul>
<li>高通：SNPE(Snapdragon Neu- ral Processing Engine)，支持大多数主流的框架比如Caffe、Tensorflow等，需要INT8量化。</li>
<li>华为海思：NPU(Neural Processing Unit), 当前总共有两款NPU，970和980。目前只支持Caffe和Tensorflow平台，论文中也提到了目前应该只支持16-bit float。</li>
<li>联发科：APU(AI Processing Unit), 需要INT8量化，论文上写只有P60，现在应该还有其他的APU发布。框架的话支持Caffe、Tensorflow、ONNX。</li>
<li>三星：VPU(Vision Processing Unit), 主要用在手机摄像头，目前没有提出对AI任务的针对性支持</li>
<li>Google：IPU( Image Processing Unit ),支持16-bit int和8-bit int，目前也没有提出对AI任务的针对性支持。</li>
</ul>
<p>论文另外一个主要内容就是介绍了其”AI Benchmark”, 至于具体的任务和具体的模型下表给了一个相对比较仔细的介绍，总共是下表的8个任务外加一个Memory Limitation测试共9个测试任务:<br><img src="AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones-b0846738e8d03725f22af52704abcfd5224cbde1.png" alt=""></p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones-9d9e02feef163a08de25b0dfa8efbc42d96c5207.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/12/24/AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/24/Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint/">Bottom-up Pose Estimation of Multiple Person with Bounding Box Constraint </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Skeleton/">Skeleton</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1807.09972" target="_blank" rel="noopener">https://arxiv.org/abs/1807.09972</a><br>一篇基于Bottom Up逻辑的pose estimation论文，但是实际上和刚读的PRN那篇论文比较类似可以理解为Top Down + Bottom Up结合的方法，论文所提的方法主要是OpenPose再结合人体框的检测来做多人的pose estimation。</p>
<p>论文所提方法的整个逻辑如下图,整体可以理解为OpenPose再加上人体BBox约束来提升group的效果：<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-ee790656fab70c4c1822e95038c1f8b7543c7177_1_690x279.jpg" alt=""></p>
<ul>
<li><strong>CNN Regression</strong>: 网络这个分支可以直接理解为OpenPose的逻辑，只是作者再OpenPose的基础上做了一些简单的修改比如更换backbone等，整体结构和逻辑和OpenPose基本是一致的：<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-c76f41b54234100ad9b73127621a3323fcb82b02.png" alt=""><br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-5071f6a4a4983012e8408382a76ed443faa1d773.png" alt=""></li>
<li><strong>Pose Parsing</strong>: 主要是利用CNN Regression分支的结果来解析pose，本论文所提方法在做pose parsing 的时候是每个人体框分别来做，不同于PRN，本文所提方法对于一个框是可以出多个人的结果的，具体贪心逻辑如下：<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-34dfefcffcd927265e9239e373f68117aea2cac8.png" alt=""></li>
<li><strong>Pose NMS</strong>：主要是制定了一些规则来定义Pose的Confidence、Pose的距离：<br>Confidence:s1是pose所有点confidence均值，s2是pose所有connection confidence的均值，B<sup>‘</sup>是pose最小外接矩形面积，B是bbox面积<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-ce3bf61db73b6e97c08606efbe034d5ae04912c7.png" alt=""><br>distance：<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-63ebd7be02c976cf5488aa5104f10dc6e99b7fde.png" alt=""><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2>作者只和OpenPose、CPM做了比较。。。。<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-8c8f1ddc7edf786ac0fc6c02cefe7f968e295bd0.png" alt=""><br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-35821768d390b09996b5101a7f1b748ccd7366fa.png" alt=""><br>Top Down和Bottom Up分别都有各自比较明显的缺点和优点，目前也陆续有两者结合的研究工作出现，感觉是一个可以去研究的内容。</li>
</ul>
</div></div><a class="button-hover more" href="../../2018/12/24/Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/24/MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network/">MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Skeleton/">Skeleton</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1807.04067" target="_blank" rel="noopener">https://arxiv.org/abs/1807.04067</a><br>ECCV2018一篇利用bottom up方法做pose estimation的论文。论文所提出的方法主要是分别利用两个分支一个分支用来检测人体框，另一个分支用来出关键点的heatmap。然后再利用一个网络来merge 两个分支出的结果，将关键点分别映射到对应的人体框上实现多人的姿态估计。</p>
<p>下图是本文所提方法的大体Pipeline，整个pipeline可以分成三个部分，人体关键点检测、人体框检测和关键点的聚类（PRN）：<br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-b80970b3e1b91db9992796fdada5d5cacddc1531.jpg" alt=""></p>
<ul>
<li><strong>Backbone</strong>：Resnet + FPN</li>
<li><strong>关键点检测</strong>：整个分支结构如下图，比较直观，L2 loss，总共出K + 1 个heatmap，+1为seg的结果：<br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-a42ffc7c2d336f7a095fc898d260ef90c728a7f6.png" alt=""></li>
<li><strong>人体框检测</strong>：就是直接上的RetinaNet<br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-96f37d11a64105bce2440ed6b6955231dd1a6949.png" alt=""></li>
<li><strong>Pose Residual Network (PRN)</strong>：这一部分内容是论文的核心，主要是利用人体框将关键点检测的结果映射到对应的instance上。具体做法是在关键点检测的结果上利用对应人体框的位置crop出一样大小的patch，这样就可以得到K x W x H大小的feature map，K为关键点个数，W、H为对应人体框的size，然后把这个feature map resize到固定大小(36 x 56)作为PRN的输入。那么对于上图的c、d这种同一个框有多个人关键点overlap的情况，主要是利用一个多层感知机（residual multilayer perceptron）来把人体框对应instance的关键点提取出来，具体逻辑如下图：<br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-c340c5ea72c9c75d031dbcba3c8b569d4e393023.jpg" alt=""><br><strong>结果：</strong><br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-9255a515da09097264bb0c17a00cd420a3a16291.png" alt=""><br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-d87795e947cb3e80cf5311605ec05df40db408f6.png" alt=""></li>
</ul>
<p>这篇论文另一个比较重要的点是虽然整个网络的pipeline比较多但是inference的速度还是比较快的，对于典型的COCO图片（～3人）可以达到23FPS的速度，用来merge的PRN网络输入比较小层数也不深。</p>
</div></div><a class="button-hover more" href="../../2018/12/24/MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/19/Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment/">Two-Stream Transformer Networks for Video-based Face Alignment</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Face/">Face</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://ieeexplore.ieee.org/document/7999169" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/7999169</a><br>这是一篇TPAMI2018的论文，论文主要的研究内容是视频场景下的facial landmark 定位的问题。论文的motivation也比较直观，和之前看过的RED-Net很类似，就是想借助于视频流提供的temporal信息再加上静态图片的spatial信息来优化视频场景下的facial landmark问题，比如pose、遮挡等。</p>
<p>下面这幅图是论文中给出的整个框架的示意图，画的比较直观论文中所谓的Two-Stream就是Spatial stream 和 Temporal stream这两个branch，两个分支的输出最后通过不同的权重整合到一起作为最后的输出：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-f1e5f5d479db4e5b4c7d609bcd2acce9a43a4002_1_690x241.jpg" alt=""></p>
<ul>
<li>Spatial Stream: 这个分支通常是和一般的静态图片处理方式是一致的，论文中所提方法分为两个部分，sampler和regression， sampler是从原图中进行采样local patch的过程，local path就是每个landmark点周围的dxd的区域，论文中取d=26。regression则是一个标准的CNN网络，接受local patch为输入回归具体的landmark坐标（实际上回归的是offset），为了达到比较高的精度论文在实际做的时候其实做了两阶段的cascade：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-6c1b39b23cda6d28051df18db474abb7ab3e5c33_1_690x83.png" alt=""></li>
<li>Temporal Stream:这一个分支通常是视频场景下facial landmark定位特有的处理过程, 为了对视频的时序信息进行处理，不少的论文都是采用RNN模型来进行处理，本文也不例外，temporal stream分支的输入是一段连续的帧，这些帧首先会经过encoder进行处理提取图片的一些context的信息，然后会根据时序依次输入到两层RNN中，第一层RNN layer通常来编码一些整体的特征信息，第二层RNN layer则用来编码一些变化的时序信息例如pose等，最后RNN的输出会经过decoder映射回输入的size保持原有的spatial 信息然后再经过比较小的回归网络得到最后的landmark输出：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-e3ebddb40d960442b71b63296cf3ecd654878f7a_1_376x500.jpg" alt=""></li>
</ul>
<p>论文主要在300-VM和TF（Talking Face）两个数据集进行了实验，相比较之前的REDN、TCDCN模型都有提升：<br>300-VW上的表现：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-d371c0da62673b1d319fc818b8ca472efe4cf8c1_1_690x187.png" alt=""><br>视频场景下的facial landmark定位相比静态图片的facial landmark定位增加了时序信息可以利用，目前的研究所用方法也比较类似，之前看过的REDNet和这篇TSTN都是通过CNN+RNN的逻辑来整合spatial信息和temporal信息，感觉这种信息融合的方法还需要仔细的去研究。</p>
</div></div><a class="button-hover more" href="../../2018/12/19/Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/19/DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks/">DropFilter: A Novel Regularization Method for Learning Convolutional Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-22</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Regularization/">Regularization</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1811.06783" target="_blank" rel="noopener">https://arxiv.org/abs/1811.06783</a><br>论文主要提出了一种新的Regularization方法DropFilter，主要是对卷积核进行抑制，具体实现的时候也比较简单，只要对卷积核和bias生成对应的0 - 1mask(Bernoulli分布),然后对应和kernel相乘即可：</p>
<p><img src="DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks-6cc74f13d759b1cbb0c4a864d637b520623e0053_1_690x465.png" alt=""><br>在DropFilter的基础上作者还提出了DropFilter-PLUS方法，主要是针对卷积操作过程中的每一个位置都进行一次独立的DropFilter操作，那么对于一个224x224的输入需要计算222x222次mask操作(3x3conv)，计算量比较大，作者在论文中分析具体的conv操作(论文中提及的alx,y)最后通过对卷积的输出进行mask来等价，这就大大加速了DropFilter-PLUS的操作：<br><img src="DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks-9fa3f026fff5f15f159490c18a254286219fda82_1_585x500.png" alt=""><br>DropFilter和Dropfilter-PLUS都有一定的道理，并且简单易实现</p>
</div></div><a class="button-hover more" href="../../2018/12/19/DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/19/Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images/">Robust Face Detection via Learning Small Faces on Hard Images</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Face/">Face</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1811.11662.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.11662.pdf</a><br>论文主要是想解决人脸检测中的小脸问题，论文的motivation其实和SNIP很像，让网络去学习一个相对固定的scale，比如在本论文中anchor大小被设置为固定的16x16，32x32，64x64三个。论文主要的内容有两个部分，一部分是提出来的检测网络，另一部分就是hard image mining。</p>
<p>论文所提的检测网络backbone采用VGG16，在接入cls和reg前经过dilation conv来获得不同的感受野，其他没有特别的内容:<br><img src="Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images-9add084a4879a4cde2af6ba16849afa33dc4a245_1_690x283.png" alt=""><br><img src="Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images-e9d7b5e89400c3d6572e883406879031b064fcd8_1_482x500.png" alt=""><br>而hard image mining是image level的，作者针对图片的难易程度定义了一个变量WPAS, 简言之是综合proposal的IOU以及score信息来量化图片的难易程度，WPAS&gt;0.85被定义为简单的图片：<br><img src="Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images-5d2afb9dc7cdd4a787fc08fb6b82bda9f305572f_1_690x106.png" alt=""><br>最后在训练的时候对于当前epoch会借助前一个epoch的信息把训练数据都划分成easy和hard两类，再从easy中以0.7的概率剔除掉一些图片，剩下的图集作为当前epoch的训练数据来训练模型。</p>
</div></div><a class="button-hover more" href="../../2018/12/19/Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/17/Deep-Layer-Aggregation/">Deep Layer Aggregation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1707.06484.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1707.06484.pdf</a><br>CVPR2018的一篇关于layer aggregation的论文，论文的motivation是作者觉得目前常见的aggregation方式（FPN、U-Net…）比较shallow，作者希望利用更加deeper的连接方式来更好的融合特征。论文中作者分别提出了IDA和HDA两种连接方式。IDA应用在stage之间，HDA应用在block之间。</p>
<p>下图是论文中对IDA和HDA给出的直观图示，下图的c是IDA（Iterative Deep Aggregation），整体和FPN的连接方式比较类似，只是方向相反，浅层的特征被不断的refine与高层特征相融合。下图的d，e，f对应着HDA（Hierarchical Deep Aggregation），d是HDA最原始的表达，整体是一个树状结构，跨越不同层级的特征分层次进行特征融合，e则是在d的基础上将前面节点的父亲节点与当前节点一同考虑进行特征融合。f则是作者出于降低模型复杂度的角度将e同一层级的节点进行merge：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.41.22.png" alt=""><br>下面这幅图是论文中将IDA和HDA合并到一起具体的模型应用，也就是论文标题的Deep Layer Aggregation模型，橙色的连线是IDA的逻辑，红色框内部的连接是HDA的逻辑：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.41.53.png" alt=""><br>而对于一些比如seg的‘image-to-image’任务，只需通过增加简单的插值操作就可以实现：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.42.30.png" alt=""><br>为了验证DLA的有效性作者还是做了比较多的实验的，作者分别在Classification、Fine-grained Recognition、Seg、Boundary Detection几个主流的cv task上都做了实验，都有不同程度的提升：<br>分类的实验：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.48.56.png" alt=""><br>识别的实验：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.49.01.png" alt=""><br>论文中作者提出的IDA和HDA感觉还是很合理的，作者同时还提及residual connection在实验时对比较深的网络是有效的，对于比较浅的网络是有负面作用的。</p>
</div></div><a class="button-hover more" href="../../2018/12/17/Deep-Layer-Aggregation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/15/Deep-Mutual-Learning/">Deep Mutual Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Mutual-learning/">Mutual learning</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://cn.arxiv.org/pdf/1706.00384v1" target="_blank" rel="noopener">http://cn.arxiv.org/pdf/1706.00384v1</a><br>论文主要在讲mutual learning相互学习，deep mutual learning整体感觉和model distillation还是比较像的,只是不是用训练好的大网络来带小网络而是用一些网络相互同步学习来提高模型整体的效果:<br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.49.05.png" alt=""></p>
<p>对于其中的单个网络除了本身分类的cross entropy loss外还会涉及到KL散度来度量两个网络预测结果之间的loss（K是网络个数）：<br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.49.47.png" alt=""><br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.50.11.png" alt=""><br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.50.24.png" alt=""><br>补充一些作者针对mutual learning的有效性做的一些探究：</p>
<ul>
<li>作者认为DML相对独立的模型得到了wider minima，因此更加鲁棒，作者设计了一组实验对网络的参数增加了一些噪声，下图是增加噪声前后的结果，正常比较的时候两者相似，增加噪声扰动之后DML效果相对更好：<br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.52.25.png" alt=""></li>
<li>K个模型的mutual learning通常可以有两种表现形式，一个就是当前这个模型和其余的K-1个模型分别进行KL distance计算，另一个就是当前模型只要和其余K-1个模型的预测结果均值来进行KL distance计算，作者实验发现后者其实效果会更差，作者认为是多模型结果的融合影响了一些网络细节的teaching signal:<br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.51.58.png" alt=""></li>
</ul>
</div></div><a class="button-hover more" href="../../2018/12/15/Deep-Mutual-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/14/A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition/">Center Loss - A Discriminative Feature Learning Approach for Deep Face Recognition</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Classification/">Classification</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://ydwen.github.io/papers/WenECCV16.pdf" target="_blank" rel="noopener">http://ydwen.github.io/papers/WenECCV16.pdf</a><br>这篇论文主要的贡献就是提出了Center Loss的损失函数，利用Softmax Loss和Center Loss联合来监督训练，在扩大类间差异的同时缩小类内差异，提升模型的鲁棒性。</p>
<p><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image002.png" alt=""><br>为了直观的说明softmax loss的影响，作者在对LeNet做了简单修改，把最后一个隐藏层输出维度改为2，然后将特征在二维平面可视化，下面两张图分别是MNIDST的train集和test集，可以发现类间差异比较明显，但是类内的差异也比较明显。<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image003.png" alt=""><br>为了减小类内差异论文提出了Center Loss：<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image004.png" alt=""><br>C<sub>yi</sub>就是类的中心点特征，Cyi的计算方法就是yi类样本特征的均值，为了让center loss在神经网络训练过程中切实可行，C<sub>yi</sub>的计算是对于每一个mini-batch而言，因此结合Softmax Loss，整个网络的损失函数就变成了， λ用来平衡这两个Loss：<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image005.png" alt=""><br>用同样的网路结构只是将Softmax Loss替换成Center Loss作者在MNIST数据集上做了同样的实验，对于不同的λ值得到了如下可视化结果可以发现Center Loss还是比较明显的减小了类内差异同时类间差异也比较突出。<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image006.png" alt=""><br>在公开数据集上的表现：<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image007.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/12/14/A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/14/CRAFT-Objects-from-Images/">CRAFT Objects from Images</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1604.03239" target="_blank" rel="noopener">https://arxiv.org/abs/1604.03239</a><br>CVPR2016的一篇论文。首先CRAFT代表 Cascade Region proposal network And FasT-rcnn，本论文主要想解决RPN网络生成的区域不太精确的问题，比如对于一些外观复杂度较低的事物如树木，会因为RPN网络产生的背景区域的存在导致比较难检测或者产生FP，因此作者尝试利用cascade的方式来解决这样的问题。<br>CRAFT模型可以分成两个部分：</p>
<ol>
<li>Cascade Proposal Generation：这一部分同样可以分成两个部分，RPN和FRCN，FRCN其实就是一个Fast R-CNN网络，两个网络分别训练，RPN网络生成相对粗糙的区域，FRCN用RPN的输出作为输入对RPN的结果进行进一步的refine，提高了候选区域的质量也减少了背景框：<br><img src="CRAFT-Objects-from-Images-image002.png" alt=""></li>
<li>Cascade Object Classification：Cascade Object Classification部分由两个FRCN网络级联而成，Cascade Proposal Generation部分的输出会作为FRCN1的输入，FRCN1的输出摒弃掉“背景”类作为FRCN2的输入，实现细节上两个模型复用参数。这里需要注意的是，作者认为为了学习类间的细微差别，模型复用了被Fast RCNN抛弃的 One-vs-Rest的分类方式，用N个二分类的交叉熵损失函数的和作为最终的loss。<br><img src="CRAFT-Objects-from-Images-image003.png" alt=""><br>最终在VOC上面的表现效果：<br><img src="CRAFT-Objects-from-Images-image004.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2018/12/14/CRAFT-Objects-from-Images/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/14/Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network/">Weighted Channel Dropout for Regularization of Deep Convolutional Neural Network</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Regularization/">Regularization</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://home.ustc.edu.cn/~saihui/papers/aaai2019_weighted.pdf" target="_blank" rel="noopener">http://home.ustc.edu.cn/~saihui/papers/aaai2019_weighted.pdf</a><br><strong>【Summary】</strong>AAAI2019的一篇关于Regularization的论文，整体感觉可以理解为SENet思想在Regularization中的应用。论文中作者提出了Weighted Channel Dropout(WCD)的逻辑（为每一个channel计算权重、构造取舍的概率…）来对channel进行选择性的DropOut。</p>
<p><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.03.34.png" alt=""><br>上面这张图是WCD的整个Pipeline，整个结构主要可以分成三个部分：</p>
<ul>
<li><strong>Rating Channels</strong>：这个模块主要是为每一个channel构造一个全权重，逻辑很简单就是Global Average Pooling：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.18.15.png" alt=""></li>
<li><strong>Weighted Random Selection (WRS)</strong>: 这一部分逻辑其实也很简单，作者定义了每个channel被选择的概率为：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.20.57.png" alt=""><br>那么在具体计算的时候是这样做的,r<sub>i</sub>是一个（0，1）之间的随机数，对于这两个计算方式的等价性，论文中作者给出了引用论文可以具体参考：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.21.58.png" alt=""><br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.23.52.png" alt=""></li>
<li><strong>Random Number Generation (RNG)</strong>: 这个可以理解为在WRS基础上的一个补充吧，在实际应用中有时间可用的数据量比较小，那么这种情况下通常会用pretrain的模型来初始化网络，那么作者认为这种情况下channel之间的差距会更大，可能只有少部分的channel会有比较大的响应，那么根据WRS选出来的channel可能也很像，所以针对WRS选中的channel依然会有1 - q的概率被抛弃,那么最终每一个channel的状态就和WRS、RNG都有关：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.30.54.png" alt=""><br>论文中取alpha为：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.31.35.png" alt=""><br><strong>一些实验结果</strong></li>
</ul>
<p><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.32.15.png" alt=""><br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.32.21.png" alt=""><br>作者还贴了一个和SENet比较的结果：</p>
<p><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.44.02.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/12/14/Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/11/Parallel-Feature-Pyramid-Network-for-Object-Detection/">Parallel Feature Pyramid Network for Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-22</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener">https://arxiv.org/abs/1612.03144</a><br>ECCV2018的一篇论文,这篇论文从某种程度上来说是为了解决小物体检测的问题，作者从feature map特征表示好坏的角度来分析目前常用检测模型的一些不足。论文分别可视化SSD、FPN、PFPNET（本文所提模型）对同样输入图片的feature map，从图中可以看出来SSD对物体的轮廓细节描述比较差，FPN对于一些遮挡物体的特征表示比较差。PFPNET则相对好一些，至于为什么好，以及这个结构设计的理由论文貌似并没有解释。</p>
<p><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image002.png" alt=""><br>论文的主要内容：</p>
<ol>
<li>Parallel Feature Pyramid Network for Object Detection （PFPNET）模型结构：整体结构和RetinaNet比较像，不同的是PFPNET的multi-scale是一个并行的结构，和RetinaNet中的feature layer之间是一个串的结构是不一样的。模型首先利用Base Model（论文中用的是VGGNET-16）得到PFPNET结构的输入（DxWxH），然后利用SPP产生不同scale的feature map（FP Pool），不同scale的feature map长宽分别以2倍大小减少，对于每一个scale分别再利用Bottleneck Layer完成特征的转化。最后就是特征的融合（MSCA），模型基本结构图：<br><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image003.png" alt=""></li>
<li>Multi-scale context aggregation（MSCA）：本文用的特征融合还是常见的concatenate方式，对于第n个分支最后的融合的特征Pn, 它有第一层FP Pool中与之对应的feature map以及第二层FP Pool中其他的N-1个feature map相融合而得到。然后对于最后的n个feature map分别进行cls + reg的任务即可。<br><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image004.png" alt=""></li>
<li>Some details: 论文主要用refinedet和ssd作为baseline来比较，PFPNet-S代表和SSD相同的anchor设置，PFPNet-R代表和RefineDet相同的anchor设置，300和512则代表具体输入图片的size。</li>
</ol>
<p>实验结果，论文主要对比的对象是SSD和RefineDett，其中从整体上来看在COCO数据集上PFPNET的表现貌似并不是最优的，作者则具体分析了在小物体等场景下的优势：<br><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image006.png" alt=""><br><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image005.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/12/11/Parallel-Feature-Pyramid-Network-for-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/04/An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/">An Analysis of Scale Invariance in Object Detection – SNIP</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1711.08189" target="_blank" rel="noopener">https://arxiv.org/abs/1711.08189</a><br><strong>【Summary】</strong>CVPR2018的一篇Oral，主要在研究scale invariance或者说是domain shift的问题，论文所提出的SNIP方法不同于multi scale的逻辑，可以理解为把网络输入的物体norm到一个相对固定的scale，inference的时候也做同样的策略，这样可以避免训练和测试数据集的scale invariance的问题。</p>
<p>论文在分析scale invariance问题的时候做了几组实验：</p>
<ul>
<li><p><strong>实验一</strong>：训练三个模型CNN-B(224x224作为输入的图)、CNN-S（和CNN-B类似的图片只是针对输入图片的尺寸修改了第一层conv的stride，可以理解为针对输入图的尺寸做的Resolution Specific Classifier）以及CNN-B-FT（224x224输入图预训练模型 + 用低分辨率图片upsample搭配224x224来finetune）。CNN-B在48x48~224x224范围的输入图上效果如下图a，可见效果随分辨率大小逐渐变好，224x224最好。CNN-B-FT的效果也有一定提升。从结果中也可以发现domain shift对模型的影响：</p>
<p><img src="An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP-屏幕快照 2018-12-04 下午9.28.51.png" alt=""></p>
</li>
<li><p><strong>实验二</strong>:验证集固定为1400x2000的分辨率</p>
<ol>
<li>分别训练800x1400和1400x2000的检测模型，1400x2000效果最好，但是提升有限，作者认为是因为提升分辨率有助于小物体的检测但是对于大物体的检测是有坏处的</li>
<li>训练1400x2000的模型忽略特别大的物体，最后的结果比800x1400的效果更差，因为抛弃了比较多的数据，丢失了variation in appearance and pose</li>
<li>Multi-Scale Training最后的结果和800x1400的效果差不多。作者认为过程中同样出现了过大和过小的物体。</li>
</ol>
<p><img src="An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP-截屏2019-12-2123.05.59.png" alt=""></p>
</li>
</ul>
<p>针对上面实验的结果论文所提出的SNIP方法想法很直接，可以理解为是Image Pyramid的改进。SNIP通过在训练和inference的时候控制物体到一个固定的scale来保证检测的效果。实验一和实验二的结果也支持来这一点。具体实现的时候针对不同scale的输入，论文中都分别定义了一些proposal的面积范围，网络训练的时候只有落在给定范围内的proposal才会回传梯度。这也是SNIP中Scale Normalization的实际含义：</p>
<p><img src="An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP-屏幕快照 2018-12-04 下午9.24.33.jpg" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/12/04/An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/11/24/SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts/">SGDR: Stochastic Gradient Descent with Warm Restarts</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Learning-Strategy/">Learning Strategy</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1608.03983" target="_blank" rel="noopener">https://arxiv.org/abs/1608.03983</a> Github: <a href="https://github.com/loshchil/SGDR" target="_blank" rel="noopener">https://github.com/loshchil/SGDR</a><br><strong>【Summary】</strong>ICLR2017一篇关于学习率的论文，论文的核心比较直接就是提出了基于cosine的学习率 warm restart逻辑，然后论文的大篇幅都是围绕这个learing rate进行了比较多的实验。论文所提的SGDR通常只需要原有模型1/2-1/4的训练epoch就可以得到差不多甚至更好的效果。</p>
<p>论文所提的cosine learning rate 公式如下，n<sub>min</sub>、n<sub>max</sub>就是学习率的区间，T<sub>cur</sub>表示当前经过多少个epoch了，T<sub>i</sub>可以理解为周期。因为从下面这个公式当T<sub>cur</sub> = T<sub>i</sub>时，n<sub>t</sub> = n<sub>min</sub>：</p>
<p><img src="SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts-c4d6e75499fcd3406791dd38969c8d52b2f59521_1_690x119.png" alt=""><br>作者在具体实验的时候其实还涉及到另一个参数n<sub>mult</sub>代表周期的系数，下图是不同的T<sub>i</sub>和T<sub>mult</sub>画出来的示意图：</p>
<p><img src="SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts-d82c1f3a92421e0ca83d33a2f7b0fe29374e7d62_1_690x288.jpg" alt=""></p>
<p>下图是作者在CIFAR-10和CIFAR-100数据集上用Wide Residual Neural Network（depth=28&amp;width=10）做的实验，效果很直观，SGDR收敛速度明显比较快：</p>
<p><img src="SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts-76589c0bc925d1e5325639557cd6ecdb400389da_1_447x499.jpg" alt=""><br>下表是具体的实验结果，从结果上看效果也是不错的：</p>
<p><img src="SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts-4441390556648037aa6d983e2edfb07a2a075d6a_1_629x500.png" alt=""><br>此外作者也从ensemble、downsampled imagenet dataset的角度做了不同的实验，结果也都很好：<br>Ensemble:</p>
<p><img src="SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts-f2310a3f482911fcebf25cb90947d106fcf3e22a_1_690x130.png" alt=""><br>Downsampled imagenet dataset:</p>
<p><img src="SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts-d308a8fb8fb5028f53e7d108006f93a5d5364a52_1_632x500.jpg" alt=""></p>
<p>论文所提的这种warm restart逻辑感觉还是很合理的，可以优化梯度下降中鞍点以及局部最小解的问题，有助于模型能快速收敛。</p>
</div></div><a class="button-hover more" href="../../2018/11/24/SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/11/19/Unconstrained-Face-Alignment-without-Face-Detection/">Unconstrained Face Alignment without Face Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Face/">Face</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://ieeexplore.ieee.org/document/8014992" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/8014992</a><br><strong>【Summary】</strong> CVPR2017 Workshop一篇关于人脸关键点检测的论文，应该主要是在参加CVPR2017的一个Menpo Challenge。  整篇论文的主要内容可以理解为把OpenPose中的一些做法应用到人脸关键点定位任务中。<br>下图是论文中给出的对所提方法整个pipeline的示意，论文所提方法主要可以分成两个部分Basic Landmark Prediction Stage（BLPS）和 Whole Landmark Regression Stage（WLRS）。第一部分负责得到人脸关键点的粗定位（其实只是所有关键点中几个主要的点比如眼球、鼻尖等），第二部分负责在第一阶段的基础上进行进一步的refine：<br><img src="Unconstrained-Face-Alignment-without-Face-Detection-7294485d9c10d1d220cb3c0ea2039e4fefcb135d_1_690x370.jpg" alt=""></p>
<ul>
<li><strong>Basic Landmark Prediction Stage</strong>：这一部分可以直接理解为OpenPose 中PAF在人脸关键点中的应用。具体细节可以直接参考OpenPose那篇文章。只是在本论文中当前阶段只处理人脸关键点中主要的几个点（左右眼球、鼻尖和左右嘴角共5个点）。<br><img src="Unconstrained-Face-Alignment-without-Face-Detection-216534462f9c38b34a4971687f33a92855b698a9_1_584x499.jpg" alt=""></li>
<li><strong>Whole Landmark Regression Stage</strong>：这一部分是直接接在BLPS之后进行的。首先会利用BLPS的结果根据关键点的可见性判断人脸的Pose，总共分成三类：left profile、right profile、semi-frontal。那么论文本身是对这三种pose计算过模版脸的，所以可以直接align 到模版脸上，最后直接送到回归网络回归最后的精确结果。感觉这也是目前人脸关键点任务中比较常用的方法：\<br><img src="Unconstrained-Face-Alignment-without-Face-Detection-167f3b8032a6d164916e82241af84a4a205b6060_1_678x500.png" alt=""></li>
</ul>
<p>论文用的训练数据来自300W + Menpo + CelebA，作者在300W数据集上做的实验结果：</p>
<p><img src="Unconstrained-Face-Alignment-without-Face-Detection-8c807a07df3ad3b1446d8e2ac598ae0448a0eea0.png" alt=""></p>
<p>这篇论文可以理解为OpenPose方法在人脸关键点定位任务上的直接应用，从方法整体的pipeline上看其实和目前我们在做的landmark定位逻辑是一致的，都是先给部分点的位置然后align在精细refine所有的点。</p>
</div></div><a class="button-hover more" href="../../2018/11/19/Unconstrained-Face-Alignment-without-Face-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/11/15/Squeeze-and-Excitation-Networks/">Squeeze-and-Excitation Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="noopener">https://arxiv.org/abs/1709.01507</a><br>Github:<a href="https://github.com/hujie-frank/SENet" target="_blank" rel="noopener">https://github.com/hujie-frank/SENet</a></p>
<p>CVPR2017的一篇论文，CNN网络卷积操作本身是整合空间信息和channel信息的，论文作者的motivation是显式地对channel做attention来提高模型的表达能力，论文主要的内容就是一个SE Block：<br><img src="Squeeze-and-Excitation-Networks-image002.png" alt=""><br>SE Block主要分成两个部分：</p>
<ol>
<li>Squeeze：上图中的Fsq, 本质就是一个Global Average Pooling，将H x W x C映射成1x1xC：<br><img src="Squeeze-and-Excitation-Networks-image003.png" alt=""></li>
<li>Excitation：上图中的Fex, 本质就是两个全连接层来学习channel的权重信息，W1是第一层FC参数，输出为C/r，W2为第二层FC参数，输出恢复到C，r为压缩比例，用来减少参数量：<br><img src="Squeeze-and-Excitation-Networks-image004.png" alt=""><br>最后将excitation得到的输出施加到对应的channel上就可以得到最终的输出（图中的Fscale）SE Block本身的设计是一个嵌入模块，所以它可以方便的结合现有的网络结构，论文中给了两个示例SE-Inception和SE-ResNet：<br><img src="Squeeze-and-Excitation-Networks-image005.png" alt=""><br><img src="Squeeze-and-Excitation-Networks-image006.png" alt=""><br>论文中还重点论述了SE Block不会对原模型带来比较大的复杂度，对于输入图片大小为224x224时，ResNet50约3.86GFLOPS，SE-ResNet-50约3.87GFLOPS，实际运行时间两者差别也不是很大，贴一张具体的数据：<br><img src="Squeeze-and-Excitation-Networks-image007.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2018/11/15/Squeeze-and-Excitation-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/11/14/Face-R-CNN/">Face R-CNN</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Face/">Face</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1706.01061" target="_blank" rel="noopener">https://arxiv.org/abs/1706.01061</a><br>腾讯AI Lab发表在CVPR2017上面的论文，在2017年可以做到FDDB 和 Wider Face数据集上面的最好结果，Face R-CNN是基于Faster R-CNN框架的模型，基本结构论文给出了比较详细的示意图：<br><img src="Face-R-CNN-image002.png" alt=""><br>模型主要有以下几个关键点：</p>
<ol>
<li>Center Loss：论文直接把人脸识别中的center loss沿用到人脸检测中来，出发点还是一样利用Softmax Loss来扩大类间差异，利用Center Loss来减小类间差异，以此来增强模型的鲁棒性，只是在人脸检测中类别总数只有2，人脸和非人脸。<br><img src="Face-R-CNN-image003.png" alt=""><br>因此模型最终的Loss就变成了cls loss + reg loss + center loss：<br><img src="Face-R-CNN-image004.png" alt=""></li>
<li>OHEM：利用标准的OHEM做法，以loss作为key排序取Top N作为hard example，作者特别说明使用Center Loss可以有效的控制hard example中的postive和negative的样本数。在最后实现的时候是分别在postive和negative样本上应用OHEM并且控制每个batch两者的比例为1:1.</li>
<li>Multi-Scale Training：这一步其实就是把图片resize成不同的大小进行训练来覆盖不同分辨率……<br>最后实验的结果：<br><img src="Face-R-CNN-image005.png" alt=""></li>
</ol>
<p>论文整体感觉更偏工程化，通过不断尝试融合现有的方法提高模型在数据集上的表现，没有很特别的地方。</p>
</div></div><a class="button-hover more" href="../../2018/11/14/Face-R-CNN/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/10/28/Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping/">Associative Embedding: End-to-End Learning for Joint Detection and Grouping</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-27</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Skeleton/">Skeleton</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://papers.nips.cc/paper/6822-associative-embedding-end-to-end-learning-for-joint-detection-and-grouping.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/6822-associative-embedding-end-to-end-learning-for-joint-detection-and-grouping.pdf</a><br><strong>【Summary】</strong> Pose estimation 任务中另一个典型的bottom up模型，论文的motivation感觉比OpenPose的PAF更加直观易懂，就是为每一个joint学习一个tag用来标记一组joint。然后再用贪心的逻辑来做group。</p>
<p>下图是论文中给出的整个方法的Pipeline，整个网络总共有两个分支，一个输出关键点位置的heatmap，文中称之为detection，另一个分支就是本文的核心associative embedding，论文中称之为grouping：<br><img src="Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping-3c5dd349b51723e15a8ec5ea15463e029db0fb03_1_690x338.jpg" alt=""><br>至于论文中用到的backbone网络是比较常见的hourglass：<br><img src="Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping-e180bb474a3cf204c9540778d188a8f57ffa2ef0.png" alt=""><br><strong>Associative Embedding</strong><br>论文中提及的embedding可以理解为对人物个体的标记，和NLP中的word embedding一样，embedding的维度其实可以是任意的，本论文中作者通过实践觉得1维的embedding 就足够了，所以对于detection的每一个channel在grouping中都一个同样大小的channel与之对应。<br>在网络训练的时候Detection Loss就是普通的MSE而Grouping Loss是如下设计的：<br><img src="Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping-3d23af6063c886a039dec53aa3ac965505139a8c.png" alt=""><br>其中：<br><img src="Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping-2193d834631344a19a46cd6ac97a7346f44364b4.png" alt=""><br>hk是grouping中一个输出的channel，x为具体的位置，这主要涉及到论文中的reference embedding，reference embedding就是一个人物个体所有joint 在grouping中输出的均值作为对这个人物个体的表示。而在具体的Loss函数中前半段就是把输入同一个人物个体的joint尽量拉近，而公式的后半段就是把不同的人物个体相互拉开。<br>具体Inference的时候通过某一个joint的heatmap的峰值来确定检测到的人的pool，然后再依次去利用其他joint的tag来和这个pool里面的人进行match。如果当前这个joint无法和pool里面的任意一个人match，那么就会作为单独的一个人加入到这个pool，然后一直进行这样的贪心流程。</p>
<p>作者在COCO和MPII数据集上分别做了测试，从结果上来看点还是比较高的：<br><img src="Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping-8c807a07df3ad3b1446d8e2ac598ae0448a0eea0.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/10/28/Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/10/22/SNIPER-Efficient-Multi-Scale-Training/">SNIPER: Efficient Multi-Scale Training</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1805.09300" target="_blank" rel="noopener">https://arxiv.org/abs/1805.09300</a> Github: <a href="https://github.com/MahyarNajibi/SNIPER" target="_blank" rel="noopener">https://github.com/MahyarNajibi/SNIPER</a><br><strong>【Summary】</strong>算是在SNIP版本上的改进吧，主要想优化multi scale训练的效率问题。论文提出了chip的概念，其实就是原图中的某一块区域，在本论文中chip的大小就是512 x 512的矩形方块。网络的输入不再是原图，而是这些生成的chip。SNIPER的主要内容就是这些chip的生成逻辑。</p>
<p>作为网络的输入，chip也分为postive chip和negative chip, 下图是postive chip的生成示例。对于给定的一张图，会有512 x 512这样大小的chip框在原图上滑动，间隔为32个pixel。包含valid gt（valid或者invalid的定义和SNIP那篇论文逻辑一样）的chip被称为postive chip。由于对于给定的一张图片不会把每一个chip都送给网络去训练，所以通常都是贪心的选择包含valid gt最多的chip作为这张图的postive chip：</p>
<p><img src="SNIPER-Efficient-Multi-Scale-Training-2feaec373ba84ada0c26428b063370d8588e35d8_1_690x386.jpg" alt=""><br>下面这张图则是negative chip的生成示例，论文中作者生成的negative chip是想包含那些比较难解的FP，所以作者用了一个简单的RPN网络来生成negative chip，下图的红点代表一个proposal（已经移除掉被postive chip包含的那些了）。negative chip被定义为至少含M个proposal的那些chip(论文中M貌似没给出具体的值)：</p>
<p><img src="SNIPER-Efficient-Multi-Scale-Training-54af81d1f0fbbac701cb898fe22e2ee48d188efe_1_690x341.png" alt=""><br>贴一张实验结果：</p>
<p><img src="SNIPER-Efficient-Multi-Scale-Training-6bcbdd2062f585821204274dfe18fd845f2a4544_1_690x316.png" alt=""></p>
<p>感觉这篇论文的重点用论文中的一句话可以很好的概括”it implies that very large context during training is not important for training high-performance detectors but sampling regions containing hard negatives is”</p>
</div></div><a class="button-hover more" href="../../2018/10/22/SNIPER-Efficient-Multi-Scale-Training/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/10/14/RMPE-Regional-Multi-Person-Pose-Estimation/">RMPE: Regional Multi-Person Pose Estimation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-27</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Skeleton/">Skeleton</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1612.00137" target="_blank" rel="noopener">https://arxiv.org/abs/1612.00137</a><br><strong>【Summary】</strong>ICCV2017关于Pose Estimation的论文，主要是基于目前Top Down方法的改进。论文主要想解决一个问题，就是Top Down方法中human detector出的框不准或者比较冗余的问题。STN来映射更准的框， Pose NMS 来更好的解决冗余框的问题。</p>
<p>下图是论文中给出的对于RMPE方法的简单示例，整个Pipeline针对human detector出框不准或者比较冗余的问题分别进行了针对性的设计，下图中的前半段STN + SPPE + SDTN主要用于fix human detector出框不准的问题，后半段的Pose NMS主要用来解决冗余的问题：</p>
<p><img src="RMPE-Regional-Multi-Person-Pose-Estimation-40073134aa3c4bacc77a9e9fda9a039e18627b2a_1_690x253.jpg" alt=""></p>
<ul>
<li><strong>SPPE的改进：</strong> SPPE（Single Person Pose Estimation）主要改进的地方就是引入STN和SDTN来实现更加精确的pose estimation。细节上STN接受human detector输出的proposal（具体实现的时候会在原有的proposal基础上，长宽各扩展30%以保证覆盖全部的人体区域），STN生成的仿射变换矩阵将当前的proposal映射的更加精确，然后再输入SPPE网络中（实际实验时是4-stack hourglass结构）得到具体的joint的位置，然后通过STN的逆变换SDTN得到原始的坐标位置。下图中的Parallel SPPE作用是对STN进行进一步的监督，监督的Label是已经center-located的pose，所以当然也就不需要SDTN这一步了。从下图中也可以比较直观的看出来，上下两个branch label还是不一样的，为了映射回原坐标，上半分支的SDTN是必要的：<br><img src="RMPE-Regional-Multi-Person-Pose-Estimation-bba38f5d3a2f3183198d1b3e1edad64fdf69cf6d_1_690x253.jpg" alt=""></li>
<li><strong>Pose NMS：</strong> 对于Pose NMS作者主要在研究一件事就是如何来定义Pose 的冗余以及消除冗余的标准，就像物体检测的NMS用IoU和confidence的逻辑一样。论文中作者用如下的方式来定义：<br><img src="RMPE-Regional-Multi-Person-Pose-Estimation-0dfc09cf7e37b3b1ca4ff52221f214f16ba73bb4.png" alt=""><br>其中，H<sub>Sim</sub>用来建模距离上的相似度，K<sub>Sim</sub>用来建模confidence之间的关系，B(K<sub>i</sub><sup>n</sup>)代表1以K<sub>i</sub><sup>n</sup>为中心的矩形，矩形大小为这个Pose对应框的1/10：<br><img src="RMPE-Regional-Multi-Person-Pose-Estimation-f21a4f8fbd194882b2ffb4506b29dfeda3696354.png" alt=""><br><img src="RMPE-Regional-Multi-Person-Pose-Estimation-144986445bc91b1f0003ee01a3ce41c772544bdb.png" alt=""></li>
<li>此外作者也针对所提的RMPE结构设计了Data Augmentation方法，主要是根据原子pose进行聚类，然后对于输入的图像首先对其进行分类，然后利用对应的offset分布构造一组新的数据，具体的分布信息：<br><img src="RMPE-Regional-Multi-Person-Pose-Estimation-36ee4b370f48e74a1f0f269815e6d4c9db9a7d06.jpg" alt=""><br>在MPII数据集上的表现：<br><img src="RMPE-Regional-Multi-Person-Pose-Estimation-70e6ebcbc5feaad8636963960565eb32feae49aa_1_690x250.png" alt=""><br>在COCO Chanllenge上的表现：<br><img src="RMPE-Regional-Multi-Person-Pose-Estimation-4b243ddf5903093632852b6f2830838278676f40.png" alt=""></li>
</ul>
</div></div><a class="button-hover more" href="../../2018/10/14/RMPE-Regional-Multi-Person-Pose-Estimation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/09/25/Pelee-A-Real-Time-Object-Detection-System-on-Mobile-Devices/">Pelee: A Real-Time Object Detection System on Mobile Devices</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-22</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>这篇论文主要做的内容是在手机终端上进行实时的物体检测并提出了PeleeNet模型，不同于目前的MobileNet、ShuffleNet等基于 depthwise separable convolution的模型，PeleeNet是基于传统的卷积来实现的，因为作者认为depthwise separable convolution操作在诸多的框架下都没有有效的支持……<br>论文主要借鉴了DenseNet、DSOD、Inception-V4等现有的网络，模型结构在论文中给出了比较详细的列表，几个需要注意的细节：</p>
<ol>
<li>作者参考GoogleNet在DenseBlock中引入2-way dense layer来获得不同大小的感受野。</li>
</ol>
<p><img src="Pelee-A-Real-Time-Object-Detection-System-on-Mobile-Devices-image004.png" alt=""></p>
<ol start="2">
<li>DenseNet中表现较好的DenseNet-BC结构是在原有的基本DenseNet结构基础上加入了Bottleneck和Compression，但是作者认为compression逻辑的加入影响模型的特征表达，所以下表中的Transition Layer没有compression的逻辑，输入输出channel数保持一致。</li>
</ol>
<p><img src="Pelee-A-Real-Time-Object-Detection-System-on-Mobile-Devices-image002.png" alt=""></p>
<ol start="3">
<li>Composite Function中用Post-activation替换掉DenseNet中的Pre-activation，以此来在inference阶段加速计算。</li>
<li>用Bottleneck Layer控制输出的channel数不大于输入的channel数，此操作除了可以节省28.5%的计算量，对准确率的影响也比较小。</li>
</ol>
<p><img src="Pelee-A-Real-Time-Object-Detection-System-on-Mobile-Devices-image003.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/09/25/Pelee-A-Real-Time-Object-Detection-System-on-Mobile-Devices/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/09/24/Cascade-R-CNN-Delving-into-High-Quality-Object-Detection/">Cascade R-CNN: Delving into High Quality Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Cascade/">Cascade</a></div></div><div class="post-content"><div class="main-content content"><p>解决问题：<br>目前的检测模型通常需要通过设置IoU阈值来定义Positive和Negative bbox，作者发现最终检测模型的表现和IoU阈值的设置相关，通过下图的c，d可以发现低IoU阈值通常会对低IoU样本表现更好，因为不同IoU阈值下的样本分布不一样，一个IoU阈值很难对所有的样本都有效。因此为了解决这个问题，论文提出了Cascade R-CNN 模型。</p>
<p><img src="Cascade-R-CNN-Delving-into-High-Quality-Object-Detection-image002.png" alt=""></p>
<p>论文实验的Cascade R-CNN模型基于典型的Two-Stage结构，Cascade R-CNN模型有多个Classification和Bounding Box header（下图中的Cx，Bx），前一个header refine过的Bounding Box 会作为下一个header的输入，并且IoU的阈值逐层增加，B0代表RPN的结果。通过这种方式实现对于每一级header输入数据的resample，保证了每一级header都有足够的正样本。</p>
<p><img src="Cascade-R-CNN-Delving-into-High-Quality-Object-Detection-image003.png" alt=""></p>
<p>实验对比，可以发现cascade rcnn的效果还是有比较大的提升，但同时也大幅大增加了模型的复杂度</p>
<p><img src="Cascade-R-CNN-Delving-into-High-Quality-Object-Detection-image004.png" alt=""></p>
<p>思考：<br>Cascade R-CNN结构是现有检测模型优化的思考方向，但是从上图第二章表中也可以看出加入cascade 结构在提高检测效果的同时也大幅度增加了模型复杂度.</p>
</div></div><a class="button-hover more" href="../../2018/09/24/Cascade-R-CNN-Delving-into-High-Quality-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/09/24/Single-Shot-Refinement-Neural-Network-for-Object-Detection/">Single-Shot Refinement Neural Network for Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>论文致力于研究整合two-stage检测器和one-stage检测器的优点提出了RefineDet网络结构。</p>
<p><img src="Single-Shot-Refinement-Neural-Network-for-Object-Detection-image003.png" alt=""></p>
<p>网络结构整体借鉴了SSD模型并结合了cascade逻辑，整个网络主要分成三个部分：</p>
<ol>
<li>Anchor  Refinement Module（ARM）：这一模块的模型结构和SSD比较类似，不同的是它只做二分类，ARM对anchor进行初步的refine并得到相对粗糙的位置信息和分类信息，这一部分信息将作为ODM模块的输入。<br>特别的，为了缓解类别失衡问题，论文提出了Negative Anchor过滤逻辑，将negative confidence大于某一个阈值（如0.95）的anchor全部舍弃掉不送入ODM模块。</li>
<li>Object Detection Module (ODM): 这一模块的结构和ARM类似，并且两者对应layer的size是一致的，论文中给出的结构图例比较直观。ODM模块主要是借助ARM模块refine过后的anchor进行进一步细致的处理并最终输出物体的类别信息和bounding box的位置信息。在具体实现的时候，ARM和ODM之间采用了类似FPN特征融合的方法将ARM高层的feature map与低层的feature map相融合并与ODM对应layer的feature map一同作为下一层layer的输入，以此来提高检测的效果。</li>
<li>Transfer Connection Block (TCB)：TCB主要用来融合低层和高层feature map，论文给的图示比较清楚主要就是对高层feature map进行deconv并与低层feature map相加；</li>
</ol>
<p><img src="Single-Shot-Refinement-Neural-Network-for-Object-Detection-image002.png" alt=""></p>
<p>实验结果：</p>
<p><img src="Single-Shot-Refinement-Neural-Network-for-Object-Detection-image004.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/09/24/Single-Shot-Refinement-Neural-Network-for-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/09/24/CoupleNet-Coupling-Global-Structure-with-Local-Parts-for-Object-Detection/">CoupleNet: Coupling Global Structure with Local Parts for Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>R-FCN将RoI划分成区域通过整合各个区域的信息来表征最后的结果，而CoupleNet的motivation是利用R-FCN中对局部信息的学习能力再整合全局和context信息来提升模型的检测效果。</p>
<p><img src="CoupleNet-Coupling-Global-Structure-with-Local-Parts-for-Object-Detection-image002.png" alt=""></p>
<p>CoupleNet网络主要分成两个分支：</p>
<ol>
<li>Local FCN：引入R-FCN中的Position-sensitive score maps 和 Position-sensitive RoI pooling 得到局部信息给出的结果，对于分类会输出C+1维的特征向量，具体细节直接参考R-FCN即可。</li>
<li>Global FCN：这个分支主要用来整合RoI的全局信息和context信息，上图中的两个RoI Pooling就是分别来映射这部分特征的，第一部分是全局的特征信息，第二部分是在第一部分RoI的基础上范围向外扩大2倍再进心RoI Pooling这样可以引入部分的context信息来增强特征提高检测的效果，最后两者feature map直接concat到一起经过kxk和1x1两个卷积层之后同样得到C+1维的特征向量</li>
<li>Coupling structure：两个分支输出的整合作者也做了很多的实验，(L2 normalization layer , 1x1 conv layer)  x (element-wise max, element-wise product, element-wise sum) 共6种组合中1x1 conv + element-wise sum 表现最好。</li>
</ol>
<p>实验结果，速度方面因为CoupleNet相当于在R-FCN的基础上引入了Global FCN的分支，所以很显然速度要慢于R-FCN的，但还是明显快于faster rcnn的，检测效果方面效果也很好。</p>
<p><img src="CoupleNet-Coupling-Global-Structure-with-Local-Parts-for-Object-Detection-image003.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/09/24/CoupleNet-Coupling-Global-Structure-with-Local-Parts-for-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/09/21/DSOD-Learning-Deeply-Supervised-Object-Detectors-from-Scratch/">DSOD: Learning Deeply Supervised Object Detectors from Scratch</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>论文主要想解决的问题是如何直接去训练一个detection模型，因为现在detection模型都是利用ImageNet数据预训练的，灵活性相对较差，分类和检测的场景也不一致，因此论文提出了DSOD模型。<br>论文中提出的DSOD模型结构通过下表可以直观的看到，Stem、Dense Block、Transition Layer都是参考现有的研究成果，论文主要的贡献是<br> Transition w/o Pooling Layer 和 Dense Prediction Structure：</p>
<ol>
<li><strong>Transition w/o Pooling Layer</strong>：这个结构主要用来实现增加Densen Block模块个数的同时保持输出的size不变，从表中可以直观的看到其实就是1x1的卷积，因为传统densenet中的transition layer会利用pooling层来对feature map下采样，这里用Transition w/o Pooling Layer来保持输出的size与transition layer区分开。</li>
</ol>
<p><img src="DSOD-Learning-Deeply-Supervised-Object-Detectors-from-Scratch-image002.png" alt=""></p>
<ol start="2">
<li><strong>Dense Prediction Structure</strong>：论文中用下图来解释Dense Prediction Structure，和常用的结构（左侧）相比较主要的差别在于 Learning Half and Reusing Half，每一层网络的输入都是上一层的输出feature map 和 前面层的下采样结果的concatenate，因此对于每一层的输出一半是通过学习得到的另一半则是直接从前面层下采样得到的，这样可以直接复用前面的信息。</li>
</ol>
<p><img src="DSOD-Learning-Deeply-Supervised-Object-Detectors-from-Scratch-image003.png" alt=""></p>
<p>具体的实验结果，或许看完这篇文章最好奇的是从头训练的检测模型和预训练过的检测模型具体效果的好坏，论文的最后作者利用<br> DS/64-12-16-1做了比较，发现从头训练的模型效果要好于预训练过的模型（70.7% vs 70.3%）</p>
<p><img src="DSOD-Learning-Deeply-Supervised-Object-Detectors-from-Scratch-image004.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/09/21/DSOD-Learning-Deeply-Supervised-Object-Detectors-from-Scratch/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/09/01/R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks/">R-FCN: Object Detection via Region-based Fully Convolutional Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1605.06409" target="_blank" rel="noopener">https://arxiv.org/abs/1605.06409</a> Github:<a href="https://github.com/daijifeng001/r-fcn" target="_blank" rel="noopener">https://github.com/daijifeng001/r-fcn</a><br>R-FCN网络设计的motivation是为了增强物体检测网络位置敏感性，提高物体检测的精度和速度。它的整体结构是一个two stage的网络，RPN网络分支生成Proposal Candidate RoIs ，另一个分支基于此做进一步的refine。</p>
<p><img src="R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks-image002.png" alt=""></p>
<p>R-FCN网络结构设计的核心是Position-sensitive score maps 和Position-sensitive RoI pooling ：</p>
<ol>
<li><strong>Position-sensitive score maps</strong>：不妨假设RoI的大小为wxh，那么将RoI均分成k x k个区域，那么每个区域的大小就约为w/k x h/k，对于C分类任务，最后的输出就有kxkx(C+1)个channel，用来表征各个区域的分类信息。而对于候选框的位置，最后的输出是4xkxk个channel。</li>
</ol>
<p><img src="R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks-image003.png" alt=""></p>
<ol start="2">
<li><strong>Position-sensitive RoI pooling</strong>：这一部分主要用来整合Position-sensitive score maps的信息得到最终的结果，Pooling公式中的i，j代表kxk个bin的id信息，c为C个类别中的一个，x0，y0为区域的左上角坐标，那么实际上在整个论文中用的是average pooling，作者也提到max pooling也是可以的。</li>
</ol>
<p><img src="R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks-image004.png" alt=""></p>
<p>所以经过Position-sensitive RoI pooling之后输出为kxkx(C + 1), 对于每一个类别c最后的打分是kxk个值的均值，因此最后的输出变成C+1维的向量，最后通过softmax得到最后的分类结果。对于bbox框位置的回归也和分类类似，只是channel数是4xkxk，最后得到4维的向量作为位置信息的偏移量。</p>
<ol start="3">
<li>实验结果，速度相比较faster rcnn还是有比较明显的提升，检测效果也不错。</li>
</ol>
<p><img src="R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks-image005.png" alt=""></p>
<p>R-FCN网络的设计有一个特点，在得到score maps之后Position-sensitive RoI pooling没有引进其他的参数，无疑有助于train和inference的速度，通过划分区域人为的对位置做细分也有实际的意义，但是个人对regression阶段也用同样的操作不是很理解..</p>
</div></div><a class="button-hover more" href="../../2018/09/01/R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks/#more">阅读全文</a></div></div><div id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="../3/"><i class="fas fa-angle-left"></i></a><a class="page-number" href="../../">1</a><span class="space">&hellip;</span><a class="page-number" href="../3/">3</a><span class="page-number current">4</span><a class="page-number" href="../5/">5</a><a class="extend next" rel="next" href="../5/"><i class="fas fa-angle-right"></i></a></div></div></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fas fa-user"></i></span><span id="busuanzi_value_site_uv"></span><span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fas fa-eye"></i></span><span id="busuanzi_value_site_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Out of Memory</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/copy.js"></script><!--script(src=url)--></body></html>