<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Live and Learn"><meta name="keywords" content=""><meta name="author" content="Out of Memory,undefined"><meta name="copyright" content="Out of Memory"><title>Live and Learn【Out of Memory】</title><link rel="stylesheet" href="../../css/fan.css"><link rel="stylesheet" href="../../css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="../../favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="../../js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="author-info"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Out of Memory</div><div class="author-info-description">Live and Learn</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/libanghuai" target="_blank">GitHub<i class="icon-dot bg-color9"></i></a><a class="links-button button-hover" href="mailto:libanghuai@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color8"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1185719433&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color3"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="../../archives"><span class="pull-top">日志</span><span class="pull-bottom">86</span></a><a class="author-info-articles-tags article-meta" href="../../tags"><span class="pull-top">标签</span><span class="pull-bottom">33</span></a><a class="author-info-articles-categories article-meta" href="../../categories"><span class="pull-top">分类</span><span class="pull-bottom">2</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Out of Memory</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><div id="recent-posts"><!-- each post in page.posts.sort('date', -1).limit(10).toArray()--><!-- config中配置按照什么排序--><div class="recent-post-item"><a class="post-title" href="../../2018/12/19/DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks/">DropFilter: A Novel Regularization Method for Learning Convolutional Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Regularization/">Regularization</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1811.06783" target="_blank" rel="noopener">https://arxiv.org/abs/1811.06783</a><br>论文主要提出了一种新的Regularization方法DropFilter，主要是对卷积核进行抑制，具体实现的时候也比较简单，只要对卷积核和bias生成对应的0 - 1mask(Bernoulli分布),然后对应和kernel相乘即可：</p>
<p><img src="DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks-6cc74f13d759b1cbb0c4a864d637b520623e0053_1_690x465.png" alt=""><br>在DropFilter的基础上作者还提出了DropFilter-PLUS方法，主要是针对卷积操作过程中的每一个位置都进行一次独立的DropFilter操作，那么对于一个224x224的输入需要计算222x222次mask操作(3x3conv)，计算量比较大，作者在论文中分析具体的conv操作(论文中提及的alx,y)最后通过对卷积的输出进行mask来等价，这就大大加速了DropFilter-PLUS的操作：<br><img src="DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks-9fa3f026fff5f15f159490c18a254286219fda82_1_585x500.png" alt=""><br>DropFilter和Dropfilter-PLUS都有一定的道理，并且简单易实现</p>
</div></div><a class="button-hover more" href="../../2018/12/19/DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/19/Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images/">Robust Face Detection via Learning Small Faces on Hard Images</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Face/">Face</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1811.11662.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.11662.pdf</a><br>论文主要是想解决人脸检测中的小脸问题，论文的motivation其实和SNIP很像，让网络去学习一个相对固定的scale，比如在本论文中anchor大小被设置为固定的16x16，32x32，64x64三个。论文主要的内容有两个部分，一部分是提出来的检测网络，另一部分就是hard image mining。</p>
<p>论文所提的检测网络backbone采用VGG16，在接入cls和reg前经过dilation conv来获得不同的感受野，其他没有特别的内容:<br><img src="Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images-9add084a4879a4cde2af6ba16849afa33dc4a245_1_690x283.png" alt=""><br><img src="Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images-e9d7b5e89400c3d6572e883406879031b064fcd8_1_482x500.png" alt=""><br>而hard image mining是image level的，作者针对图片的难易程度定义了一个变量WPAS, 简言之是综合proposal的IOU以及score信息来量化图片的难易程度，WPAS&gt;0.85被定义为简单的图片：<br><img src="Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images-5d2afb9dc7cdd4a787fc08fb6b82bda9f305572f_1_690x106.png" alt=""><br>最后在训练的时候对于当前epoch会借助前一个epoch的信息把训练数据都划分成easy和hard两类，再从easy中以0.7的概率剔除掉一些图片，剩下的图集作为当前epoch的训练数据来训练模型。</p>
</div></div><a class="button-hover more" href="../../2018/12/19/Robust-Face-Detection-via-Learning-Small-Faces-on-Hard-Images/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/17/Deep-Layer-Aggregation/">Deep Layer Aggregation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1707.06484.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1707.06484.pdf</a><br>CVPR2018的一篇关于layer aggregation的论文，论文的motivation是作者觉得目前常见的aggregation方式（FPN、U-Net…）比较shallow，作者希望利用更加deeper的连接方式来更好的融合特征。论文中作者分别提出了IDA和HDA两种连接方式。IDA应用在stage之间，HDA应用在block之间。</p>
<p>下图是论文中对IDA和HDA给出的直观图示，下图的c是IDA（Iterative Deep Aggregation），整体和FPN的连接方式比较类似，只是方向相反，浅层的特征被不断的refine与高层特征相融合。下图的d，e，f对应着HDA（Hierarchical Deep Aggregation），d是HDA最原始的表达，整体是一个树状结构，跨越不同层级的特征分层次进行特征融合，e则是在d的基础上将前面节点的父亲节点与当前节点一同考虑进行特征融合。f则是作者出于降低模型复杂度的角度将e同一层级的节点进行merge：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.41.22.png" alt=""><br>下面这幅图是论文中将IDA和HDA合并到一起具体的模型应用，也就是论文标题的Deep Layer Aggregation模型，橙色的连线是IDA的逻辑，红色框内部的连接是HDA的逻辑：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.41.53.png" alt=""><br>而对于一些比如seg的‘image-to-image’任务，只需通过增加简单的插值操作就可以实现：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.42.30.png" alt=""><br>为了验证DLA的有效性作者还是做了比较多的实验的，作者分别在Classification、Fine-grained Recognition、Seg、Boundary Detection几个主流的cv task上都做了实验，都有不同程度的提升：<br>分类的实验：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.48.56.png" alt=""><br>识别的实验：<br><img src="Deep-Layer-Aggregation-屏幕快照 2018-12-17 下午8.49.01.png" alt=""><br>论文中作者提出的IDA和HDA感觉还是很合理的，作者同时还提及residual connection在实验时对比较深的网络是有效的，对于比较浅的网络是有负面作用的。</p>
</div></div><a class="button-hover more" href="../../2018/12/17/Deep-Layer-Aggregation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/15/Deep-Mutual-Learning/">Deep Mutual Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Mutual-learning/">Mutual learning</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Classification/">Classification</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://cn.arxiv.org/pdf/1706.00384v1" target="_blank" rel="noopener">http://cn.arxiv.org/pdf/1706.00384v1</a><br>论文主要在讲mutual learning相互学习，deep mutual learning整体感觉和model distillation还是比较像的,只是不是用训练好的大网络来带小网络而是用一些网络相互同步学习来提高模型整体的效果:<br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.49.05.png" alt=""></p>
<p>对于其中的单个网络除了本身分类的cross entropy loss外还会涉及到KL散度来度量两个网络预测结果之间的loss（K是网络个数）：<br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.49.47.png" alt=""><br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.50.11.png" alt=""><br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.50.24.png" alt=""><br>补充一些作者针对mutual learning的有效性做的一些探究：</p>
<ul>
<li>作者认为DML相对独立的模型得到了wider minima，因此更加鲁棒，作者设计了一组实验对网络的参数增加了一些噪声，下图是增加噪声前后的结果，正常比较的时候两者相似，增加噪声扰动之后DML效果相对更好：<br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.52.25.png" alt=""></li>
<li>K个模型的mutual learning通常可以有两种表现形式，一个就是当前这个模型和其余的K-1个模型分别进行KL distance计算，另一个就是当前模型只要和其余K-1个模型的预测结果均值来进行KL distance计算，作者实验发现后者其实效果会更差，作者认为是多模型结果的融合影响了一些网络细节的teaching signal:<br><img src="Deep-Mutual-Learning-屏幕快照 2018-12-15 下午8.51.58.png" alt=""></li>
</ul>
</div></div><a class="button-hover more" href="../../2018/12/15/Deep-Mutual-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/14/A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition/">Center Loss - A Discriminative Feature Learning Approach for Deep Face Recognition</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Loss/">Loss</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://ydwen.github.io/papers/WenECCV16.pdf" target="_blank" rel="noopener">http://ydwen.github.io/papers/WenECCV16.pdf</a><br>这篇论文主要的贡献就是提出了Center Loss的损失函数，利用Softmax Loss和Center Loss联合来监督训练，在扩大类间差异的同时缩写类内差异，提升模型的鲁棒性。</p>
<p><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image002.png" alt=""><br>为了直观的说明softmax loss的影响，作者在对LeNet做了简单修改，把最后一个隐藏层输出维度改为2，然后将特征在二维平面可视化，下面两张图分别是MNIDST的train集和test集，可以发现类间差异比较明显，但是类内的差异也比较明显。<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image003.png" alt=""><br>为了减小类内差异论文提出了Center Loss：<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image004.png" alt=""><br>C<sub>yi</sub>就是类的中心点特征，Cyi的计算方法就是yi类样本特征的均值，为了让center loss在神经网络训练过程中切实可行，C<sub>yi</sub>的计算是对于每一个mini-batch而言，因此结合Softmax Loss，整个网络的损失函数就变成了， λ用来平衡这两个Loss：<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image005.png" alt=""><br>用同样的网路结构只是将Softmax Loss替换成Center Loss作者在MNIST数据集上做了同样的实验，对于不同的λ值得到了如下可视化结果可以发现Center Loss还是比较明显的减小了类内差异同时类间差异也比较突出。<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image006.png" alt=""><br>在公开数据集上的表现：<br><img src="A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition-image007.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/12/14/A-Discriminative-Feature-Learning-Approach-for-Deep-Face-Recognition/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/14/CRAFT-Objects-from-Images/">CRAFT Objects from Images</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-16</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1604.03239" target="_blank" rel="noopener">https://arxiv.org/abs/1604.03239</a><br>CVPR2016的一篇论文。首先CRAFT代表 Cascade Region proposal network And FasT-rcnn，本论文主要想解决RPN网络生成的区域不太精确的问题，比如对于一些外观复杂度较低的事物如树木，会因为RPN网络产生的背景区域的存在导致比较难检测或者产生FP，因此作者尝试利用cascade的方式来解决这样的问题。<br>CRAFT模型可以分成两个部分：</p>
<ol>
<li>Cascade Proposal Generation：这一部分同样可以分成两个部分，RPN和FRCN，FRCN其实就是一个Fast R-CNN网络，两个网络分别训练，RPN网络生成相对粗糙的区域，FRCN用RPN的输出作为输入对RPN的结果进行进一步的refine，提高了候选区域的质量也减少了背景框：<br><img src="CRAFT-Objects-from-Images-image002.png" alt=""></li>
<li>Cascade Object Classification：Cascade Object Classification部分由两个FRCN网络级联而成，Cascade Proposal Generation部分的输出会作为FRCN1的输入，FRCN1的输出摒弃掉“背景”类作为FRCN2的输入，实现细节上两个模型复用参数。这里需要注意的是，作者认为为了学习类间的细微差别，模型复用了被Fast RCNN抛弃的 One-vs-Rest的分类方式，用N个二分类的交叉熵损失函数的和作为最终的loss。<br><img src="CRAFT-Objects-from-Images-image003.png" alt=""><br>最终在VOC上面的表现效果：<br><img src="CRAFT-Objects-from-Images-image004.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2018/12/14/CRAFT-Objects-from-Images/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/14/Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network/">Weighted Channel Dropout for Regularization of Deep Convolutional Neural Network</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Regularization/">Regularization</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://home.ustc.edu.cn/~saihui/papers/aaai2019_weighted.pdf" target="_blank" rel="noopener">http://home.ustc.edu.cn/~saihui/papers/aaai2019_weighted.pdf</a><br><strong>【Summary】</strong>AAAI2019的一篇关于Regularization的论文，整体感觉可以理解为SENet思想在Regularization中的应用。论文中作者提出了Weighted Channel Dropout(WCD)的逻辑（为每一个channel计算权重、构造取舍的概率…）来对channel进行选择性的DropOut。</p>
<p><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.03.34.png" alt=""><br>上面这张图是WCD的整个Pipeline，整个结构主要可以分成三个部分：</p>
<ul>
<li><strong>Rating Channels</strong>：这个模块主要是为每一个channel构造一个全权重，逻辑很简单就是Global Average Pooling：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.18.15.png" alt=""></li>
<li><strong>Weighted Random Selection (WRS)</strong>: 这一部分逻辑其实也很简单，作者定义了每个channel被选择的概率为：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.20.57.png" alt=""><br>那么在具体计算的时候是这样做的,r<sub>i</sub>是一个（0，1）之间的随机数，对于这两个计算方式的等价性，论文中作者给出了引用论文可以具体参考：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.21.58.png" alt=""><br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.23.52.png" alt=""></li>
<li><strong>Random Number Generation (RNG)</strong>: 这个可以理解为在WRS基础上的一个补充吧，在实际应用中有时间可用的数据量比较小，那么这种情况下通常会用pretrain的模型来初始化网络，那么作者认为这种情况下channel之间的差距会更大，可能只有少部分的channel会有比较大的响应，那么根据WRS选出来的channel可能也很像，所以针对WRS选中的channel依然会有1 - q的概率被抛弃,那么最终每一个channel的状态就和WRS、RNG都有关：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.30.54.png" alt=""><br>论文中取alpha为：<br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.31.35.png" alt=""><br><strong>一些实验结果</strong></li>
</ul>
<p><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.32.15.png" alt=""><br><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.32.21.png" alt=""><br>作者还贴了一个和SENet比较的结果：</p>
<p><img src="Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network-屏幕快照 2018-12-14 下午9.44.02.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/12/14/Weighted-Channel-Dropout-for-Regularization-of-Deep-Convolutional-Neural-Network/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/11/Parallel-Feature-Pyramid-Network-for-Object-Detection/">Parallel Feature Pyramid Network for Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener">https://arxiv.org/abs/1612.03144</a><br>ECCV2018的一篇论文,这篇论文从某种程度上来说是为了解决小物体检测的问题，作者从feature map特征表示好坏的角度来分析目前常用检测模型的一些不足。论文分别可视化SSD、FPN、PFPNET（本文所提模型）对同样输入图片的feature map，从图中可以看出来SSD对物体的轮廓细节描述比较差，FPN对于一些遮挡物体的特征表示比较差。PFPNET则相对好一些，至于为什么好，以及这个结构设计的理由论文貌似并没有解释。</p>
<p><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image002.png" alt=""><br>论文的主要内容：</p>
<ol>
<li>Parallel Feature Pyramid Network for Object Detection （PFPNET）模型结构：整体结构和RetinaNet比较像，不同的是PFPNET的multi-scale是一个并行的结构，和RetinaNet中的feature layer之间是一个串的结构是不一样的。模型首先利用Base Model（论文中用的是VGGNET-16）得到PFPNET结构的输入（DxWxH），然后利用SPP产生不同scale的feature map（FP Pool），不同scale的feature map长宽分别以2倍大小减少，对于每一个scale分别再利用Bottleneck Layer完成特征的转化。最后就是特征的融合（MSCA），模型基本结构图：<br><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image003.png" alt=""></li>
<li>Multi-scale context aggregation（MSCA）：本文用的特征融合还是常见的concatenate方式，对于第n个分支最后的融合的特征Pn, 它有第一层FP Pool中与之对应的feature map以及第二层FP Pool中其他的N-1个feature map相融合而得到。然后对于最后的n个feature map分别进行cls + reg的任务即可。<br><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image004.png" alt=""></li>
<li>Some details: 论文主要用refinedet和ssd作为baseline来比较，PFPNet-S代表和SSD相同的anchor设置，PFPNet-R代表和RefineDet相同的anchor设置，300和512则代表具体输入图片的size。</li>
</ol>
<p>实验结果，论文主要对比的对象是SSD和RefineDett，其中从整体上来看在COCO数据集上PFPNET的表现貌似并不是最优的，作者则具体分析了在小物体等场景下的优势：<br><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image006.png" alt=""><br><img src="Parallel-Feature-Pyramid-Network-for-Object-Detection-image005.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/12/11/Parallel-Feature-Pyramid-Network-for-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/04/An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/">An Analysis of Scale Invariance in Object Detection – SNIP</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1711.08189" target="_blank" rel="noopener">https://arxiv.org/abs/1711.08189</a><br><strong>【Summary】</strong>CVPR2018的一篇Oral，主要在研究scale invariance或者说是domain shift的问题，论文所提出的SNIP方法不同于multi scale的逻辑，可以理解为把网络输入的物体norm到一个相对固定的scale，inference的时候也做同样的策略，这样可以避免训练和测试数据集的scale invariance的问题。</p>
<p>论文在分析scale invariance问题的时候做了几组实验：</p>
<ul>
<li><strong>实验一</strong>：训练三个模型CNN-B(224x224作为输入的图)、CNN-S（和CNN-B类似的图片只是针对输入图片的尺寸修改了第一层conv的stride，可以理解为针对输入图的尺寸做的Resolution Specific Classifier）以及CNN-B-FT（224x224输入图预训练模型 + 用低分辨率图片upsample搭配224x224来finetune）。CNN-B在48x48~224x224范围的输入图上效果如下图a，可见效果随分辨率大小逐渐变好，224x224最好。CNN-B-FT的效果也有一定提升。从结果中也可以发现domain shift对模型的影响：<br><img src="./An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/a.png" alt="51|690x198"></li>
<li><strong>实验二</strong>:验证集固定为1400x2000的分辨率</li>
</ul>
<ol>
<li>分别训练800x1400和1400x2000的检测模型，1400x2000效果最好，但是提升有限，作者认为是因为提升分辨率有助于小物体的检测但是对于大物体的检测是有坏处的</li>
<li>训练1400x2000的模型忽略特别大的物体，最后的结果比800x1400的效果更差，因为抛弃了比较多的数据，丢失了variation in appearance and pose</li>
<li>Multi-Scale Training最后的结果和800x1400的效果差不多。作者认为过程中同样出现了过大和过小的物体。<br><img src="./An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/屏幕快照 2018-12-04 下午9.34.43.png" alt="43|508x178"><br>针对上面实验的结果论文所提出的SNIP方法想法很直接，可以理解为是Image Pyramid的改进。SNIP通过在训练和inference的时候控制物体到一个固定的scale来保证检测的效果。实验一和实验二的结果也支持来这一点。具体实现的时候针对不同scale的输入，论文中都分别定义了一些proposal的面积范围，网络训练的时候只有落在给定范围内的proposal才会回传梯度。这也是SNIP中Scale Normalization的实际含义：<br><img src="./An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/屏幕快照 2018-12-04 下午8.39.47.png" alt="33|690x302"><br>一些实验结果：<br><img src="./An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/屏幕快照 2018-12-04 下午9.59.10.png" alt="10|690x169"><br><img src="./An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/屏幕快照 2018-12-04 下午9.59.16.png" alt="16|690x259"></li>
</ol>
</div></div><a class="button-hover more" href="../../2018/12/04/An-Analysis-of-Scale-Invariance-in-Object-Detection-–-SNIP/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/11/24/SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts/">SGDR: Stochastic Gradient Descent with Warm Restarts</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Learning-Strategy/">Learning Strategy</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1608.03983" target="_blank" rel="noopener">https://arxiv.org/abs/1608.03983</a> Github: <a href="https://github.com/loshchil/SGDR" target="_blank" rel="noopener">https://github.com/loshchil/SGDR</a><br><strong>【Summary】</strong>ICLR2017一篇关于学习率的论文，论文的核心比较直接就是提出了基于cosine的学习率 warm restart逻辑，然后论文的大篇幅都是围绕这个learing rate进行了比较多的实验。论文所提的SGDR通常只需要原有模型1/2-1/4的训练epoch就可以得到差不多甚至更好的效果。</p>
<p>论文所提的cosine learning rate 公式如下，n<sub>min</sub>、n<sub>max</sub>就是学习率的区间，T<sub>cur</sub>表示当前经过多少个epoch了，T<sub>i</sub>可以理解为周期。因为从下面这个公式当T<sub>cur</sub> = T<sub>i</sub>时，n<sub>t</sub> = n<sub>min</sub>：</p>
<p><img src="SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts-c4d6e75499fcd3406791dd38969c8d52b2f59521_1_690x119.png" alt=""><br>作者在具体实验的时候其实还涉及到另一个参数n<sub>mult</sub>代表周期的系数，下图是不同的T<sub>i</sub>和T<sub>mult</sub>画出来的示意图：</p>
<p><img src="SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts-d82c1f3a92421e0ca83d33a2f7b0fe29374e7d62_1_690x288.jpg" alt=""></p>
<p>下图是作者在CIFAR-10和CIFAR-100数据集上用Wide Residual Neural Network（depth=28&amp;width=10）做的实验，效果很直观，SGDR收敛速度明显比较快：</p>
<p><img src="SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts-76589c0bc925d1e5325639557cd6ecdb400389da_1_447x499.jpg" alt=""><br>下表是具体的实验结果，从结果上看效果也是不错的：</p>
<p><img src="SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts-4441390556648037aa6d983e2edfb07a2a075d6a_1_629x500.png" alt=""><br>此外作者也从ensemble、downsampled imagenet dataset的角度做了不同的实验，结果也都很好：<br>Ensemble:</p>
<p><img src="SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts-f2310a3f482911fcebf25cb90947d106fcf3e22a_1_690x130.png" alt=""><br>Downsampled imagenet dataset:</p>
<p><img src="SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts-d308a8fb8fb5028f53e7d108006f93a5d5364a52_1_632x500.jpg" alt=""></p>
<p>论文所提的这种warm restart逻辑感觉还是很合理的，可以优化梯度下降中鞍点以及局部最小解的问题，有助于模型能快速收敛。</p>
</div></div><a class="button-hover more" href="../../2018/11/24/SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/11/19/Unconstrained-Face-Alignment-without-Face-Detection/">Unconstrained Face Alignment without Face Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Face/">Face</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://ieeexplore.ieee.org/document/8014992" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/8014992</a><br><strong>【Summary】</strong> CVPR2017 Workshop一篇关于人脸关键点检测的论文，应该主要是在参加CVPR2017的一个Menpo Challenge。  整篇论文的主要内容可以理解为把OpenPose中的一些做法应用到人脸关键点定位任务中。<br>下图是论文中给出的对所提方法整个pipeline的示意，论文所提方法主要可以分成两个部分Basic Landmark Prediction Stage（BLPS）和 Whole Landmark Regression Stage（WLRS）。第一部分负责得到人脸关键点的粗定位（其实只是所有关键点中几个主要的点比如眼球、鼻尖等），第二部分负责在第一阶段的基础上进行进一步的refine：<br><img src="Unconstrained-Face-Alignment-without-Face-Detection-7294485d9c10d1d220cb3c0ea2039e4fefcb135d_1_690x370.jpg" alt=""></p>
<ul>
<li><strong>Basic Landmark Prediction Stage</strong>：这一部分可以直接理解为OpenPose 中PAF在人脸关键点中的应用。具体细节可以直接参考OpenPose那篇文章。只是在本论文中当前阶段只处理人脸关键点中主要的几个点（左右眼球、鼻尖和左右嘴角共5个点）。<br><img src="Unconstrained-Face-Alignment-without-Face-Detection-216534462f9c38b34a4971687f33a92855b698a9_1_584x499.jpg" alt=""></li>
<li><strong>Whole Landmark Regression Stage</strong>：这一部分是直接接在BLPS之后进行的。首先会利用BLPS的结果根据关键点的可见性判断人脸的Pose，总共分成三类：left profile、right profile、semi-frontal。那么论文本身是对这三种pose计算过模版脸的，所以可以直接align 到模版脸上，最后直接送到回归网络回归最后的精确结果。感觉这也是目前人脸关键点任务中比较常用的方法：\<br><img src="Unconstrained-Face-Alignment-without-Face-Detection-167f3b8032a6d164916e82241af84a4a205b6060_1_678x500.png" alt=""></li>
</ul>
<p>论文用的训练数据来自300W + Menpo + CelebA，作者在300W数据集上做的实验结果：</p>
<p><img src="Unconstrained-Face-Alignment-without-Face-Detection-8c807a07df3ad3b1446d8e2ac598ae0448a0eea0.png" alt=""></p>
<p>这篇论文可以理解为OpenPose方法在人脸关键点定位任务上的直接应用，从方法整体的pipeline上看其实和目前我们在做的landmark定位逻辑是一致的，都是先给部分点的位置然后align在精细refine所有的点。</p>
</div></div><a class="button-hover more" href="../../2018/11/19/Unconstrained-Face-Alignment-without-Face-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/11/15/Squeeze-and-Excitation-Networks/">Squeeze-and-Excitation Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="noopener">https://arxiv.org/abs/1709.01507</a> Github:<a href="https://github.com/hujie-frank/SENet" target="_blank" rel="noopener">https://github.com/hujie-frank/SENet</a><br>CVPR2017的一篇论文，CNN网络卷积操作本身是整合空间信息和channel信息的，论文作者的motivation是显式地对channel做attention来提高模型的表达能力，论文主要的内容就是一个SE Block：<br><img src="Squeeze-and-Excitation-Networks-image002.png" alt=""><br>SE Block主要分成两个部分：</p>
<ol>
<li>Squeeze：上图中的Fsq, 本质就是一个Global Average Pooling，将H x W x C映射成1x1xC：<br><img src="Squeeze-and-Excitation-Networks-image003.png" alt=""></li>
<li>Excitation：上图中的Fex, 本质就是两个全连接层来学习channel的权重信息，W1是第一层FC参数，输出为C/r，W2为第二层FC参数，输出恢复到C，r为压缩比例，用来减少参数量：<br><img src="Squeeze-and-Excitation-Networks-image004.png" alt=""><br>最后将excitation得到的输出施加到对应的channel上就可以得到最终的输出（图中的Fscale）SE Block本身的设计是一个嵌入模块，所以它可以方便的结合现有的网络结构，论文中给了两个示例SE-Inception和SE-ResNet：<br><img src="Squeeze-and-Excitation-Networks-image005.png" alt=""><br><img src="Squeeze-and-Excitation-Networks-image006.png" alt=""><br>论文中还重点论述了SE Block不会对原模型带来比较大的复杂度，对于输入图片大小为224x224时，ResNet50约3.86GFLOPS，SE-ResNet-50约3.87GFLOPS，实际运行时间两者差别也不是很大，贴一张具体的数据：<br><img src="Squeeze-and-Excitation-Networks-image007.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2018/11/15/Squeeze-and-Excitation-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/11/14/Face-R-CNN/">Face R-CNN</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Face/">Face</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1706.01061" target="_blank" rel="noopener">https://arxiv.org/abs/1706.01061</a><br>腾讯AI Lab发表在CVPR2017上面的论文，在2017年可以做到FDDB 和 Wider Face数据集上面的最好结果，Face R-CNN是基于Faster R-CNN框架的模型，基本结构论文给出了比较详细的示意图：<br><img src="Face-R-CNN-image002.png" alt=""><br>模型主要有以下几个关键点：</p>
<ol>
<li>Center Loss：论文直接把人脸识别中的center loss沿用到人脸检测中来，出发点还是一样利用Softmax Loss来扩大类间差异，利用Center Loss来减小类间差异，以此来增强模型的鲁棒性，只是在人脸检测中类别总数只有2，人脸和非人脸。<br><img src="Face-R-CNN-image003.png" alt=""><br>因此模型最终的Loss就变成了cls loss + reg loss + center loss：<br><img src="Face-R-CNN-image004.png" alt=""></li>
<li>OHEM：利用标准的OHEM做法，以loss作为key排序取Top N作为hard example，作者特别说明使用Center Loss可以有效的控制hard example中的postive和negative的样本数。在最后实现的时候是分别在postive和negative样本上应用OHEM并且控制每个batch两者的比例为1:1.</li>
<li>Multi-Scale Training：这一步其实就是把图片resize成不同的大小进行训练来覆盖不同分辨率……<br>最后实验的结果：<br><img src="Face-R-CNN-image005.png" alt=""><br>论文整体感觉更偏工程化，通过不断尝试融合现有的方法提高模型在数据集上的表现，没有很特别的地方。</li>
</ol>
</div></div><a class="button-hover more" href="../../2018/11/14/Face-R-CNN/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/10/28/Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping/">Associative Embedding: End-to-End Learning for Joint Detection and Grouping</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-27</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Skeleton/">Skeleton</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://papers.nips.cc/paper/6822-associative-embedding-end-to-end-learning-for-joint-detection-and-grouping.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/6822-associative-embedding-end-to-end-learning-for-joint-detection-and-grouping.pdf</a><br><strong>【Summary】</strong> Pose estimation 任务中另一个典型的bottom up模型，论文的motivation感觉比OpenPose的PAF更加直观易懂，就是为每一个joint学习一个tag用来标记一组joint。然后再用贪心的逻辑来做group。</p>
<p>下图是论文中给出的整个方法的Pipeline，整个网络总共有两个分支，一个输出关键点位置的heatmap，文中称之为detection，另一个分支就是本文的核心associative embedding，论文中称之为grouping：<br><img src="Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping-3c5dd349b51723e15a8ec5ea15463e029db0fb03_1_690x338.jpg" alt=""><br>至于论文中用到的backbone网络是比较常见的hourglass：<br><img src="Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping-e180bb474a3cf204c9540778d188a8f57ffa2ef0.png" alt=""><br><strong>Associative Embedding</strong><br>论文中提及的embedding可以理解为对人物个体的标记，和NLP中的word embedding一样，embedding的维度其实可以是任意的，本论文中作者通过实践觉得1维的embedding 就足够了，所以对于detection的每一个channel在grouping中都一个同样大小的channel与之对应。<br>在网络训练的时候Detection Loss就是普通的MSE而Grouping Loss是如下设计的：<br><img src="Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping-3d23af6063c886a039dec53aa3ac965505139a8c.png" alt=""><br>其中：<br><img src="Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping-2193d834631344a19a46cd6ac97a7346f44364b4.png" alt=""><br>hk是grouping中一个输出的channel，x为具体的位置，这主要涉及到论文中的reference embedding，reference embedding就是一个人物个体所有joint 在grouping中输出的均值作为对这个人物个体的表示。而在具体的Loss函数中前半段就是把输入同一个人物个体的joint尽量拉近，而公式的后半段就是把不同的人物个体相互拉开。<br>具体Inference的时候通过某一个joint的heatmap的峰值来确定检测到的人的pool，然后再依次去利用其他joint的tag来和这个pool里面的人进行match。如果当前这个joint无法和pool里面的任意一个人match，那么就会作为单独的一个人加入到这个pool，然后一直进行这样的贪心流程。</p>
<p>作者在COCO和MPII数据集上分别做了测试，从结果上来看点还是比较高的：<br><img src="Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping-8c807a07df3ad3b1446d8e2ac598ae0448a0eea0.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/10/28/Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/10/22/SNIPER-Efficient-Multi-Scale-Training/">SNIPER: Efficient Multi-Scale Training</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1805.09300" target="_blank" rel="noopener">https://arxiv.org/abs/1805.09300</a> Github: <a href="https://github.com/MahyarNajibi/SNIPER" target="_blank" rel="noopener">https://github.com/MahyarNajibi/SNIPER</a><br><strong>【Summary】</strong>算是在SNIP版本上的改进吧，主要想优化multi scale训练的效率问题。论文提出了chip的概念，其实就是原图中的某一块区域，在本论文中chip的大小就是512 x 512的矩形方块。网络的输入不再是原图，而是这些生成的chip。SNIPER的主要内容就是这些chip的生成逻辑。</p>
<p>作为网络的输入，chip也分为postive chip和negative chip, 下图是postive chip的生成示例。对于给定的一张图，会有512 x 512这样大小的chip框在原图上滑动，间隔为32个pixel。包含valid gt（valid或者invalid的定义和SNIP那篇论文逻辑一样）的chip被称为postive chip。由于对于给定的一张图片不会把每一个chip都送给网络去训练，所以通常都是贪心的选择包含valid gt最多的chip作为这张图的postive chip：</p>
<p><img src="SNIPER-Efficient-Multi-Scale-Training-2feaec373ba84ada0c26428b063370d8588e35d8_1_690x386.jpg" alt=""><br>下面这张图则是negative chip的生成示例，论文中作者生成的negative chip是想包含那些比较难解的FP，所以作者用了一个简单的RPN网络来生成negative chip，下图的红点代表一个proposal（已经移除掉被postive chip包含的那些了）。negative chip被定义为至少含M个proposal的那些chip(论文中M貌似没给出具体的值)：</p>
<p><img src="SNIPER-Efficient-Multi-Scale-Training-54af81d1f0fbbac701cb898fe22e2ee48d188efe_1_690x341.png" alt=""><br>贴一张实验结果：</p>
<p><img src="SNIPER-Efficient-Multi-Scale-Training-6bcbdd2062f585821204274dfe18fd845f2a4544_1_690x316.png" alt=""></p>
<p>感觉这篇论文的重点用论文中的一句话可以很好的概括”it implies that very large context during training is not important for training high-performance detectors but sampling regions containing hard negatives is”</p>
</div></div><a class="button-hover more" href="../../2018/10/22/SNIPER-Efficient-Multi-Scale-Training/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/10/14/RMPE-Regional-Multi-Person-Pose-Estimation/">RMPE: Regional Multi-Person Pose Estimation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-27</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Skeleton/">Skeleton</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1612.00137" target="_blank" rel="noopener">https://arxiv.org/abs/1612.00137</a><br><strong>【Summary】</strong>ICCV2017关于Pose Estimation的论文，主要是基于目前Top Down方法的改进。论文主要想解决一个问题，就是Top Down方法中human detector出的框不准或者比较冗余的问题。STN来映射更准的框， Pose NMS 来更好的解决冗余框的问题。</p>
<p>下图是论文中给出的对于RMPE方法的简单示例，整个Pipeline针对human detector出框不准或者比较冗余的问题分别进行了针对性的设计，下图中的前半段STN + SPPE + SDTN主要用于fix human detector出框不准的问题，后半段的Pose NMS主要用来解决冗余的问题：</p>
<p><img src="RMPE-Regional-Multi-Person-Pose-Estimation-40073134aa3c4bacc77a9e9fda9a039e18627b2a_1_690x253.jpg" alt=""></p>
<ul>
<li><strong>SPPE的改进：</strong> SPPE（Single Person Pose Estimation）主要改进的地方就是引入STN和SDTN来实现更加精确的pose estimation。细节上STN接受human detector输出的proposal（具体实现的时候会在原有的proposal基础上，长宽各扩展30%以保证覆盖全部的人体区域），STN生成的仿射变换矩阵将当前的proposal映射的更加精确，然后再输入SPPE网络中（实际实验时是4-stack hourglass结构）得到具体的joint的位置，然后通过STN的逆变换SDTN得到原始的坐标位置。下图中的Parallel SPPE作用是对STN进行进一步的监督，监督的Label是已经center-located的pose，所以当然也就不需要SDTN这一步了。从下图中也可以比较直观的看出来，上下两个branch label还是不一样的，为了映射回原坐标，上半分支的SDTN是必要的：<br><img src="RMPE-Regional-Multi-Person-Pose-Estimation-bba38f5d3a2f3183198d1b3e1edad64fdf69cf6d_1_690x253.jpg" alt=""></li>
<li><strong>Pose NMS：</strong> 对于Pose NMS作者主要在研究一件事就是如何来定义Pose 的冗余以及消除冗余的标准，就像物体检测的NMS用IoU和confidence的逻辑一样。论文中作者用如下的方式来定义：<br><img src="RMPE-Regional-Multi-Person-Pose-Estimation-0dfc09cf7e37b3b1ca4ff52221f214f16ba73bb4.png" alt=""><br>其中，H<sub>Sim</sub>用来建模距离上的相似度，K<sub>Sim</sub>用来建模confidence之间的关系，B(K<sub>i</sub><sup>n</sup>)代表1以K<sub>i</sub><sup>n</sup>为中心的矩形，矩形大小为这个Pose对应框的1/10：<br><img src="RMPE-Regional-Multi-Person-Pose-Estimation-f21a4f8fbd194882b2ffb4506b29dfeda3696354.png" alt=""><br><img src="RMPE-Regional-Multi-Person-Pose-Estimation-144986445bc91b1f0003ee01a3ce41c772544bdb.png" alt=""></li>
<li>此外作者也针对所提的RMPE结构设计了Data Augmentation方法，主要是根据原子pose进行聚类，然后对于输入的图像首先对其进行分类，然后利用对应的offset分布构造一组新的数据，具体的分布信息：<br><img src="RMPE-Regional-Multi-Person-Pose-Estimation-36ee4b370f48e74a1f0f269815e6d4c9db9a7d06.jpg" alt=""><br>在MPII数据集上的表现：<br><img src="RMPE-Regional-Multi-Person-Pose-Estimation-70e6ebcbc5feaad8636963960565eb32feae49aa_1_690x250.png" alt=""><br>在COCO Chanllenge上的表现：<br><img src="RMPE-Regional-Multi-Person-Pose-Estimation-4b243ddf5903093632852b6f2830838278676f40.png" alt=""></li>
</ul>
</div></div><a class="button-hover more" href="../../2018/10/14/RMPE-Regional-Multi-Person-Pose-Estimation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/09/25/Pelee-A-Real-Time-Object-Detection-System-on-Mobile-Devices/">Pelee: A Real-Time Object Detection System on Mobile Devices</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-09</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="../../categories/Paper-Reading/">Paper Reading</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>这篇论文主要做的内容是在手机终端上进行实时的物体检测并提出了PeleeNet模型，不同于目前的MobileNet、ShuffleNet等基于 depthwise separable convolution的模型，PeleeNet是基于传统的卷积来实现的，因为作者认为depthwise separable convolution操作在诸多的框架下都没有有效的支持……<br>论文主要借鉴了DenseNet、DSOD、Inception-V4等现有的网络，模型结构在论文中给出了比较详细的列表，几个需要注意的细节：</p>
<ol>
<li>作者参考GoogleNet在DenseBlock中引入2-way dense layer来获得不同大小的感受野。</li>
</ol>
<p><img src="Pelee-A-Real-Time-Object-Detection-System-on-Mobile-Devices-image004.png" alt=""></p>
<ol start="2">
<li>DenseNet中表现较好的DenseNet-BC结构是在原有的基本DenseNet结构基础上加入了Bottleneck和Compression，但是作者认为compression逻辑的加入影响模型的特征表达，所以下表中的Transition Layer没有compression的逻辑，输入输出channel数保持一致。</li>
</ol>
<p><img src="Pelee-A-Real-Time-Object-Detection-System-on-Mobile-Devices-image002.png" alt=""></p>
<ol start="3">
<li>Composite Function中用Post-activation替换掉DenseNet中的Pre-activation，以此来在inference阶段加速计算。</li>
<li>用Bottleneck Layer控制输出的channel数不大于输入的channel数，此操作除了可以节省28.5%的计算量，对准确率的影响也比较小。</li>
</ol>
<p><img src="Pelee-A-Real-Time-Object-Detection-System-on-Mobile-Devices-image003.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/09/25/Pelee-A-Real-Time-Object-Detection-System-on-Mobile-Devices/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/09/24/Cascade-R-CNN-Delving-into-High-Quality-Object-Detection/">Cascade R-CNN: Delving into High Quality Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Cascade/">Cascade</a></div></div><div class="post-content"><div class="main-content content"><p>解决问题：<br>目前的检测模型通常需要通过设置IoU阈值来定义Positive和Negative bbox，作者发现最终检测模型的表现和IoU阈值的设置相关，通过下图的c，d可以发现低IoU阈值通常会对低IoU样本表现更好，因为不同IoU阈值下的样本分布不一样，一个IoU阈值很难对所有的样本都有效。因此为了解决这个问题，论文提出了Cascade R-CNN 模型。</p>
<p><img src="Cascade-R-CNN-Delving-into-High-Quality-Object-Detection-image002.png" alt=""></p>
<p>论文实验的Cascade R-CNN模型基于典型的Two-Stage结构，Cascade R-CNN模型有多个Classification和Bounding Box header（下图中的Cx，Bx），前一个header refine过的Bounding Box 会作为下一个header的输入，并且IoU的阈值逐层增加，B0代表RPN的结果。通过这种方式实现对于每一级header输入数据的resample，保证了每一级header都有足够的正样本。</p>
<p><img src="Cascade-R-CNN-Delving-into-High-Quality-Object-Detection-image003.png" alt=""></p>
<p>实验对比，可以发现cascade rcnn的效果还是有比较大的提升，但同时也大幅大增加了模型的复杂度</p>
<p><img src="Cascade-R-CNN-Delving-into-High-Quality-Object-Detection-image004.png" alt=""></p>
<p>思考：<br>Cascade R-CNN结构是现有检测模型优化的思考方向，但是从上图第二章表中也可以看出加入cascade 结构在提高检测效果的同时也大幅度增加了模型复杂度.</p>
</div></div><a class="button-hover more" href="../../2018/09/24/Cascade-R-CNN-Delving-into-High-Quality-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/09/24/Single-Shot-Refinement-Neural-Network-for-Object-Detection/">Single-Shot Refinement Neural Network for Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-09</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>论文致力于研究整合two-stage检测器和one-stage检测器的优点提出了RefineDet网络结构。</p>
<p><img src="Single-Shot-Refinement-Neural-Network-for-Object-Detection-image003.png" alt=""></p>
<p>网络结构整体借鉴了SSD模型并结合了cascade逻辑，整个网络主要分成三个部分：</p>
<ol>
<li>Anchor  Refinement Module（ARM）：这一模块的模型结构和SSD比较类似，不同的是它只做二分类，ARM对anchor进行初步的refine并得到相对粗糙的位置信息和分类信息，这一部分信息将作为ODM模块的输入。<br>特别的，为了缓解类别失衡问题，论文提出了Negative Anchor过滤逻辑，将negative confidence大于某一个阈值（如0.95）的anchor全部舍弃掉不送入ODM模块。</li>
<li>Object Detection Module (ODM): 这一模块的结构和ARM类似，并且两者对应layer的size是一致的，论文中给出的结构图例比较直观。ODM模块主要是借助ARM模块refine过后的anchor进行进一步细致的处理并最终输出物体的类别信息和bounding box的位置信息。在具体实现的时候，ARM和ODM之间采用了类似FPN特征融合的方法将ARM高层的feature map与低层的feature map相融合并与ODM对应layer的feature map一同作为下一层layer的输入，以此来提高检测的效果。</li>
<li>Transfer Connection Block (TCB)：TCB主要用来融合低层和高层feature map，论文给的图示比较清楚主要就是对高层feature map进行deconv并与低层feature map相加；</li>
</ol>
<p><img src="Single-Shot-Refinement-Neural-Network-for-Object-Detection-image002.png" alt=""></p>
<p>实验结果：</p>
<p><img src="Single-Shot-Refinement-Neural-Network-for-Object-Detection-image004.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/09/24/Single-Shot-Refinement-Neural-Network-for-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/09/24/CoupleNet-Coupling-Global-Structure-with-Local-Parts-for-Object-Detection/">CoupleNet: Coupling Global Structure with Local Parts for Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-09</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>R-FCN将RoI划分成区域通过整合各个区域的信息来表征最后的结果，而CoupleNet的motivation是利用R-FCN中对局部信息的学习能力再整合全局和context信息来提升模型的检测效果。</p>
<p><img src="CoupleNet-Coupling-Global-Structure-with-Local-Parts-for-Object-Detection-image002.png" alt=""></p>
<p>CoupleNet网络主要分成两个分支：</p>
<ol>
<li>Local FCN：引入R-FCN中的Position-sensitive score maps 和 Position-sensitive RoI pooling 得到局部信息给出的结果，对于分类会输出C+1维的特征向量，具体细节直接参考R-FCN即可。</li>
<li>Global FCN：这个分支主要用来整合RoI的全局信息和context信息，上图中的两个RoI Pooling就是分别来映射这部分特征的，第一部分是全局的特征信息，第二部分是在第一部分RoI的基础上范围向外扩大2倍再进心RoI Pooling这样可以引入部分的context信息来增强特征提高检测的效果，最后两者feature map直接concat到一起经过kxk和1x1两个卷积层之后同样得到C+1维的特征向量</li>
<li>Coupling structure：两个分支输出的整合作者也做了很多的实验，(L2 normalization layer , 1x1 conv layer)  x (element-wise max, element-wise product, element-wise sum) 共6种组合中1x1 conv + element-wise sum 表现最好。</li>
</ol>
<p>实验结果，速度方面因为CoupleNet相当于在R-FCN的基础上引入了Global FCN的分支，所以很显然速度要慢于R-FCN的，但还是明显快于faster rcnn的，检测效果方面效果也很好。</p>
<p><img src="CoupleNet-Coupling-Global-Structure-with-Local-Parts-for-Object-Detection-image003.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/09/24/CoupleNet-Coupling-Global-Structure-with-Local-Parts-for-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/09/21/DSOD-Learning-Deeply-Supervised-Object-Detectors-from-Scratch/">DSOD: Learning Deeply Supervised Object Detectors from Scratch</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-09</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>论文主要想解决的问题是如何直接去训练一个detection模型，因为现在detection模型都是利用ImageNet数据预训练的，灵活性相对较差，分类和检测的场景也不一致，因此论文提出了DSOD模型。<br>论文中提出的DSOD模型结构通过下表可以直观的看到，Stem、Dense Block、Transition Layer都是参考现有的研究成果，论文主要的贡献是<br> Transition w/o Pooling Layer 和 Dense Prediction Structure：</p>
<ol>
<li><strong>Transition w/o Pooling Layer</strong>：这个结构主要用来实现增加Densen Block模块个数的同时保持输出的size不变，从表中可以直观的看到其实就是1x1的卷积，因为传统densenet中的transition layer会利用pooling层来对feature map下采样，这里用Transition w/o Pooling Layer来保持输出的size与transition layer区分开。</li>
</ol>
<p><img src="DSOD-Learning-Deeply-Supervised-Object-Detectors-from-Scratch-image002.png" alt=""></p>
<ol start="2">
<li><strong>Dense Prediction Structure</strong>：论文中用下图来解释Dense Prediction Structure，和常用的结构（左侧）相比较主要的差别在于 Learning Half and Reusing Half，每一层网络的输入都是上一层的输出feature map 和 前面层的下采样结果的concatenate，因此对于每一层的输出一半是通过学习得到的另一半则是直接从前面层下采样得到的，这样可以直接复用前面的信息。</li>
</ol>
<p><img src="DSOD-Learning-Deeply-Supervised-Object-Detectors-from-Scratch-image003.png" alt=""></p>
<p>具体的实验结果，或许看完这篇文章最好奇的是从头训练的检测模型和预训练过的检测模型具体效果的好坏，论文的最后作者利用<br> DS/64-12-16-1做了比较，发现从头训练的模型效果要好于预训练过的模型（70.7% vs 70.3%）</p>
<p><img src="DSOD-Learning-Deeply-Supervised-Object-Detectors-from-Scratch-image004.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/09/21/DSOD-Learning-Deeply-Supervised-Object-Detectors-from-Scratch/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/09/01/R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks/">R-FCN: Object Detection via Region-based Fully Convolutional Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-09</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1605.06409" target="_blank" rel="noopener">https://arxiv.org/abs/1605.06409</a> Github:<a href="https://github.com/daijifeng001/r-fcn" target="_blank" rel="noopener">https://github.com/daijifeng001/r-fcn</a><br>R-FCN网络设计的motivation是为了增强物体检测网络位置敏感性，提高物体检测的精度和速度。它的整体结构是一个two stage的网络，RPN网络分支生成Proposal Candidate RoIs ，另一个分支基于此做进一步的refine。</p>
<p><img src="R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks-image002.png" alt=""></p>
<p>R-FCN网络结构设计的核心是Position-sensitive score maps 和Position-sensitive RoI pooling ：</p>
<ol>
<li><strong>Position-sensitive score maps</strong>：不妨假设RoI的大小为wxh，那么将RoI均分成k x k个区域，那么每个区域的大小就约为w/k x h/k，对于C分类任务，最后的输出就有kxkx(C+1)个channel，用来表征各个区域的分类信息。而对于候选框的位置，最后的输出是4xkxk个channel。</li>
</ol>
<p><img src="R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks-image003.png" alt=""></p>
<ol start="2">
<li><strong>Position-sensitive RoI pooling</strong>：这一部分主要用来整合Position-sensitive score maps的信息得到最终的结果，Pooling公式中的i，j代表kxk个bin的id信息，c为C个类别中的一个，x0，y0为区域的左上角坐标，那么实际上在整个论文中用的是average pooling，作者也提到max pooling也是可以的。</li>
</ol>
<p><img src="R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks-image004.png" alt=""></p>
<p>所以经过Position-sensitive RoI pooling之后输出为kxkx(C + 1), 对于每一个类别c最后的打分是kxk个值的均值，因此最后的输出变成C+1维的向量，最后通过softmax得到最后的分类结果。对于bbox框位置的回归也和分类类似，只是channel数是4xkxk，最后得到4维的向量作为位置信息的偏移量。</p>
<ol start="3">
<li>实验结果，速度相比较faster rcnn还是有比较明显的提升，检测效果也不错。</li>
</ol>
<p><img src="R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks-image005.png" alt=""></p>
<p>R-FCN网络的设计有一个特点，在得到score maps之后Position-sensitive RoI pooling没有引进其他的参数，无疑有助于train和inference的速度，通过划分区域人为的对位置做细分也有实际的意义，但是个人对regression阶段也用同样的操作不是很理解..</p>
</div></div><a class="button-hover more" href="../../2018/09/01/R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/07/22/UnitBox-An-Advanced-Object-Detection-Network/">UnitBox: An Advanced Object Detection Network</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="../../categories/Paper-Reading/">Paper Reading</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Detection/">Detection</a></div></div><div class="post-content"><div class="main-content content"><p>You can find this paper <a href="https://arxiv.org/pdf/1608.01471.pdf" target="_blank" rel="noopener">here</a></p>
<h1 id="Two-Main-Contributions"><a href="#Two-Main-Contributions" class="headerlink" title="Two Main Contributions:"></a>Two Main Contributions:</h1><h2 id="IoU-Loss"><a href="#IoU-Loss" class="headerlink" title="- IoU Loss"></a>- <strong>IoU Loss</strong></h2><p>在目前的神经网络训练中，L2 Loss是比较常用的Loss函数，它其实就是欧式距离的计算，L2 Loss有两个比较典型的缺点：</p>
<ol>
<li>在L2 Loss中，Bounding Box的四个角被看作相互独立的单元，计算Loss的时候都是分别去优化左上角的坐标和Bounding Box的长、宽，但是实际上在检测问题中一个物体的边界是高度相关的。忽略其中的联系也会导致一些badcase，比如预测的Bounding Box可能有一、两条边和Ground Truth的很接近但是整个预测的Bounding Box偏离的比较离谱。</li>
<li>标准的L2 Loss是没有normalize的, 那么在训练过程中模型就会更加关注大一点的Bounding Box，当然现在也有了很多normalize的方法被提出来。</li>
</ol>
<p>基于此，论文作者提出了IoU Loss：<br><img src="UnitBox-An-Advanced-Object-Detection-Network/IoU_Loss.png" alt="IoU Loss"></p>
<h2 id="UnitBox-Network-Structure"><a href="#UnitBox-Network-Structure" class="headerlink" title="- UnitBox Network Structure"></a>- UnitBox Network Structure</h2><p><img src="UnitBox-An-Advanced-Object-Detection-Network/UnitBox.png" alt="IoU Loss"></p>
</div></div><a class="button-hover more" href="../../2018/07/22/UnitBox-An-Advanced-Object-Detection-Network/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/07/20/Build-Hadoop-Spark-Cluster/">Build Hadoop &amp;&amp; Spark Cluster</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time></div><div class="post-content"><div class="main-content content"><p>单机配置</p>
<ul>
<li>创建用户和用户组<br>groupadd hadoop//新建用户组<br>useradd hadoop -g hadoop//把用户加入用户组<br>passwd hadoop//为用户hadoop设置密码然后会提示输入密码和确认密码</li>
<li>修改hostname和hosts<br>vim /etc/hostname<br>vim /etc/hosts</li>
<li><p>修改用户权限<br>vim /etc/sudoers</p>
</li>
<li><p>配置无密码登录<br>cd ~/.ssh/     //如果目录下没有这个文件夹，需要执行以下ssh localhost生成这个文件夹<br>ssh-keygen -t rsa //生成key,过程中只管敲回车<br>cat ./id_rsa.pub &gt;&gt; ./authorized_keys //授权, 有时候非root用户授权会失败，就是直接执行ssh localhost还是会提示输入密码，那么多半是文件权限的问题：chmod 600 .ssh/authorized_keys 修改权限即可</p>
</li>
<li>安装Java JDK<br>下载Java JDK: <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a><br>设置环境变量: vim /etc/profile,在文件最后加入:<br>#Java Env<br>export JAVA_HOME=/usr/local/java/jdk1.8.0_162<br>export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar<br>export PATH=$PATH:$JAVA_HOME/bin<br>使配置生效: source /etc/profile<br>测试java环境:<br>java<br>java -version<br>javac</li>
<li>安装Hadoop<br>下载Hadoop: <a href="http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.6.5/hadoop-2.6.5.tar.gz" target="_blank" rel="noopener">http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.6.5/hadoop-2.6.5.tar.gz</a><br>解压Hadoop到/usr/local/目录下：tar -zxf ~/hadoop-2.6.5.tar.gz -C /usr/local<br>mv hadoop-2.6.5/ hadoop<br>chown -R hadoop ./hadoop/  //设置权限<br>./hadoop/bin/hadoop version //查看hadoop安装包是否完整<br>配置环境变量，vim /etc/profile,在文件的最后加入：<br>export HADOOP_HOME=/usr/local/hadoop<br>export PATH=$HADOOP_HOME/bin:/$HADOOP_HOME/sbin:$PATH</li>
<li>配置Hadoop<ul>
<li>总共配置6个文件: slaves、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml</li>
<li>slaves: 将作为 DataNode也就是Slave机器的主机名写入该文件，每行一个，默认为 localhost，所以在伪分布式配置时，节点即作为 NameNode 也作为 DataNode。分布式配置可以保留 localhost，也可以删掉，让 Master 节点仅作为 NameNode 使用。 本项目让 Master 节点仅作为 NameNode 使用，因此将文件中原来的 localhost 删除，只添加一行内容：162.105.67.62</li>
<li>hadoop-env.sh：添加Java JDK路径<h1 id="The-java-implementation-to-use"><a href="#The-java-implementation-to-use" class="headerlink" title="The java implementation to use."></a>The java implementation to use.</h1>export JAVA_HOME=/usr/local/java/jdk1.8.0_162</li>
</ul>
</li>
<li>core-site.xml:<configuration><br>  <property><br>      <name>fs.defaultFS</name><br>      <value>hdfs://HadoopMaster:9000</value><br>  </property><br>  <property><br>      <name>hadoop.tmp.dir</name><br>      <value>file:/usr/local/hadoop/tmp</value><br>      <description>Abase for other temporary directories.</description><br>  </property><br></configuration></li>
<li>hdfs-site.xml:dfs.replication 一般设为 3，但我们只有一个 Slave 节点，所以 dfs.replication 的值还是设为 1</li>
</ul>
<configuration><br>        <property><br>                <name>dfs.namenode.secondary.http-address</name><br>                <value>HadoopMaster:50090</value><br>        </property><br>        <property><br>                <name>dfs.replication</name><br>                <value>1</value><br>        </property><br>        <property><br>                <name>dfs.namenode.name.dir</name><br>                <value>file:/usr/local/hadoop/tmp/dfs/name</value><br>        </property><br>        <property><br>                <name>dfs.datanode.data.dir</name><br>                <value>file:/usr/local/hadoop/tmp/dfs/data</value><br>        </property><br></configuration><br><em> mapred-site.xml:需要将mapred-site.xml.template文件重命名<br><configuration><br>        <property><br>                <name>mapreduce.framework.name</name><br>                <value>yarn</value><br>        </property><br>        <property><br>                <name>mapreduce.jobhistory.address</name><br>                <value>HadoopMaster:10020</value><br>        </property><br>        <property><br>                <name>mapreduce.jobhistory.webapp.address</name><br>                <value>HadoopMaster:19888</value><br>        </property><br></configuration>
</em>   yarn-site.xml:<br><configuration><br><br><!-- Site specific YARN configuration properties --><br>    <property><br>                <name>yarn.resourcemanager.hostname</name><br>                <value>HadoopMaster</value><br>        </property><br>        <property><br>                <name>yarn.nodemanager.aux-services</name><br>                <value>mapreduce_shuffle</value><br>        </property><br></configuration>
</div></div><a class="button-hover more" href="../../2018/07/20/Build-Hadoop-Spark-Cluster/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/07/19/Build-Jitamin-in-CentOS7/">Build Jitamin in CentOS7</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="../../categories/Engineering/">Engineering</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Tools/">Tools</a></div></div><div class="post-content"><div class="main-content content"><p>环境:CentOS7</p>
<ul>
<li><strong>安装相关依赖</strong>：<ul>
<li>安装apache: yum install httpd</li>
<li>安装php 7:<ul>
<li>yum -y install epel-release</li>
<li>rpm -Uvh <a href="https://mirror.webtatic.com/yum/el7/webtatic-release.rpm" target="_blank" rel="noopener">https://mirror.webtatic.com/yum/el7/webtatic-release.rpm</a></li>
<li>yum install php70w</li>
<li>查看PHP是否安装成功: php -v</li>
</ul>
</li>
<li>安装Composer:<ul>
<li>curl -sS <a href="https://getcomposer.org/installer" target="_blank" rel="noopener">https://getcomposer.org/installer</a> | php</li>
<li>mv composer.phar /usr/local/bin/composer</li>
<li>检查composer是否安装成功: composer</li>
</ul>
</li>
<li>安装Mysql:<ul>
<li>wget -i -c <a href="http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm" target="_blank" rel="noopener">http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm</a></li>
<li>yum -y install mysql57-community-release-el7-10.noarch.rpm</li>
<li>yum -y install mysql-community-server</li>
</ul>
</li>
</ul>
</li>
<li><strong>安装Jitamin</strong>:<ul>
<li>cd &lt;安装目录&gt;如：cd /usr/local</li>
<li>下载jitamin: git clone <a href="https://github.com/jitamin/jitamin.git" target="_blank" rel="noopener">https://github.com/jitamin/jitamin.git</a> jitamin</li>
<li>cd jitamin/</li>
<li>cp .env.example .env</li>
<li>配置.env文件:<ul>
<li>修改DB_HOSTNAME,DB_USERNAME等数据库属性</li>
</ul>
</li>
<li>安装相关依赖:composer install -o –no-dev<ul>
<li><font color="#FF4500"><strong>可能会遇到一些problem</strong></font>:<ul>
<li>缺失PHP-gd: yum install php70w-gd</li>
<li>缺失PHP-mbstring: yum install php70w-mbstring</li>
<li>缺失PHP-dom: yum install php70w-dom</li>
</ul>
</li>
</ul>
</li>
<li>创建数据库表: vendor/bin/phinx migrate</li>
<li>安装初始数据: vendor/bin/phinx seed:run</li>
<li>确保bootstrap/cache和storage目录可写:<ul>
<li>chmod -R 0777 bootstrap/cache</li>
<li>chmod -R 0777 storage</li>
</ul>
</li>
<li>同步cache:<ul>
<li>php artisan config:cache</li>
<li>php artisan route:cache</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>配置web服务器</strong>:</p>
<ul>
<li>主要就是配置apache的配置文件</li>
<li>利用yum安装的apache默认配置文件存放于: /etc/httpd/conf/httpd.conf</li>
<li>我们要做的就是在/etc/httpd/conf.d目录下新建jitamin.conf内容如下:<pre><code>&lt;VirtualHost *:80&gt;
DocumentRoot &quot;/usr/local/jitamin/public&quot;
DirectoryIndex index.php
&lt;Directory &quot;/usr/local/jitamin/public&quot;&gt;
  AllowOverride all
  Options -Indexes +FollowSymLinks +ExecCGI
  AllowOverride All
  Order allow,deny
  Allow from all
  Require all granted
&lt;/Directory&gt;
&lt;/VirtualHost&gt;
</code></pre><ul>
<li>最后重启apache即可: service httpd restart</li>
</ul>
</li>
</ul>
</li>
<li><p><font color="#FF4500"><strong>可能会遇到的问题</strong></font></p>
<ul>
<li>在创建数据库表和插入数据库数据的时候可能会遇到xxx pdo xxxx相关的问题<ul>
<li>解决方法<ul>
<li>yum install php70w-pdo</li>
<li>yum install php70w-mysql</li>
</ul>
</li>
</ul>
</li>
<li>这个时候访问xxx.xxx.xxx.xxx(主机地址)可能会出现500 internal error的问题，查看log会发现是显示PHP无法连接上mysql<ul>
<li>解决方法<ul>
<li>grant授权，本机配置时是直接grant all到所有的机器上</li>
</ul>
</li>
</ul>
</li>
<li>继续访问还是会出现500的error<ul>
<li>解决方法<ul>
<li>最后发现是selinux的问题</li>
<li>setenforce 0</li>
<li>setsebool httpd_can_network_connect_db=1</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div></div><a class="button-hover more" href="../../2018/07/19/Build-Jitamin-in-CentOS7/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/07/19/Build-Gitlab-in-CentOS7/">Build Gitlab in CentOS7</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="../../categories/Engineering/">Engineering</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Tools/">Tools</a></div></div><div class="post-content"><div class="main-content content"><h2 id="预配置相关环境"><a href="#预配置相关环境" class="headerlink" title="预配置相关环境"></a>预配置相关环境</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install curl policycoreutils openssh-server openssh-clients</span><br><span class="line">sudo systemctl enable sshd</span><br><span class="line">sudo systemctl start sshd</span><br><span class="line">sudo yum install postfix</span><br><span class="line">sudo systemctl enable postfix</span><br><span class="line">sudo systemctl start postfix</span><br><span class="line">sudo firewall-cmd --permanent --add-service=http</span><br><span class="line">sudo systemctl reload firewalld</span><br></pre></td></tr></table></figure>
<h2 id="添加GitLab镜像源并安装"><a href="#添加GitLab镜像源并安装" class="headerlink" title="添加GitLab镜像源并安装"></a>添加GitLab镜像源并安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -sS http://packages.gitlab.com.cn/install/gitlab-ce/script.rpm.sh | sudo bash</span><br><span class="line">sudo yum install gitlab-ce</span><br></pre></td></tr></table></figure>
<p>或者直接从源下载拷贝到服务器上RPM -ivh xxx.rpm 安装: <a href="https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/</a></p>
<h2 id="配置并启动-GitLab"><a href="#配置并启动-GitLab" class="headerlink" title="配置并启动 GitLab"></a>配置并启动 GitLab</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gitlab-ctl reconfigure</span><br></pre></td></tr></table></figure>
<h2 id="可能遇到的问题"><a href="#可能遇到的问题" class="headerlink" title="可能遇到的问题"></a><font color="red">可能遇到的问题</font></h2><ul>
<li>安装GitLab出现ruby_block[supervise_redis_sleep] action run<ul>
<li>按住CTRL+C强制结束</li>
<li>运行：sudo systemctl restart gitlab-runsvdir</li>
<li>再次执行：sudo gitlab-ctl reconfigure</li>
</ul>
</li>
<li>有可能遇到全部配置好之后网页访问返回refuse<ul>
<li>关闭selinux：setenforce 0</li>
</ul>
</li>
</ul>
</div></div><a class="button-hover more" href="../../2018/07/19/Build-Gitlab-in-CentOS7/#more">阅读全文</a></div></div><div id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="../2/"><i class="fas fa-angle-left"></i></a><a class="page-number" href="../../">1</a><a class="page-number" href="../2/">2</a><span class="page-number current">3</span></div></div></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fas fa-user"></i></span><span id="busuanzi_value_site_uv"></span><span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fas fa-eye"></i></span><span id="busuanzi_value_site_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2019 By Out of Memory</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/copy.js"></script><!--script(src=url)--></body></html>