<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Live and Learn"><title>Out of Memory | Live and Learn</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Out of Memory</h1><a id="logo" href="/.">Out of Memory</a><p class="description">Live and Learn</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title"><a href="/2018/11/24/SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts/">SGDR: Stochastic Gradient Descent with Warm Restarts</a></h1><div class="post-meta">2018-11-24</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1608.03983" target="_blank" rel="noopener">https://arxiv.org/abs/1608.03983</a> Github: <a href="https://github.com/loshchil/SGDR" target="_blank" rel="noopener">https://github.com/loshchil/SGDR</a><br><strong>【Summary】</strong>ICLR2017一篇关于学习率的论文，论文的核心比较直接就是提出了基于cosine的学习率 warm restart逻辑，然后论文的大篇幅都是围绕这个learing rate进行了比较多的实验。论文所提的SGDR通常只需要原有模型1/2-1/4的训练epoch就可以得到差不多甚至更好的效果。</p></div><p class="readmore"><a href="/2018/11/24/SGDR-Stochastic-Gradient-Descent-with-Warm-Restarts/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/11/19/Unconstrained-Face-Alignment-without-Face-Detection/">Unconstrained Face Alignment without Face Detection</a></h1><div class="post-meta">2018-11-19</div><div class="post-content"><p>URL: <a href="https://ieeexplore.ieee.org/document/8014992" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/8014992</a><br><strong>【Summary】</strong> CVPR2017 Workshop一篇关于人脸关键点检测的论文，应该主要是在参加CVPR2017的一个Menpo Challenge。  整篇论文的主要内容可以理解为把OpenPose中的一些做法应用到人脸关键点定位任务中。<br>下图是论文中给出的对所提方法整个pipeline的示意，论文所提方法主要可以分成两个部分Basic Landmark Prediction Stage（BLPS）和 Whole Landmark Regression Stage（WLRS）。第一部分负责得到人脸关键点的粗定位（其实只是所有关键点中几个主要的点比如眼球、鼻尖等），第二部分负责在第一阶段的基础上进行进一步的refine：<br><img src="Unconstrained-Face-Alignment-without-Face-Detection-7294485d9c10d1d220cb3c0ea2039e4fefcb135d_1_690x370.jpg" alt=""></p></div><p class="readmore"><a href="/2018/11/19/Unconstrained-Face-Alignment-without-Face-Detection/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/11/15/Squeeze-and-Excitation-Networks/">Squeeze-and-Excitation Networks</a></h1><div class="post-meta">2018-11-15</div><div class="post-content"><p>URL:<a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="noopener">https://arxiv.org/abs/1709.01507</a> Github:<a href="https://github.com/hujie-frank/SENet" target="_blank" rel="noopener">https://github.com/hujie-frank/SENet</a><br>CVPR2017的一篇论文，CNN网络卷积操作本身是整合空间信息和channel信息的，论文作者的motivation是显式地对channel做attention来提高模型的表达能力，论文主要的内容就是一个SE Block：<br><img src="Squeeze-and-Excitation-Networks-image002.png" alt=""><br>SE Block主要分成两个部分：</p></div><p class="readmore"><a href="/2018/11/15/Squeeze-and-Excitation-Networks/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/11/14/Face-R-CNN/">Face R-CNN</a></h1><div class="post-meta">2018-11-14</div><div class="post-content"><p>URL:<a href="https://arxiv.org/abs/1706.01061" target="_blank" rel="noopener">https://arxiv.org/abs/1706.01061</a><br>腾讯AI Lab发表在CVPR2017上面的论文，在2017年可以做到FDDB 和 Wider Face数据集上面的最好结果，Face R-CNN是基于Faster R-CNN框架的模型，基本结构论文给出了比较详细的示意图：<br><img src="Face-R-CNN-image002.png" alt=""><br>模型主要有以下几个关键点：</p></div><p class="readmore"><a href="/2018/11/14/Face-R-CNN/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/10/28/Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping/">Associative Embedding: End-to-End Learning for Joint Detection and Grouping</a></h1><div class="post-meta">2018-10-28</div><div class="post-content"><p>URL:<a href="https://papers.nips.cc/paper/6822-associative-embedding-end-to-end-learning-for-joint-detection-and-grouping.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/6822-associative-embedding-end-to-end-learning-for-joint-detection-and-grouping.pdf</a><br><strong>【Summary】</strong> Pose estimation 任务中另一个典型的bottom up模型，论文的motivation感觉比OpenPose的PAF更加直观易懂，就是为每一个joint学习一个tag用来标记一组joint。然后再用贪心的逻辑来做group。</p></div><p class="readmore"><a href="/2018/10/28/Associative-Embedding-End-to-End-Learning-for-Joint-Detection-and-Grouping/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/10/22/SNIPER-Efficient-Multi-Scale-Training/">SNIPER: Efficient Multi-Scale Training</a></h1><div class="post-meta">2018-10-22</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1805.09300" target="_blank" rel="noopener">https://arxiv.org/abs/1805.09300</a> Github: <a href="https://github.com/MahyarNajibi/SNIPER" target="_blank" rel="noopener">https://github.com/MahyarNajibi/SNIPER</a><br><strong>【Summary】</strong>算是在SNIP版本上的改进吧，主要想优化multi scale训练的效率问题。论文提出了chip的概念，其实就是原图中的某一块区域，在本论文中chip的大小就是512 x 512的矩形方块。网络的输入不再是原图，而是这些生成的chip。SNIPER的主要内容就是这些chip的生成逻辑。</p></div><p class="readmore"><a href="/2018/10/22/SNIPER-Efficient-Multi-Scale-Training/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/10/14/RMPE-Regional-Multi-Person-Pose-Estimation/">RMPE: Regional Multi-Person Pose Estimation</a></h1><div class="post-meta">2018-10-14</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1612.00137" target="_blank" rel="noopener">https://arxiv.org/abs/1612.00137</a><br><strong>【Summary】</strong>ICCV2017关于Pose Estimation的论文，主要是基于目前Top Down方法的改进。论文主要想解决一个问题，就是Top Down方法中human detector出的框不准或者比较冗余的问题。STN来映射更准的框， Pose NMS 来更好的解决冗余框的问题。</p></div><p class="readmore"><a href="/2018/10/14/RMPE-Regional-Multi-Person-Pose-Estimation/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/09/25/Pelee-A-Real-Time-Object-Detection-System-on-Mobile-Devices/">Pelee: A Real-Time Object Detection System on Mobile Devices</a></h1><div class="post-meta">2018-09-25</div><div class="post-content"><p>这篇论文主要做的内容是在手机终端上进行实时的物体检测并提出了PeleeNet模型，不同于目前的MobileNet、ShuffleNet等基于 depthwise separable convolution的模型，<br>PeleeNet是基于传统的卷积来实现的，因为作者认为depthwise separable convolution操作在诸多的框架下都没有有效的支持……<br>论文主要借鉴了DenseNet、DSOD、Inception-V4等现有的网络，模型结构在论文中给出了比较详细的列表，几个需要注意的细节：</p></div><p class="readmore"><a href="/2018/09/25/Pelee-A-Real-Time-Object-Detection-System-on-Mobile-Devices/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/09/24/Cascade-R-CNN-Delving-into-High-Quality-Object-Detection/">Cascade R-CNN: Delving into High Quality Object Detection</a></h1><div class="post-meta">2018-09-24</div><div class="post-content"><p>解决问题：<br>目前的检测模型通常需要通过设置IoU阈值来定义Positive和Negative bbox，作者发现<br>最终检测模型的表现和IoU阈值的设置相关，通过下图的c，d可以发现低IoU阈值通常<br>会对低IoU样本表现更好，因为不同IoU阈值下的样本分布不一样，一个IoU阈值很难对<br>所有的样本都有效。因此为了解决这个问题，论文提出了Cascade R-CNN 模型。<br><img src="Cascade-R-CNN-Delving-into-High-Quality-Object-Detection/image002.png" alt="Cascade R-CNN"><br>论文实验的Cascade R-CNN模型基于典型的Two-Stage结构，Cascade R-CNN模型有多个<br>Classification和Bounding Box header（下图中的Cx，Bx），前一个header refine过的Bounding Box 会作为下一个header的输入，并且IoU的阈值逐层增加，B0代表RPN的结果。<br>通过这种方式实现对于每一级header输入数据的resample，保证了每一级header都有足够的正样本。<br><img src="Cascade-R-CNN-Delving-into-High-Quality-Object-Detection/image003.png" alt="Cascade R-CNN"><br>实验对比，可以发现cascade rcnn的效果还是有比较大的提升，但同时也大幅大增加了模型的复杂度</p></div><p class="readmore"><a href="/2018/09/24/Cascade-R-CNN-Delving-into-High-Quality-Object-Detection/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/09/24/Single-Shot-Refinement-Neural-Network-for-Object-Detection/">Single-Shot Refinement Neural Network for Object Detection</a></h1><div class="post-meta">2018-09-24</div><div class="post-content"><p>论文致力于研究整合two-stage检测器和one-stage检测器的优点提出了RefineDet网络结构。<br><img src="Single-Shot-Refinement-Neural-Network-for-Object-Detection/image003.png" alt="RefineDet"><br>网络结构整体借鉴了SSD模型并结合了cascade逻辑，整个网络主要分成三个部分：</p></div><p class="readmore"><a href="/2018/09/24/Single-Shot-Refinement-Neural-Network-for-Object-Detection/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/09/24/CoupleNet-Coupling-Global-Structure-with-Local-Parts-for-Object-Detection/">CoupleNet: Coupling Global Structure with Local Parts for Object Detection</a></h1><div class="post-meta">2018-09-24</div><div class="post-content"><p>R-FCN将RoI划分成区域通过整合各个区域的信息来表征最后的结果，而CoupleNet的motivation是利用R-FCN中对局部信息的学习能力再整合全局和context信息来提升模型的检测效果。<br><img src="CoupleNet-Coupling-Global-Structure-with-Local-Parts-for-Object-Detection/image002.png" alt="CoupleNet"><br>CoupleNet网络主要分成两个分支：</p></div><p class="readmore"><a href="/2018/09/24/CoupleNet-Coupling-Global-Structure-with-Local-Parts-for-Object-Detection/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/09/21/DSOD-Learning-Deeply-Supervised-Object-Detectors-from-Scratch/">DSOD: Learning Deeply Supervised Object Detectors from Scratch</a></h1><div class="post-meta">2018-09-21</div><div class="post-content"><p>论文主要想解决的问题是如何直接去训练一个detection模型，因为现在detection模型都是利用ImageNet数据预训练的，灵活性相对较差，分类和检测的场景也不一致，因此论文提出了DSOD模型。<br>论文中提出的DSOD模型结构通过下表可以直观的看到，Stem、Dense Block、Transition Layer都是参考现有的研究成果，论文主要的贡献是<br> Transition w/o Pooling Layer 和 Dense Prediction Structure：</p></div><p class="readmore"><a href="/2018/09/21/DSOD-Learning-Deeply-Supervised-Object-Detectors-from-Scratch/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/09/01/R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks/">R-FCN: Object Detection via Region-based Fully Convolutional Networks</a></h1><div class="post-meta">2018-09-01</div><div class="post-content"><p>URL:<a href="https://arxiv.org/abs/1605.06409" target="_blank" rel="noopener">https://arxiv.org/abs/1605.06409</a> Github:<a href="https://github.com/daijifeng001/r-fcn" target="_blank" rel="noopener">https://github.com/daijifeng001/r-fcn</a><br>R-FCN网络设计的motivation是为了增强物体检测网络位置敏感性，提高物体检测的精度和速度。它的整体结构是一个two stage的网络，RPN网络分支生成Proposal Candidate RoIs ，另一个分支基于此做进一步的refine。<br><img src="R-FCN: Object Detection via Region-based Fully Convolutional Networks/image002.png" alt="R-FCN"><br>R-FCN网络结构设计的核心是Position-sensitive score maps 和Position-sensitive RoI pooling ：</p></div><p class="readmore"><a href="/2018/09/01/R-FCN-Object-Detection-via-Region-based-Fully-Convolutional-Networks/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/07/30/Inception-Series/">Inception Series</a></h1><div class="post-meta">2018-07-30</div><p class="readmore"><a href="/2018/07/30/Inception-Series/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/07/22/UnitBox-An-Advanced-Object-Detection-Network/">UnitBox: An Advanced Object Detection Network</a></h1><div class="post-meta">2018-07-22</div><div class="post-content"><p>You can find this paper <a href="https://arxiv.org/pdf/1608.01471.pdf" target="_blank" rel="noopener">here</a></p>
<h1 id="Two-Main-Contributions"><a href="#Two-Main-Contributions" class="headerlink" title="Two Main Contributions:"></a>Two Main Contributions:</h1><h2 id="IoU-Loss"><a href="#IoU-Loss" class="headerlink" title="- IoU Loss"></a>- <strong>IoU Loss</strong></h2><p>在目前的神经网络训练中，L2 Loss是比较常用的Loss函数，它其实就是欧式距离的计算，L2 Loss有两个比较典型的缺点：</p></div><p class="readmore"><a href="/2018/07/22/UnitBox-An-Advanced-Object-Detection-Network/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/07/20/Quick-Sort/">Quick Sort</a></h1><div class="post-meta">2018-07-20</div><p class="readmore"><a href="/2018/07/20/Quick-Sort/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/07/20/Build-Hadoop-Spark-Cluster/">Build Hadoop &amp;&amp; Spark Cluster</a></h1><div class="post-meta">2018-07-20</div><div class="post-content"><p>单机配置</p>
<ul>
<li>创建用户和用户组<br>groupadd hadoop//新建用户组<br>useradd hadoop -g hadoop//把用户加入用户组<br>passwd hadoop//为用户hadoop设置密码然后会提示输入密码和确认密码</li>
<li>修改hostname和hosts<br>vim /etc/hostname<br>vim /etc/hosts</li></ul></div><p class="readmore"><a href="/2018/07/20/Build-Hadoop-Spark-Cluster/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/07/19/Build-Jitamin-in-CentOS7/">Build Jitamin in CentOS7</a></h1><div class="post-meta">2018-07-19</div><div class="post-content"><p>环境:CentOS7</p>
<ul>
<li><strong>安装相关依赖</strong>：<ul>
<li>安装apache: yum install httpd</li>
<li>安装php 7:<ul></ul></li></ul></li></ul></div><p class="readmore"><a href="/2018/07/19/Build-Jitamin-in-CentOS7/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/07/19/Build-Gitlab-in-CentOS7/">Build Gitlab in CentOS7</a></h1><div class="post-meta">2018-07-19</div><div class="post-content"><h2 id="预配置相关环境"><a href="#预配置相关环境" class="headerlink" title="预配置相关环境"></a>预配置相关环境</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install curl policycoreutils openssh-server openssh-clients</span><br><span class="line">sudo systemctl enable sshd</span><br><span class="line">sudo systemctl start sshd</span><br><span class="line">sudo yum install postfix</span><br><span class="line">sudo systemctl enable postfix</span><br><span class="line">sudo systemctl start postfix</span><br><span class="line">sudo firewall-cmd --permanent --add-service=http</span><br><span class="line">sudo systemctl reload firewalld</span><br></pre></td></tr></table></figure></div><p class="readmore"><a href="/2018/07/19/Build-Gitlab-in-CentOS7/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/07/19/RetinaNet-Focal-Loss-for-Dense-Object-Detection/">RetinaNet: Focal Loss for Dense Object Detection </a></h1><div class="post-meta">2018-07-19</div><div class="post-content"><p>You can find this paper <a href="https://arxiv.org/pdf/1708.02002.pdf" target="_blank" rel="noopener">here</a></p>
</div><p class="readmore"><a href="/2018/07/19/RetinaNet-Focal-Loss-for-Dense-Object-Detection/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/07/17/Build-Personal-Blog-with-WordPress/">Build Personal Blog  with WordPress</a></h1><div class="post-meta">2018-07-17</div><p class="readmore"><a href="/2018/07/17/Build-Personal-Blog-with-WordPress/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2018/07/17/Build-Personal-Blog-with-Hexo-Github-Page/">Build Personal Blog  with Hexo + Github Page</a></h1><div class="post-meta">2018-07-17</div><p class="readmore"><a href="/2018/07/17/Build-Personal-Blog-with-Hexo-Github-Page/">阅读全文</a></p></div><nav class="page-navigator"><a class="extend prev" rel="prev" href="/page/2/">上一页</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span></nav></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://yoursite.com"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Engineering/">Engineering</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper-Reading/">Paper Reading</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Mobile/" style="font-size: 15px;">Mobile</a> <a href="/tags/Platform/" style="font-size: 15px;">Platform</a> <a href="/tags/Hardware/" style="font-size: 15px;">Hardware</a> <a href="/tags/Bottom-Up/" style="font-size: 15px;">Bottom_Up</a> <a href="/tags/Detection/" style="font-size: 15px;">Detection</a> <a href="/tags/Landmark/" style="font-size: 15px;">Landmark</a> <a href="/tags/Pose/" style="font-size: 15px;">Pose</a> <a href="/tags/BottomUp/" style="font-size: 15px;">BottomUp</a> <a href="/tags/KeyPoint/" style="font-size: 15px;">KeyPoint</a> <a href="/tags/Tools/" style="font-size: 15px;">Tools</a> <a href="/tags/Engineering/" style="font-size: 15px;">Engineering</a> <a href="/tags/Loss/" style="font-size: 15px;">Loss</a> <a href="/tags/Mutual-learning/" style="font-size: 15px;">Mutual learning</a> <a href="/tags/Classification/" style="font-size: 15px;">Classification</a> <a href="/tags/Segmentation/" style="font-size: 15px;">Segmentation</a> <a href="/tags/Regularization/" style="font-size: 15px;">Regularization</a> <a href="/tags/Face/" style="font-size: 15px;">Face</a> <a href="/tags/VID/" style="font-size: 15px;">VID</a> <a href="/tags/SOT/" style="font-size: 15px;">SOT</a> <a href="/tags/Track/" style="font-size: 15px;">Track</a> <a href="/tags/Tracking/" style="font-size: 15px;">Tracking</a> <a href="/tags/Mimick/" style="font-size: 15px;">Mimick</a> <a href="/tags/3D/" style="font-size: 15px;">3D</a> <a href="/tags/Template/" style="font-size: 15px;">Template</a> <a href="/tags/Bottom-UP/" style="font-size: 15px;">Bottom UP</a> <a href="/tags/Learning-Strategy/" style="font-size: 15px;">Learning Strategy</a> <a href="/tags/DeepLab/" style="font-size: 15px;">DeepLab</a> <a href="/tags/Classic/" style="font-size: 15px;">Classic</a> <a href="/tags/Basic/" style="font-size: 15px;">Basic</a> <a href="/tags/Quantization/" style="font-size: 15px;">Quantization</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/11/18/Towards-Universal-Object-Detection-by-Domain-Attention/">Towards Universal Object Detection by Domain Attention</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/28/Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm/">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/28/Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection/">Quantization Mimic: Towards Very Tiny CNN for Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/24/Mimicking-Very-Efficient-Network-for-Object-Detection/">Mimicking Very Efficient Network for Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/20/Tone-Mapping/">Tone Mapping</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/15/Lightweight-Real-time-Makeup-Try-on-in-Mobile-Browsers-with-Tiny-CNN-Models-for-Facial-Tracking/">Lightweight Real-time Makeup Try-on in Mobile Browsers with Tiny CNN Models for Facial Tracking</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/10/Improving-Landmark-Localization-with-Semi-Supervised-Learning/">Improving Landmark Localization with Semi-Supervised Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/05/Seeing-Small-Faces-from-Robust-Anchor’s-Perspective/">Seeing Small Faces from Robust Anchor’s Perspective</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/01/Style-Aggregated-Network-for-Facial-Landmark-Detection/">Style Aggregated Network for Facial Landmark Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/25/Deep-Regionlets-for-Object-Detection/">Deep Regionlets for Object Detection</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Out of Memory.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>