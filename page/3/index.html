<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Live and Learn"><meta name="keywords" content=""><meta name="author" content="Out of Memory,undefined"><meta name="copyright" content="Out of Memory"><title>Live and Learn【Out of Memory】</title><link rel="stylesheet" href="../../css/fan.css"><link rel="stylesheet" href="../../css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="../../favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="../../js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="author-info"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Out of Memory</div><div class="author-info-description">Live and Learn</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/libanghuai" target="_blank">GitHub<i class="icon-dot bg-color3"></i></a><a class="links-button button-hover" href="mailto:libanghuai@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color2"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1185719433&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color4"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="../../archives"><span class="pull-top">日志</span><span class="pull-bottom">117</span></a><a class="author-info-articles-tags article-meta" href="../../tags"><span class="pull-top">标签</span><span class="pull-bottom">38</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Out of Memory</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><div id="recent-posts"><!-- each post in page.posts.sort('date', -1).limit(10).toArray()--><!-- config中配置按照什么排序--><div class="recent-post-item"><a class="post-title" href="../../2019/04/10/Spatial-Transformer-Networks/">Spatial Transformer Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Classic/">Classic</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1506.02025.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.02025.pdf</a><br>这是一篇NIPS 2015的文章，主要提出了STN网络结构直接的赋予了网络对于各种变换的不变性。<br>STN网络主要分为三个部分：</p>
<ol>
<li>Localisation Network：一个子网络-用来学习变换参数θ ，θ的大小则和具体的变换有关，比如一般的仿射变换就是6维的参数，子网络的形式可以是FC也可以是CNN结构。</li>
<li>Parameterised Sampling Grid：这一部分负责将目标feature map和 源 feature map的像素之间形成映射，主要计算目标feature map的每一个位置在源feature map上对应的位置 TG。<br><img src="Spatial-Transformer-Networks-image001.png" alt=""></li>
<li>Differentiable Image Sampling：这一部分主要根据Parameterised Sampling Grid生成的TG和源feature map信息采样得到目标feature map.<br><img src="Spatial-Transformer-Networks-image002.png" alt=""><br>一些实验结果：<br><img src="Spatial-Transformer-Networks-image003.png" alt=""><br><img src="Spatial-Transformer-Networks-image004.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="../../2019/04/10/Spatial-Transformer-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/04/04/Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection/">Semantic Alignment: Finding Semantically Consistent Ground-truth for Facial Landmark Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1903.10661" target="_blank" rel="noopener">https://arxiv.org/abs/1903.10661</a><br>CVPR2019最新挂出来的一篇关于人脸landmark的论文，论文的出发点是觉得目前landmark定位精度受限于部分标注点”语意”模糊有关,比如说脸部轮廓点或者眼部轮廓点不像眼球、鼻尖这些点有明确的语意定义，因此标注引入的误差就相对影响比较大。所以作者从这方面入手在模型每次迭代的时候去寻找这样一个“真正”的gt来监督网络的训练，此外为了来修正一些偏移比较厉害的点，作者又引入了一个子网络来refine整体的landmark。这篇论文整体个人感觉很有意义。</p>
<p>首先作者是利用4个级联的Hourglass结构网络来进行landmark点定位的，下图是作者可视化语意明确和语意不明确点在输出heatmap上的结果，在2D空间可以发现，语意比较明确的点比如眼球中心点它的分布更加接近高斯分布，在3D空间这些点的分布更加的锐利，而语意不明确的点比如轮廓点在3D空间就会形如一个“flat hat”：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-屏幕快照 2019-04-03 下午11.53.38.png" alt=""><br>同时当网络已经差不多收敛的时候如果继续训练也会发现那些语意不明确的点依然在gt附近来回抖动，这也一定程度上验证了语意不明确导致标注带来的noise。<br>从网络输出的heatmap上出landamrk点可以将landmark点理解为一种数据分布，w为网络权重、x为输入图片、o为landmark点：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-3043ff99afa634151fd3e0f785d7b1dbfbdd0c63.png" alt=""><br>那么既然gt也不是那么的准确，作者不妨就假设目前存在这样一个真正的gt，不会引入任何的语意不确定性，那么上述的公式可以表示为，y为定义的真正的gt, 那么o就可以理解为是y的一个观测值：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-77c089022e93074fcd6ae6a27cff0e58657635b3.png" alt=""><br>为了缩小后续的搜索空间作者做了一个合理的假定：y<sup>k</sup>存在于o<sup>k</sup>的附近，所以可以用高斯相似度来衡量这种先验概率：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-e1fa30cf7bebe4160d61ca0066ab6f041513d19c.png" alt=""><br>而至于公式的后半段似然概率作者认为对于模型输出的heatmap，如果位置(x, y)周围的region越符合高斯分布，那么点(x, y )就更接近y这样的真正的gt, 所以作者就利用两个分布（预测分布，实际分布(y的分布)）之间的相似度来度量这个似然概率，Φ为抠patch的操作，E为真正gt的分布：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-b8ba7247a1f5ce534923f05834a51f56ae2145cd.png" alt=""><br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-9d0211bc90065c7cb20afa4bd494d72283d417cc.png" alt=""><br>那么通过简单的转化就可以把前面的优化目标转换为，N(ok)可以理解为以点ok为中心的一小部分区域，其实也就是y的搜索空间：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-屏幕快照 2019-04-04 下午12.58.56.jpg" alt=""><br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-9774dd8b68edeb975c901d5289f5bed290e42e57.png" alt=""><br>那么在实际做的时候，作者将整个方法的优化分成两部分来进行，第一步是固定模型的参数W，去搜索最好的y，因为一旦w固定除了yk其他都是已知的，所以直接去搜索y<sup>k</sup>,搜索空间实际应该是o<sup>k</sup>为中心的17x17大小的区域，那么第一个iteration，标注结果就是gt，在二个iteration，前一个iteration搜索得到的y就是gt，依次类推。第二步是固定y去训练模型的参数W，然后这一步就是具体的模型训练了，训练目标为：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-d29dc1db9c88f3c357b807b8cf756a1e40ffb5db.png" alt=""><br>因为从模型的输出直接出landmark点没有完全的考虑脸的整个形态，更多的是考虑了单个点的相关信息，所以作者最后加了一个GHCU的模块来refine landmark点：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-屏幕快照 2019-04-04 下午1.10.55.png" alt=""><br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-屏幕快照 2019-04-04 下午1.12.28.png" alt=""><br>300-W上的实验结果：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-屏幕快照 2019-04-04 下午1.13.23.png" alt=""><br>AFLW的实验结果：<br><img src="Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection-屏幕快照 2019-04-04 下午1.14.02.png" alt=""><br>感觉作者的想法还是很make sense的，目前我们正在用的landmark标注数据也存在这样的语意模糊导致引入标注误差的问题，但是这种标注误差带来的影响还是和实际的任务比较相关，从论文给出的例子来看，预测的landmark点通常在gt附近有一定的抖动，如果这种抖动是贴合轮廓这个影响就相对比较小</p>
</div></div><a class="button-hover more" href="../../2019/04/04/Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/25/Object-Detection-based-on-Region-Decomposition-and-Assembly/">Object Detection based on Region Decomposition and Assembly</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-22</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1901.08225" target="_blank" rel="noopener">https://arxiv.org/abs/1901.08225</a><br>AAAI2019的一篇关于检测的论文，论文主要的出发点是想解决遮挡场景下的物体检测问题，整个逻辑基于Faster RCNN的框架来做，主要思路是先把proposal分part分别来提取特征然后再通过一定的方法将其merge到一起来突出可见部分的特征，从而得到更可信的信息。</p>
<p>论文所提方法整体基于Faster RCNN的逻辑，具体分成两个部分MRP和RDA，下面是具体的示意图：<br><img src="Object-Detection-based-on-Region-Decomposition-and-Assembly-图片 1.jpg" alt=""><br><strong>Multi-scale region proposal (MRP) network</strong><br>这一部分做法其实很简单，就是给RPN的输出proposal给一些scale来丰富porposal的覆盖程度，论文中用了[0.5, 0.7, 1, 1.2, 1.5]共5个scale：<br><img src="Object-Detection-based-on-Region-Decomposition-and-Assembly-屏幕快照 2019-03-16 下午5.47.30.jpg" alt=""><br><strong>Region decomposition and assembly (RDA) network</strong><br>这一部分可以理解为整个方法的核心了，主要分成Decomposition 和 Assembly两个部分， Decomposition 部分将经过ROI Pooling之后的feature map x2然后将其等分成上下左右四部分，每一个部分都会通过conv提取依次特征然后分别merge，merge的方法实际就是element wise max，从而可以显著一些可见区域的特征。这样4个part最终还是会merge为一个feature map，megre完的结果最后再跟全图的feature map在做一次同样的操作得到最后的output，整个逻辑的话图示很清楚：<br><img src="Object-Detection-based-on-Region-Decomposition-and-Assembly-屏幕快照 2019-03-16 下午5.52.33.png" alt=""><br><strong>实验结果</strong><br><img src="Object-Detection-based-on-Region-Decomposition-and-Assembly-屏幕快照 2019-03-16 下午5.56.03.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/03/25/Object-Detection-based-on-Region-Decomposition-and-Assembly/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/19/Mask-Scoring-R-CNN/">Mask Scoring R-CNN</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1903.00241" target="_blank" rel="noopener">https://arxiv.org/abs/1903.00241</a><br>论文的出发点很直观，就是为了优化在目前的一些instance segmentation的方法中用classification score来标注一个mask的质量，这个其实很显然和实际应用场景是完全不一致的，比如论文中给出的例子：<br><img src="Mask-Scoring-R-CNN-屏幕快照 2019-03-19 下午10.56.07.png" alt=""></p>
<p>所以为了解决这个问题，作者以mask rcnn为依托在其基础上增加了 MaskIoU 分支用来出mask和gt之间的IoU，而mask的质量分S<sub>mask</sub> = S<sub>cls</sub> * S<sub>mask_IoU</sub>：<br><img src="Mask-Scoring-R-CNN-屏幕快照 2019-03-19 下午11.01.42.png" alt=""><br>具体MaskIoU分支的逻辑图例给的比较清楚，mask分支的输出通过max pooling的作用之后和RoIAlign的结果进行concat作为MaskIoU分支的输入，经过conv和fc之后得到最后的C个iou，maskiou的计算也比较简单，mask分支的输出卡一个阈值0.5就可以实现二值化，二值化后的mask可以和gt可以比较简单的计算出IoU。</p>
<p>针对MaskIoU分支输入的形式作者也做了好几个尝试：<br><img src="Mask-Scoring-R-CNN-屏幕快照 2019-03-22 下午8.54.04.png" alt=""><br>最后显示直接相加效果是最好的：<br><img src="Mask-Scoring-R-CNN-屏幕快照 2019-03-22 下午8.55.55.png" alt=""></p>
<p>最后的点：<br><img src="Mask-Scoring-R-CNN-屏幕快照 2019-03-22 下午8.53.56.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/03/19/Mask-Scoring-R-CNN/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/18/Region-Proposal-by-Guided-Anchoring/">Region Proposal by Guided Anchoring</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1901.03278" target="_blank" rel="noopener">https://arxiv.org/abs/1901.03278</a><br>CVPR2019的一篇对anchor进行优化的论文，主要将原来需要预先定义的anchor改成直接end2end学习anchor位置和size。首先anchor的定义通常为(x, y, w, h) (x, y为中心点)，formulate一下：<br><img src="Region-Proposal-by-Guided-Anchoring-屏幕快照 2019-03-18 下午10.33.58.png" alt=""><br>因此本文所提的guided anchoring利用两个branch分别预测anchor的位置和w、h：<br><img src="Region-Proposal-by-Guided-Anchoring-屏幕快照 2019-03-18 下午10.28.29.png" alt=""></p>
<p>guided anchoring的主要内容有如下几点：<br><strong>Anchor Location Prediction</strong><br>逻辑很简单，利用一个1x1的conv将输入的feature map转换成 W x H x 1的heatmap，通过卡阈值t来得到anchor可能出现的位置，在训练的时候可以通过gt的框来生成heatmap的groudtruth，negtive、positive、ignore的pixel定义论文中有比较详细的介绍。<br><img src="Region-Proposal-by-Guided-Anchoring-屏幕快照 2019-03-18 下午10.46.37.png" alt=""><br><strong>Anchor Shape Prediction</strong><br>这一部分逻辑和上一部分一样，也是通过一个1x1的conv将输入的feature map转换成W x H x 2的heatmap，只是考虑到如果直接回归w和h范围太广会比较不稳定，作者做了一定的转化将预测值约束到[-1,1],实际使用的时候再映射回去，s为feature map的stride，sigma为8：<br><img src="Region-Proposal-by-Guided-Anchoring-屏幕快照 2019-03-18 下午10.41.23.png" alt=""><br>需要注意的是和传统的anchor设置不一样的是，guider anchoring在某一个pixel下只会设置一个anchor。<br>这一部分的训练其实会是比较需要特别注意的地方，论文中使用来IoU loss来监督，但是这样存在一个问题，因为这个分支本身是预测w，h的，所以IoU Loss的计算无法知道match的具体gt，作者提出的方法是sample 9组常见的w、h，这样就可以利用这9组w、h构建9个不同的anchor去和gt匹配，IoU最大的匹配gt就是当前需要去计算IoU Loss的gt，然后直接用heatmap的w、h和这个gt计算IoU Loss即可：<br><img src="Region-Proposal-by-Guided-Anchoring-屏幕快照 2019-03-18 下午10.48.54.png" alt=""><br><strong>Anchor-Guided Feature Adaptation</strong><br>这一个模块主要是针对feature有可能和anchor不一致而提出的，因为对于原先预定义的anchor而言，每一个pixel对应位置的anchor其实都是一样的，所以也就无所谓feature的异同，但是guided anchoring逻辑下不同的pixel有可能anchor的size差别很大，仍然像之前那样直接出cls和reg很显然是不合适的，所以作者就提出了adaptation的模块，利用deformable conv来处理不同形状的anchor对应的feature。</p>
<p>论文的最后作者也提了一下因为GA-RPN可以得到很多高质量的porposal，通过提高阈值可以进一步优化检测的效果。<br>实验结果：<br><img src="Region-Proposal-by-Guided-Anchoring-屏幕快照 2019-03-18 下午11.21.29.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/03/18/Region-Proposal-by-Guided-Anchoring/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/15/Grid-RCNN/">Grid RCNN</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-10</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1811.12030" target="_blank" rel="noopener">https://arxiv.org/abs/1811.12030</a><br>CVPR2018的一篇论文，从某种程度上来说是借鉴Bottom Up的方法来优化目前检测方面的一些问题，主要出发点还是希望检测器出的框能尽可能的准，所以相比较一般的检测器直接出四维的坐标信息，Grid RCNN则是出9个点，用9个点的信息来表示一个bbox。<br>具体的PipeLine如下：<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.04.35.png" alt=""></p>
<p><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.04.42.png" alt=""><br>Grid RCNN本身是基于RCNN这一套Two Stage的逻辑来做的，所以相比较Faster RCNN主要就是Fast RCNN那个分支做了一些优化，主要几个方面：</p>
<ol>
<li><strong>Grid Guided Localization</strong><br>用NxN个均匀的点来表示一个框而不再是直接回归两个顶点坐标，这样做相比较FC回归点的好处是Conv保留了物体的一些空间位置信息，有助于物体的定位，而类似的 Grid RCNN相比较CornerNet之类基于脚点的检测模型好处在于CornerNet是直接出两个顶点的信息，但是实际上对于一个框的两个顶点它实际上多数处在一个backbroud上，实际可利用的有价值的信息很有限，因此Grid RCNN以及ExteamNet实际上在一定程度上都缓解了这个问题，那么通过Heatmap得到NxN个点之后(共NxN个Heatmap)就可以通过简单的坐标转换得到在原图中NxN个点的坐标，而将NxN个点转换称框的时候作者也提出了自己的逻辑：<br>（ (Px,Py) is the position of upper left corner of the proposal in input image, wp and hp are width and height of proposal, wo and ho are width and height of output heatmap）<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.11.08.png" alt=""><br>本质是用四条边上N个点坐标的加权平均作为边的坐标：<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.11.11.png" alt=""></li>
<li><strong>Grid Points Feature Fusion</strong><br>这一部分可以理解为对Grid RCNN的优化了，作者认为NxN点之间是存在比较强的关联信息的，点与点之间相辅相成可以达到共同促进的作用，所以提出了fusion的逻辑：<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.14.19.png" alt=""><br>做法也很粗暴直接，就是直接将最近点的heatmap通过<strong>3层5x5的conv提取特征</strong>之后直接和当前点的heatmap<strong>取sum</strong>:<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.17.14.png" alt=""><br>那么对于<strong>最近点</strong>的定义论文中也给的很清楚，比如距离为1，那就是相邻的所有点，距离为2，那就是所有距离当前点2个单位长度的点综合来fusion，上面的示意图给的比较清楚，对于当前点的<strong>最近点</strong>被定义为’source point’</li>
<li><strong>Extended Region Mapping</strong><br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.19.26.png" alt=""><br>这个优化主要是针对RPN给出的Proposal不够准，导致定义的NxN个点其实并不能包含检测的物体，那么最简单的方法就是把proposal认为放大，但是这样人为放大之后会严重影响检测的效果，尤其是对小物体而言，所以作者认为考虑到整个CNN的运算过程中感受野是足够的，所以就可以把这些个proposal<strong>看作</strong>是4倍于原来Proposal大小,那么我们就需要直接改坐标的映射关系就好：<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.23.47.png" alt=""><br>这个可以和上面提供的原公式做一个简单的化简其实就是在原来的基础上加了一个偏移量来smooth这个操作，其实整体感觉也好理解，虽然proposla给的框比较小，但是因为感受野的原因最后抽取的特征是可以包含object的信息的，所以就可以直接理解为这个点的坐标偏移相比正常的proposal来说更大，所以需要重新计算加一个偏移量。</li>
</ol>
<p>结果上也是不错的：<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.32.52.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/03/15/Grid-RCNN/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/03/Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks/">Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1711.06753" target="_blank" rel="noopener">https://arxiv.org/abs/1711.06753</a><br>CVPR2018一篇关于人脸Landmark的论文，这篇论文主要是关于人脸关键点的定位，因为论文的重点是loss function和data augmentation所以论文所实验的模型结构是比较简单的CNN结构来实验：<br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image002.png" alt=""></p>
<p>论文主要的研究内容：</p>
<ul>
<li><strong>Wing Loss</strong>：论文首先通过实验直观的反映了常见的L1 Loss、L2 Loss、Smooth L1 Loss的优劣，通过分析不同损失函数的走势，作者认为landmark定位任务中需要更加重视中小范围误差的那些样本(small or medium range error)：<br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image003.png" alt=""><br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image004.png" alt=""><br>因此论文提出了wing loss损失函数，利用对数函数来增强小误差那些样本的表现，其中C是个常数 C = W - Wln(1 + W / e)，至于最终两个变量的取值只能一一尝试，论文也给出了具体的尝试，W = 10 , e = 2最终表现最好：<br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image007.png" alt=""><br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image008.png" alt=""></li>
<li><strong>Pose-based data balancing</strong>：PDB主要用来解决大Pose表现不好的问题，作者认为 大 pose表现不好的根本原因是样本数据不均衡，因此提出了PDB的策略。论文首先利用Procrustes Analysis和PCA将数据集中不同的人脸转化到一维向量空间中用来分析样本pose的分布（具体操作逻辑还需要仔细看，论文说的比较少还不是很清楚），比如对于AFLW数据集可以得到下面的分布图，然后根据具体的样本分布对于那些占比比较小的pose类别通过基本的data augmentation方法来增加这类样本的数量(其实就是直接多复制几份这样的数据):<br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image009.png" alt=""></li>
<li><strong>Two-stage landmark localisation</strong>：这一优化比较常见，利用cascade的逻辑讲landmark的定位分到两阶段CNN网络中，第一阶段就是上面提到的CNN-6，第二阶段则是CNN-7，与CNN-6的差别就是输入从64x64x3变到了128x128x3，增加了一层卷积层，卷积核的个数也略有增加，其他没有什么特殊的设计，最后cascade的逻辑和PDB数据增强带来的效果，CNN-6/7就代表two stage的模型：<br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image010.png" alt=""><br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image011.png" alt=""></li>
</ul>
</div></div><a class="button-hover more" href="../../2019/03/03/Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/03/Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression/">Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL：<a href="https://arxiv.org/abs/1902.09630" target="_blank" rel="noopener">https://arxiv.org/abs/1902.09630</a><br>这是CVPR2019的一篇论文。本论文主要提出了GIoU的概念来优化IoU在评估或者IoU Loss在训练中的一些问题<br>这是利用Ln-Loss来优化bbox回归问题的常见bug，不同的overlap程度在Ln-loss看来都一样，但是实际上对于IoU或者GIoU确实不一样的，很显然后者更合理：</p>
<p><img src="Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression-屏幕快照 2019-03-03 上午11.01.57.png" alt=""></p>
<p>然后就是IoU的不足，第一是对于IoU为0的情况也就是两个box没有交集的情况无法处理，IoU Loss在这种情况下没有梯度的回传。第二就是对于IoU的评价指标对于overlap的方式没有什么体现，比如下图中的示例，IoU都是0.33，但是它们的链接方法是不一样的，或者说对于box回归的任务来说我们的接受度也是不一样的，很显然下图是依次递减，恰好GIoU刚好可以做到这一点：<br><img src="Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression-屏幕快照 2019-03-03 上午11.05.36.png" alt=""><br>接下来就是GIoU的计算，C(AUB)可以理解为两框在convex之内的空白部分了,GIoU计算的方法主要claim一点，更整齐的overlap方法会导致空白部分很小所以GIoU更大：<br><img src="Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression-屏幕快照 2019-03-03 上午11.08.11.png" alt=""><br>GIoU Loss：<br><img src="Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression-屏幕快照 2019-03-03 上午11.09.13.png" alt=""><br>不过从结果上来看，GIoU似乎只对YoloV3这样anchor相对比较稀疏的模型比较有效，也算比较符合GIoU的定义吧：<br><img src="Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression-屏幕快照 2019-03-03 上午11.11.37.png" alt=""><br><img src="Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression-屏幕快照 2019-03-03 上午11.11.42.png" alt=""><br><img src="Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression-屏幕快照 2019-03-03 上午11.11.50.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/03/03/Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/02/Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points/">Bottom-up Object Detection by Grouping Extreme and Center Points</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1901.08043.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.08043.pdf</a><br>Codebase:<a href="https://github.com/xingyizhou/ExtremeNet" target="_blank" rel="noopener">https://github.com/xingyizhou/ExtremeNet</a><br>一篇比较有意义的论文，主要是用bottom up的方法来做检测的问题，整个工作是基于ECCV2018的cornernet来做的，对于一个框ExtremeNet会出5个点，四个边界点和一个中心点，中心点主要是用来做group。下图是标注的示例图：</p>
<p><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-02 下午4.52.39.png" alt=""></p>
<p>论文的整体框架也很简单，主要都是基于CornerNet的code进行改的，对于一张图片出5个点的heatmap和四个offset的heatmap：<br><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-02 下午4.44.43.png" alt=""><br>当拿到最后的5点heatmap和4个offset之后，对于点的instance group方法也很简单甚至比较暴力，首先会设置一个阈值T<sub>p</sub>，那么4个边界点heatmap上大于T<sub>p</sub>的话就会被记为一个candidate，论文中也说了在 coco数据集上一般会有40个左右，那么匹配方法很简单，暴力O(N^4)轮询，然后对于当前的4个点，直接通过取平均的方式得到中心点，然后按这个中心点的位置去中心点的heatmap上取值，如果这个值大于某个阈值T<sub>c</sub>，那么就认为这是一个合法的框：<br><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-02 下午4.45.10.png" alt=""><br>这样利用bottom up进行detection任务的整个框架大体就是如此，那么这种方法也有两个比较明显的问题：</p>
<ul>
<li>Ghost box：假设有三个一样大小、水平或者竖直等距离排布的物体，那么利用extremenet来预测的时候应该会出现4个比较高置信度的框！，因为外围两个框的四个边界点可以组成比较高置信度的框同时中心点还落在中间框的中心点会大概率满足阈值，论文中将这种box称之为ghost box，解决方法比较直接，如果出现一个框内部三个框的score之后大于本身这个大框(‘ghost box’)score的3倍，那么就将这个大框的置信度/2，这样有助于在解析来的NMS中remove掉这个box。</li>
<li>Edge aggregation：假设对于方方正正的物体，比如汽车等，那么由于它的边界点并不是很唯一，它的整条边的点都可以作为边界点，所以后果就是整条边的confidence都不高影响最后的box的生成，因此论文中作者借鉴来cornernet中的pooling的一些想法，对给定的一个extreme point在水平和竖直两个方向以单调递减(heatmap的score)的方法一直遍历直到找到一个局部最小值，过程中遍历的点score的和会作为加权的一部分算到当前extreme point的置信度上：<br>单调递减过程中的点：<br><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-03 上午10.20.37.png" alt=""><br>extreme point score的计算：<br><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-03 上午10.20.32.png" alt=""><br>具体的case：<br><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-03 上午10.22.36.png" alt=""><br>作者同时也把这个方法应用到instance seg的任务中，只是标注略微粗糙利用一个八边形来做mask，具体细节不赘述，具体示例：<br><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-03 上午10.12.15.png" alt=""><br><strong>实验结果还是很不错的：</strong><br><img src="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points-屏幕快照 2019-03-03 上午10.12.06.png" alt=""></li>
</ul>
</div></div><a class="button-hover more" href="../../2019/03/02/Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/03/02/PFLD-A-Practical-Facial-Landmark-Detector/">PFLD: A Practical Facial Landmark Detector</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1902.10859" target="_blank" rel="noopener">https://arxiv.org/abs/1902.10859</a><br>相关主页：<a href="https://sites.google.com/view/xjguo/fld" target="_blank" rel="noopener">https://sites.google.com/view/xjguo/fld</a><br>这两天刚挂出来的关于landmark的论文，论文中对300w数据集报的点是要比LAB、SAN等CVPR2018论文的点是要高的，在845手机上也可以达到140FPS的速度，论文所提方法主要在设计加权的loss，比如考虑人脸的yaw、pitch、roll等信息来加权loss。</p>
<p>论文首先总结了一下目前landmark检测的一些问题，比如局部遮挡、局部光照、人脸Pose、数据不均衡等，同时计算平台对算力的限制也是一个需要关注的点。除了最后的减少模型size是通过尝试不同的backbone网络，其他的论文所提问题可以理解为都集中在Loss的设计：<br><img src="PFLD-A-Practical-Facial-Landmark-Detector-屏幕快照 2019-03-02 上午10.22.31.png" alt=""><br>而整个Loss的设计又可以理解为加权的权重该以什么逻辑加上去，上式是论文中所提的Loss：<br>m是人脸数，n为landmark数，Wnc是当前脸所属的类别c所占总脸数的比例的倒数，theta为具体的角度，k为yaw、roll、pitch，dnm为距离计算的方式比如L1，L2.至此Loss的具体含义就不用赘述了；<br><img src="PFLD-A-Practical-Facial-Landmark-Detector-屏幕快照 2019-03-02 上午10.22.27.png" alt=""><br>这个是论文中所提出的整个网络结构：<br>backbone基于mobilenet v2，为了得到人脸的yaw、pitch、roll等值（默认送进网络的图经过align所以不需要考虑其他的状态信息），作者在backbone的基础上又加了一个分支来专门出这几个值，至于这几个值gt的生成是通过mean face直接计算得到的。</p>
<p>在300w数据集上的表现（ION和IPN是不同的距离norm方式）：<br><img src="PFLD-A-Practical-Facial-Landmark-Detector-屏幕快照 2019-03-02 上午10.22.48.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/03/02/PFLD-A-Practical-Facial-Landmark-Detector/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/02/28/S3FD-Single-Shot-Scale-invariant-Face-Detector/">S3FD: Single Shot Scale-invariant Face Detector</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_S3FD_Single_Shot_ICCV_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_S3FD_Single_Shot_ICCV_2017_paper.pdf</a><br>ICCV2017的一篇论文，主要研究小人脸的检测，作者针对目前基于anchor的检测器对小人脸表现不好的现象分析了几个可能的原因，并针对性的提出了具体的解决方法，比如新的anchor匹配策略、max out backgroud label等方法。</p>
<p>作者首先提出了目前小人脸的难点：<br><img src="S3FD-Single-Shot-Scale-invariant-Face-Detector-屏幕快照 2019-03-02 上午10.08.37.png" alt=""><br>论文中也正是从这四点出发来解决小人脸的检测问题。</p>
<p><img src="S3FD-Single-Shot-Scale-invariant-Face-Detector-屏幕快照 2019-03-02 上午10.07.42.png" alt=""><br>上图是论文中提出的基于VGG16的检测模型，比较特别的地方是Normalization Layer、conv3_3输出结果为Nm+4 而不是一般的2 + 4 以及 anchor设计的策略：</p>
<ul>
<li>Normalization Layer是考虑到Conv3_3 - Conv5_3 feature scale差异比较大，所以对activation做了norm来加速训练。</li>
<li>Conv3_3作为最底层的detection layer，主要负责小人脸的检测，对于小人脸的检测通常需要设置比较多的anchor，这就会导致比较验证的正负样本不均衡的现象，于是作者就提出了max-out backgroud label的逻辑，cls的那个分支会预测N + 1个类别，1就是face，N就是backgroud，然后取最大的backgroud的score去参与计算loss以此来提高cls分支的分类能力，毕竟background归为一类很难去精确分类。<br>max out backgroud逻辑：<br><img src="S3FD-Single-Shot-Scale-invariant-Face-Detector-屏幕快照 2019-03-02 上午10.19.42.png" alt=""></li>
<li>至于anchor的设计策略，可以细看论文中的table：<br><img src="S3FD-Single-Shot-Scale-invariant-Face-Detector-屏幕快照 2019-03-02 上午10.09.36.png" alt=""><br>anchor / stride 恒为4， 这样做的好处就是不同scale保证采样密度一致（两个anchor之间的overlap都是1/4anchor的大小）从而不同的人脸能基本匹配相同数目的anchor。而anchor具体的scale设置也是和一些观察经验有关的，比如论文提到的erf，有效感受野，因为对于图片的不同位置可以理解为权重是不一样的，比如靠近图片的中心他会有很大概率被其他的kernel重复计算，而边缘的pixel则相对被更加稀疏的计算，所以论文提到了ERF的概念，anchor也是针对ERF设计的：<br><img src="S3FD-Single-Shot-Scale-invariant-Face-Detector-屏幕快照 2019-03-02 上午10.09.40.png" alt=""><br>因为anchor的匹配是根据IoU来的，那么对于一些face还是会不可避免的匹配不上anchor，所以论文就提出了一个补充匹配的逻辑来缓解这个问题，方法也很简单，用基本的匹配逻辑匹配完一轮之后，对于剩下没有匹配的那些小脸取IoU 大于0.1的anchor，排序取top N补充进来。这个想法相对比较直观吧。<br>具体的实验结果：<br><img src="S3FD-Single-Shot-Scale-invariant-Face-Detector-屏幕快照 2019-03-02 上午10.14.05.png" alt=""></li>
</ul>
</div></div><a class="button-hover more" href="../../2019/02/28/S3FD-Single-Shot-Scale-invariant-Face-Detector/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/02/22/谈一谈模型量化/">谈一谈模型量化</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Quantization/">Quantization</a></div></div><div class="post-content"><div class="main-content content"><p>说到量化，首先需要弄清楚我们为什么需要量化？为什么可以量化？<br>对于为什么需要量化可以从两个实际的需求来说，第一，对于一般float32存储的模型来说通常硬盘占用相对会比较大，那么在现有的一些端上比如手机端对模型的内存占用是有很明显的限制的，将float32的模型转化为int8的模型就可以带来75%存储的节约，因此在某些场景下模型的量化也是一个必然的选择。第二，从模型的运算效率上来说，int8只有一个字节，float32有四个字节，所以int8参数的获取只需要25%float32的内存带宽，因此可以更好地使用缓存，同时每个时钟周期执行更多操作的SIMD操作。另外DSP等计算单元本身对int8计算很友好，int8模型的使用可以提高速度同时可以降低功耗。<br>至于为什么可以量化，可以这么理解，我们知道CNN网络在对图片进行各种处理的时候比如识别、比如分类、比如检测，都有比较强的鲁棒性，对噪声的兼容性相对比较高。那么落到量化这件事情上来说，可能带来的问题就是精度的损失，那么其实我们可以把这一部分精度的损失理解为外界带给网络的<strong>“噪声”</strong>,所以结合CNN在具体任务的表现我们有理由相信量化操作给网络带来的精度影响不会那么严重。当然了这算是比较理想的状态了，其实在具体的实验过程中对于目前已经做的比较轻量的网络，量化对模型的精度还是会有比较大的影响的。所以量化通常也是根据具体的任务需要做的一个选择。</p>
</div></div><a class="button-hover more" href="../../2019/02/22/谈一谈模型量化/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/02/03/You-Only-Look-Once-Unified-Real-Time-Object-Detection/">You Only Look Once: Unified, Real-Time Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Classic/">Classic</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" target="_blank" rel="noopener">https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf</a><br><strong>重读经典系列第一篇：YOLO</strong><br>YOLO系列是另一个经典的Single Stage的检测器：<br><img src="You-Only-Look-Once-Unified-Real-Time-Object-Detection-屏幕快照 2019-02-03 下午10.49.49.png" alt=""></p>
<p>YOLO整体的流程是这样的，第一步首先将输入图划分成SxS个Grid,</p>
</div></div><a class="button-hover more" href="../../2019/02/03/You-Only-Look-Once-Unified-Real-Time-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/02/03/SSD-Single-Shot-MultiBox-Detector/">SSD: Single Shot MultiBox Detector</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Classic/">Classic</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1512.02325.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1512.02325.pdf</a><br>Code: <a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">https://github.com/weiliu89/caffe/tree/ssd</a><br><strong>重读经典系列第一篇：SSD</strong><br>SSD是很经典的Single Stage检测网络，至今仍有很多的工作是基于SSD在改进。<br>不同于典型的Two stage检测网络将proposal的生成放在RPN网络来做，然后后续的网络branch基于RPN的结果进行Refine，SSD将Proposal直接放到网络后面的feature map上来做，基本逻辑如下图，实际和FasterRCNN网络生成anchor的逻辑是一模一样的：<br><img src="SSD-Single-Shot-MultiBox-Detector-屏幕快照 2019-02-03 下午10.33.52.png" alt=""></p>
<p>至于proposal具体的scale，论文中也是给出具体的公式的，整体其实就是给定最大和最小scale(s<sub>min</sub>, s<sub>max</sub>)，中间的feature map层均匀变化：<br><img src="SSD-Single-Shot-MultiBox-Detector-屏幕快照 2019-02-03 下午10.38.39.png" alt=""><br>k就是具体的层，m是全部产生proposal的层<br>SSD网络整体的结构目前看来也比较直接，相比较传统的VGG等网络，SSD考虑到multi scale的情况，所以在VGG16 backbone的基础上又引出了N层feature map，每一层feature map的感受野都不一样，作者通过这种方式来实现multi scale的检测，每一个feature map都会出4个offset和一个分类器，回归分支的结果是用中心点坐标（x，y） + w和h来表示一个框：<br><img src="SSD-Single-Shot-MultiBox-Detector-屏幕快照 2019-02-03 下午10.39.31.png" alt=""><br>训练用到的loss，对于回归用的smooth l1:<br><img src="SSD-Single-Shot-MultiBox-Detector-屏幕快照 2019-02-03 下午10.43.17.png" alt=""><br><img src="SSD-Single-Shot-MultiBox-Detector-屏幕快照 2019-02-03 下午10.43.21.png" alt=""><br>一些benchmark上的结果：<br><img src="SSD-Single-Shot-MultiBox-Detector-屏幕快照 2019-02-03 下午10.42.36.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/02/03/SSD-Single-Shot-MultiBox-Detector/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/31/High-Speed-Tracking-by-Detection-Without-Using-Image-Information/">High-Speed Tracking-by-Detection Without Using Image Information</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Tracking/">Tracking</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://ieeexplore.ieee.org/document/8078516/" target="_blank" rel="noopener">http://ieeexplore.ieee.org/document/8078516/</a><br>一篇关于IoU tracker的论文，整个跟踪的逻辑都基于检测框来做，对于当前的帧f，检测模型首先会检测出这一帧上面所有的bounding box，然后利用贪心的逻辑将捡出来的框尝试加入到对应的track中，匹配的逻辑就是根据当前的bbox和track的bbox之间的IoU，然后IoU &gt; IoU<sub>threshold</sub>,就把它加到当前track中否则看整个bbox的score，如果score  &gt;= score<sub>threshold</sub> 并且 track的长度  &gt;= length<sub>threshold</sub>就把当前帧作为这个track的结束帧，否则就直接终端当前的这个track，因为他有很大的概率是一段fp，当然论文中也提到匹配的时候也是可以用最大匹配（IoU最大）的一些算法来做的：</p>
<p><img src="High-Speed-Tracking-by-Detection-Without-Using-Image-Information-image.jpg" alt=""><br><img src="High-Speed-Tracking-by-Detection-Without-Using-Image-Information-屏幕快照 2019-01-29 下午8.01.57.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/31/High-Speed-Tracking-by-Detection-Without-Using-Image-Information/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/28/Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose/">Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-09</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Skeleton/">Skeleton</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1811.12004.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.12004.pdf</a><br>论文主要在探索把OpenPose做的轻量化一点，从模型结构到工程优化都做了不少工作，基本把一些常见的优化操作都尝试了一下，比如把原有的VGG backbone替换成针对手机端的mobilenet、把7x7的conv化成1 x 1 + 3 x 3 + 3 x 3 + residual connection、把PAF和joint两个分支进行部分合并共享参数等等。最后模型的复杂度从61.7GFlops降到9GFlops的同时AP只从43.3%降到42.8%，速度直接的量化结果（NUC支持FP16、CPU支持FP32）：<br><img src="Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose-屏幕快照 2018-12-23 下午11.55.18.png" alt=""></p>
<p><img src="Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose-屏幕快照 2019-01-28 下午4.16.00.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/28/Real-time-2D-Multi-Person-Pose-Estimation-on-CPU-Lightweight-OpenPose/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/28/Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation/">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1802.02611.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1802.02611.pdf</a><br>这是DeepLab系列的第四篇文章，DeepLab V3+，这篇论文的主要内容主要是在DeepLab V3的基础上接了一个decoder形成一个hourglass的结构结果直接出原图的resolution，这相比前面版本直接插值要更好一点，整个结构中encoder就直接用的DeepLab V3的结构，输出结果concat到一起之后会和低层的网络特征直接concat，低层的特征通过1x1卷积来降维，简单的示意图：<br><img src="Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation-屏幕快照 2019-01-26 上午11.18.27.png" alt=""></p>
<p>论文中另外一个主要内容是depthwise separable convolution的使用，论文中用深度可分离的空洞卷积来替换掉原有xception中的max pooling 层，主要也是出于模型速度的优化，整体的网络结构：<br><img src="Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation-屏幕快照 2019-01-26 上午11.29.11.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/28/Encoder-Decoder-with-Atrous-Separable-Convolution-for-Semantic-Image-Segmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/28/Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/">Rethinking Atrous Convolution for Semantic Image Segmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/DeepLab/">DeepLab</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1706.05587.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.05587.pdf</a><br>这篇论文是DeepLab系列的第三篇论文DeepLab V3. 相比较DeepLab V1和V2主要在研究如何更好的运用空洞卷积, 同时也去掉了之前的DenseCRF后处理模块。论文中主要涉及了两个方面一个是串行的连接空洞卷积，因为相比较传统的直接用conv层来增加网络的深度会导致抽象到最后一层会丢掉很多的原始信息，利用空洞卷积可以一定程度上优化这个问题。另一方面是在ASPP的基础上优化空洞卷积的使用，作者通过实验发现，如果一味的通过扩大空洞卷积的rate来扩大感受野，有时候并不能得到很好的结果，比如下图就说明当rate足够大的时候其实卷积核有效的参数量只有一个：<br><img src="Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation-屏幕快照 2019-01-26 上午11.08.28.png" alt=""></p>
<p>所以最后作者在ASPP模块中就直接用全图的global average pooling来替代空洞卷积来获取比较大区域的特征：<br><img src="Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation-20181221233024.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/28/Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/28/DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs/">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/DeepLab/">DeepLab</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener">https://arxiv.org/abs/1606.00915</a><br>DeepLab系列的第二篇论文，主要是在DeepLab V1的基础上提出了ASPP的结构来做多尺度的问题。<br>在DeepLab V1中作者其实也提到了multiscale的问题，当时论文中提到的方法就是对输入进行多次rescale，那么这种方法很显然太naive，并且也会带来比较大的运算量。因此DeepLab V2中作者就借鉴SPP的方法提出了ASPP的结构来解决多尺度的问题，ASPP本质就是用不同rate的空洞卷积来实现不同的感受野这样就可以覆盖比较多scale的物体，具体的应用逻辑论文中也给出了具体的结构：<br><img src="DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs-屏幕快照 2019-01-25 下午11.01.55.png" alt=""></p>
<p><img src="DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs-20181220234952.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/28/DeepLab-Semantic-Image-Segmentation-with-Deep-Convolutional-Nets-Atrous-Convolution-and-Fully-Connected-CRFs/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/24/SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS/">SEMANTIC IMAGE SEGMENTATION WITH DEEP CON- VOLUTIONAL NETS AND FULLY CONNECTED CRFS</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Segmentation/">Segmentation</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/DeepLab/">DeepLab</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1606.00915.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1606.00915.pdf</a><br>Semantic Segmentation比较经典的DeepLab系列的第一篇，DeepLab V1，主要利用FCN和DenseCRF来实现比较出色的segmentation效果，DeepLab V1的整个Pipeline整体可以分成两个部分: FCN得到相对比较粗糙的分割结果，DenseCRF在FCN结果的基础上对边缘进行Refine得到相对比较分明的物体分割轮廓。<br>在FCN部分主要是基于VGG16，将VGG16中FC换成Conv变成全卷积网络，而为了保持相对比较dense的feaure map,从VGG16原来的32倍下采样提高到8倍下采样，这个可以通过改变stride再加上空洞卷积来保持感受野，空洞卷积的具体示意图如下，网上也可以找到比较详细的解释：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午8.53.19.png" alt=""><br>最后在inference的时候可以直接从8倍下采样的结果直接插值回原图resolution.</p>
<p>第一阶段FCN得到的pixel wise的结果无疑是相对比较粗糙的，论文中作者也给了一些对比的图,FCN的结果相比较gt还是有比较大的差距的：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.21.37.png" alt=""><br>论文中提及的FC CRF本身是针对轮廓的处理提出来的，论文中也给出了具体的计算公式，x<sub>i</sub>为pixel的label，I为pixel的颜色轻度，整体逻辑是基于pixel的位置和pixel的颜色强度尽量让“相似”的pixel归为一类，让“不同”的pixel归为另一类，这样就可以在物体的边缘区域有一个比较好的划分：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.29.59.png" alt=""><br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.30.02.png" alt=""></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在VOC2012上的测试结果，MSc代表多尺度的特征融合，论文中是对网络的最后4个mas pooling层进行特征融合，FOV则是和空洞卷积有关：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.38.17.png" alt=""><br>作者也过空洞卷积的参数设置进行了简单的实验：<br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.40.38.png" alt=""><br><img src="SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS-屏幕快照 2019-01-24 下午9.41.21.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/24/SEMANTIC-IMAGE-SEGMENTATION-WITH-DEEP-CON-VOLUTIONAL-NETS-AND-FULLY-CONNECTED-CRFS/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/24/PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model/">PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Skeleton/">Skeleton</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1803.08225" target="_blank" rel="noopener">https://arxiv.org/abs/1803.08225</a><br>这篇论文其实和之前读的G-RMI那篇论文是同一个作者，G-RMI是top down的逻辑，而这篇论文是bottom up的逻辑。论文所提方法同时在做pose estimation和instance-level person segmentation两个task。pose estimation主要是通过预测点对之间的向量来做group，seg则是主要借鉴embedding的逻辑通过设计新的loss函数来优化效果。<br>下图是论文所提方法的整个pipeline，两个大的分支，pose + seg：<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-03 下午11.18.23.jpg" alt=""></p>
<p>先说一下论文中对pose estimation的做法，这一部分主要涉及三个内容：</p>
<ul>
<li><strong>heatmap生成</strong>：k个joint， k channel的heatmap，同时针对每一个joint人为围绕这个joint设置一个半径为R的圆（论文中R=32pixel），圆内的点为1，其余为0，构造一个分类任务来监督heatmap的生成。</li>
<li><strong>short-range offset</strong>: 把G-RMI那篇论文中用到的方法拿过来应用，主要是在heatmap的基础上还会预测在joint半径R pixel内的点与当前joint点的offset。然后再整合heatmap和offset（hough voting）得到最后的位置信息具体如上图。</li>
<li><strong>mid-range offset</strong>: 这个内容主要是用来对多人做group的。主要是想通过预测两个joint之间的offset向量在inference的进行instance的划分，但是在整张图中预测两点之间准确的offset是很难的，所以作者就把这个问题转换成两个joint半径R区域内相对应的点之间的offset和圆内点到joint之间的offset的加和来避免这样的问题，相当于用两个相对比较糙的结果来refine 最后的offset：<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-03 下午11.20.40.png" alt=""><br>论文中另外一个主要的部分就是关于seg的内容, 也引出了论文提到的第三个offset，long-range offset：<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-06 下午11.19.49.jpg" alt=""><br>论文所提做seg的方法主要是借鉴了embedding 的逻辑比如associate embedding方法，这些方法的一个主要思想就是设计一个loss使得属于同一个instance的点离的尽量近，不属于同一个instance的点离的尽量远，long-range offset就是作者在设计loss函数（距离函数）时提到的概念。long-range offset就是instance内的点到某一个joint的offset，上图画的比较清楚，具体计算distance的时候是：<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-06 下午11.30.27.png" alt=""><br>其中G(x) = x + L(x), L(x）就是long-range offset.</li>
</ul>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>Pose Estimation:<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-06 下午11.32.43.png" alt=""><br>Segmentation:<br><img src="PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model-屏幕快照 2019-01-06 下午11.33.19.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/24/PersonLab-Person-Pose-Estimation-and-Instance-Segmentation-with-a-Bottom-Up-Part-Based-Geometric-Embedding-Model/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/22/Fully-Convolutional-Siamese-Networks-for-Object-Tracking/">Fully-Convolutional Siamese Networks for Object Tracking </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Tracking/">Tracking</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1606.09549" target="_blank" rel="noopener">https://arxiv.org/abs/1606.09549</a><br>Siamese-FC v1 , 利用Siamese网络来做跟踪，论文中把跟踪的任务定义为两个图像patch之间的相似度，通过计算两个图像patch之间的相似度来定位物体，通过多次rescale 输入图片来实现多尺度物体的跟踪。<br>下图是论文中给出的Siamese-FC v1基本的示意图：<br><img src="Fully-Convolutional-Siamese-Networks-for-Object-Tracking-d0c371c53a45ca34e71ad8a64fdc055b53e0f958.png" alt=""></p>
<p>网络的第一个分支为目标object，输入被resize到127x127，经过全卷积网络之后得到6x6x128的输出。<br>网络的第二个分支为搜索的图片，经过同样的全卷积网络之后也会得到相对应的输出比如上图的22x22x128, 这两个分支的输出最后会通过 cross- correlation 操作(第一个分支的输出会作为第二个分支输出的kernel进行卷积操作)得到最后的输出，比如上图的17x17x1，那么直观上来看对于最后输出的score map上的每一个点其实就对应到原图和目标object图像同样大小的区域，而score map上这个点的取值就可以理解为原图中这块区域和目标object之间的相似度，那么最后相似度最高的点就被定位为目标object在当前帧上面的位置。<br>至于具体训练的时候论文中也提到了一些细节，比如正负例的定义，score map中落在中心半径R范围内的点被定义为正例label为1，其余为-1. 输出的score map会利用cosine window来抑制距离中心比较远的点。多尺度物体的检测则是直接通过rescale输入图片来实现的。<br><img src="Fully-Convolutional-Siamese-Networks-for-Object-Tracking-9ec817caf543acb7d9a52ed7008a1a31d9679a8d.jpeg" alt=""><br>一些实验结果,SiamFC-3s代表进行三次scale缩放：<br><img src="Fully-Convolutional-Siamese-Networks-for-Object-Tracking-b23460370758b3eca2601241e9fcda1eec173e47.png" alt=""><br><img src="Fully-Convolutional-Siamese-Networks-for-Object-Tracking-f83536950b93ff0983f49a137ba3b018ba9c7fd1.png" alt=""><br>Siamese-FC因为是从分类的角度来做位置的定位的，所以感觉框的精度会比较不够精确</p>
</div></div><a class="button-hover more" href="../../2019/01/22/Fully-Convolutional-Siamese-Networks-for-Object-Tracking/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/20/IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks/">IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1806.00178" target="_blank" rel="noopener">https://arxiv.org/abs/1806.00178</a><br>IGCV3，BMVC2018，在IGCV2基础上进行的改进，主要引入bottleneck的网络结构来改善IGCV2网络的信息交互，从而进一步提高网络效果。<br>在IGCV2中曾提到过一个互补条件：前面一级group convolution里不同partition的channel在后面一级的group convolution要在同一个partition里面，那么follow这个逻辑设计的IGCV2网络结构的每一个输出channel、输入channel之间将有且仅有一条路径可以连接，示意图如下，这就会导致网络过于稀疏不一定有利于网络的整体性能：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-a0f037095b27476caa3cf49ca30929b2923f2315_1_690x253.jpeg" alt=""></p>
<p>那么在IGCV3网络设计中借助bottleneck结构提出了super-channel的概念，每一个super-channel其实就是一组channel，first group convolution是1x1的卷积，它把网络变宽，second group convolution是3x3的spatial conv，third group convolution 同样是1x1的卷积，它又把网络结构压缩到原来的宽度，整个过程中的permutation和IGCV1、IGCV2的逻辑是一致的，这样设计之后每一个输出和输入的channel之间都会有多条路径可以交互，这样就优化了IGCV2网络设计中的问题，具体逻辑如下图：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-dfcbf9fea9f926a00f49a63e9242ce7a83ec10e8_1_690x316.png" alt=""><br>IGCV1、IGCV2、IGCV3在CIFAR上面的对比：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-2460c1c050b642ec41193bbc0f29770f65933ac2_1_690x242.png" alt=""><br>和MobileNetV2的对比：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-屏幕快照 2019-01-19 上午11.38.42.png" alt=""><br>和其他现有针对移动平台设计的网络对比：<br><img src="IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks-屏幕快照 2019-01-19 上午11.39.27.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/20/IGCV3-Interleaved-Low-Rank-Group-Convolutions-for-Efficient-Deep-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/20/IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks/">IGCV2: Interleaved Structured Sparse Convolutional Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1804.06202" target="_blank" rel="noopener">https://arxiv.org/pdf/1804.06202</a><br>IGCV2, CVPR2018, 主要把IGCV1中提到的group convolution进行了推广，将卷积操作变的更加稀疏，以此来达到减少冗余的目的。<br>在IGCV1论文中作者把IGC操作抽象成如下的表达式，x为输入，x前面的表达式整体是一个dense convolution kernel：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-f1ed32b809c7dda6f6c1b5a8834e594e6c0b99ce.png" alt=""></p>
<p>本论文中提到的Interleaved Structured Sparse Convolution就是它的一般表达：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-c181db945a7431adc7c182e16398ada2cc287be4.png" alt=""><br>那么为了保证输入x前面的表达式同样是dense convolutional kernel，论文中做了条件约束：前面一级group convolution里不同partition的channel在后面一级的group convolution要在同一个partition里面, 那么follow IGCV1的结构设计第一级group convolution为spatial conv (3x3)其余为1x1:<br>下图是wangjingdong的PPT上对IGCV2结构的描述示意图，比论文里面的示意图要清晰直观很多，其实就是把IGCV1的第二个group convolution再划分成多个1x1的group convolution操作 ：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-6268924f405c5224270e8fe5c80de272c04dbfa2_1_690x286.png" alt=""><br>实验Group Convolution的组数对结果的影响：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-b493611cc06c1996f7187b42c8cceaffa7452a20_1_539x500.png" alt=""><br>和其他一些网络模型的结果比较：<br><img src="IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks-e6c4c428b992b470ad7c2655ef49528184b321e9_1_608x500.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/20/IGCV2-Interleaved-Structured-Sparse-Convolutional-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/20/Interleaved-Group-Convolutions-for-Deep-Neural-Networks/">Interleaved Group Convolutions for Deep Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-10</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1707.02725" target="_blank" rel="noopener">https://arxiv.org/abs/1707.02725</a><br>下图是IGCv1的整体示意图：<br><img src="Interleaved-Group-Convolutions-for-Deep-Neural-Networks-7d3b43e53abfbc53dc3dfca907e8935d56f3fcf5.jpeg" alt=""></p>
<p>具体做法，首先整个IGC block会被分成两个部分，primary group convolutions 和 secondary group convolutions：<br>primary group convolutions的输入首先会被分成L个partition，每个partition共M个channel， 每个partition内部进行一般的spatial conv操作，最后的输出还会是ML个channel，那么在secondary group convolutions阶段，上一阶段的输出会先被shuffle一下然后再划分为M个partition，每个partition L个channel，对于第i个partition它其中的M个channel分别来自于前一个阶段的每个partition的第i个channel组成，然后每个partition内部进行1x1的卷积操作，最后还是会输出ML个channel，这ML个打乱的channel会再次映射回输入时候的顺序, 整个过程被抽象成如下的卷积操作，P为序列化：<br><img src="Interleaved-Group-Convolutions-for-Deep-Neural-Networks-f1ed32b809c7dda6f6c1b5a8834e594e6c0b99ce.png" alt=""><br>除了IGC主要的逻辑以外论文中也花了比较多的篇幅在论文，IGC比常规的conv操作在同等参数量的情况下宽度更宽，论证比较简单具体可以参考论文，同时作者也通过实验证明了更宽的网络可以得到更好的结果：<br><img src="Interleaved-Group-Convolutions-for-Deep-Neural-Networks-9a13e111aef4563a2957a372c32d4f8136975302.png" alt=""></p>
<p>最后的一组实验结果<br><img src="Interleaved-Group-Convolutions-for-Deep-Neural-Networks-e51af6de7667412d2511a7cfb8d8d61c3d045247.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/20/Interleaved-Group-Convolutions-for-Deep-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/15/High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network/">High Performance Visual Tracking with Siamese Region Proposal Network</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Tracking/">Tracking</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2951.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2951.pdf</a><br>Siamese-RPN, CVPR2018一篇关于tracking的论文，论文所提方法的整个逻辑还是基于孪生网络来做，整体也可以理解为对Siamese-FC结构的改进，下图是Siamese-RPN的整个Pipeline：<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-15 下午11.31.31.png" alt=""></p>
<p>Siamese-RPN网络的整个逻辑还是分成两个分支, 上面一个分支针对关键帧，下面一个分支针对检测帧，两个分支本身的参数是共享的，Siamese网络完成对两张输入图片的特征提取，两个分支的输出还会经过上图的Region Proposal Network网络来整合两者的信息.<br>具体做法和RPN网络类似，前一层Siamese网络的每一个分支都会接入两层的conv然后分成两个分支，一支为cls，另一支为reg，关键帧的输出如上图的4×4×(2k×256) 和 4×4×(4k×256)，这两个输出会分别作为对应的检测帧输出的kernel来进行卷积运算（上图中的*号操作）。这两者最后的输出就是检测帧最后的检测结果。<br>那么在得到最后预测的结果时针对tracking的应用场景，作者也提出了一些选框的策略，一是默认物体的移动速度不快，所以只选取靠近中心的anchor：<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.43.46.png" alt=""><br>另外一个做法就是对当前得到的proposal进行重新排名，比如利用cosine window来对距离重点点比较远的候选框进行抑制或者同样基于物体移动速度不快的假设可以默认物体的size变化也不大所以对scale进行penalty，形变比较大的得分会被抑制，具体打分如下，k为超参数，r为proposal的ratio，r’为上一帧的ratio，s和s’则是对应的整体的ratio（输入图片多尺度scale），论文中应该采取的是后者：<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.48.02.png" alt=""><br><strong>实验结果</strong><br>VOT2015:<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.53.58.png" alt=""><br>VOT2016:<br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.55.21.png" alt=""><br><img src="High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network-屏幕快照 2019-01-20 上午10.55.25.png" alt=""><br>SiamRPN网络相比SiamFC直接出了框的具体坐标位置，感觉对于跟踪的任务更加合理，也避免了SiamFC相对比较繁琐的后处理，在测试集上的效果也是要比SiamFC好一些。</p>
</div></div><a class="button-hover more" href="../../2019/01/15/High-Performance-Visual-Tracking-with-Siamese-Region-Proposal-Network/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2019/01/14/Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks/">Learning to Track at 100 FPS with Deep Regression Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Tracking/">Tracking</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1604.01802" target="_blank" rel="noopener">https://arxiv.org/abs/1604.01802</a><br>论文主要提出了GOTURN的tracking框架，整体还是比较naive的一些tracking逻辑，下图是GOTURN整个的pipeline:<br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.26.02.png" alt=""></p>
<p>GOTURN整个框架是针对连续的两帧来做的，整个框架因此也主要分成两个分支，分别针对当前帧和前一帧，给定当前帧(t)和前一帧(t-1),t-1帧的物体是已知的，比如物体框中心为center(c<sub>x</sub>, c<sub>y</sub>),长宽分别是w、h。那么对于下面一个分支首先会以center为中心，长宽分别是k1w, k1h去抠图，k1这个系数用于控制context信息的量。那么对于上面一个分支，会从同样的位置center作为中心，长宽分别是k2w,k2h去抠图作为当前帧的搜索区域。论文中也提及了一下通常k2 = 2。这两个分支的输出最后会被concat到一起送入一个3层的FC网络，每层FC有4096个神经元，每个分支本身的结构是CaffeNet的前五层。</p>
<p>那么在具体训练的时候，论文中主要提及了一些数据增强的细节，比如论文就直接说明GOTURN本身在训练的时候就是针对低速运动的物体进行的设计，对于高速运动的物体并不能cover。至于GOTURN对低速运行物体的跟踪也跟论文中提到的数据增强方法有关，模型在具体训练的时候会构造这样的pair对：<br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.43.16.png" alt=""><br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.43.23.png" alt=""><br>其中控制偏移幅度的变量delta服从Laplace分布。</p>
<p><strong>具体的实验结果</strong><br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.46.49.png" alt=""><br>Ablation Study:<br><img src="Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks-屏幕快照 2019-01-14 下午10.47.31.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2019/01/14/Learning-to-Track-at-100-FPS-with-Deep-Regression-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/24/AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones/">AI Benchmark: Running Deep Neural Networks on Android Smartphones </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Mobile/">Mobile</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Platform/">Platform</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Hardware/">Hardware</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1810.01109" target="_blank" rel="noopener">https://arxiv.org/abs/1810.01109</a><br>关于手机各种硬件平台一篇比较好的科普论文，论文主要提出了一个AI Benchmark来客观评估各个手机平台包括华为海思、高通、联发科、三星、谷歌Pixel等在标准CV任务比如识别、分割上面的具体表现。</p>
<p>论文主要介绍了华为海思、高通、联发科、三星、谷歌Pixel等手机平台针对AI任务所做的具体优化：<br><img src="AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones-9992f821aff8fcd693990d97c8aa896813ce65b8.png" alt=""></p>
<ul>
<li>高通：SNPE(Snapdragon Neu- ral Processing Engine)，支持大多数主流的框架比如Caffe、Tensorflow等，需要INT8量化。</li>
<li>华为海思：NPU(Neural Processing Unit), 当前总共有两款NPU，970和980。目前只支持Caffe和Tensorflow平台，论文中也提到了目前应该只支持16-bit float。</li>
<li>联发科：APU(AI Processing Unit), 需要INT8量化，论文上写只有P60，现在应该还有其他的APU发布。框架的话支持Caffe、Tensorflow、ONNX。</li>
<li>三星：VPU(Vision Processing Unit), 主要用在手机摄像头，目前没有提出对AI任务的针对性支持</li>
<li>Google：IPU( Image Processing Unit ),支持16-bit int和8-bit int，目前也没有提出对AI任务的针对性支持。</li>
</ul>
<p>论文另外一个主要内容就是介绍了其”AI Benchmark”, 至于具体的任务和具体的模型下表给了一个相对比较仔细的介绍，总共是下表的8个任务外加一个Memory Limitation测试共9个测试任务:<br><img src="AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones-b0846738e8d03725f22af52704abcfd5224cbde1.png" alt=""></p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones-9d9e02feef163a08de25b0dfa8efbc42d96c5207.png" alt=""></p>
</div></div><a class="button-hover more" href="../../2018/12/24/AI-Benchmark-Running-Deep-Neural-Networks-on-Android-Smartphones/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/24/Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint/">Bottom-up Pose Estimation of Multiple Person with Bounding Box Constraint </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Skeleton/">Skeleton</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1807.09972" target="_blank" rel="noopener">https://arxiv.org/abs/1807.09972</a><br>一篇基于Bottom Up逻辑的pose estimation论文，但是实际上和刚读的PRN那篇论文比较类似可以理解为Top Down + Bottom Up结合的方法，论文所提的方法主要是OpenPose再结合人体框的检测来做多人的pose estimation。</p>
<p>论文所提方法的整个逻辑如下图,整体可以理解为OpenPose再加上人体BBox约束来提升group的效果：<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-ee790656fab70c4c1822e95038c1f8b7543c7177_1_690x279.jpg" alt=""></p>
<ul>
<li><strong>CNN Regression</strong>: 网络这个分支可以直接理解为OpenPose的逻辑，只是作者再OpenPose的基础上做了一些简单的修改比如更换backbone等，整体结构和逻辑和OpenPose基本是一致的：<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-c76f41b54234100ad9b73127621a3323fcb82b02.png" alt=""><br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-5071f6a4a4983012e8408382a76ed443faa1d773.png" alt=""></li>
<li><strong>Pose Parsing</strong>: 主要是利用CNN Regression分支的结果来解析pose，本论文所提方法在做pose parsing 的时候是每个人体框分别来做，不同于PRN，本文所提方法对于一个框是可以出多个人的结果的，具体贪心逻辑如下：<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-34dfefcffcd927265e9239e373f68117aea2cac8.png" alt=""></li>
<li><strong>Pose NMS</strong>：主要是制定了一些规则来定义Pose的Confidence、Pose的距离：<br>Confidence:s1是pose所有点confidence均值，s2是pose所有connection confidence的均值，B<sup>‘</sup>是pose最小外接矩形面积，B是bbox面积<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-ce3bf61db73b6e97c08606efbe034d5ae04912c7.png" alt=""><br>distance：<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-63ebd7be02c976cf5488aa5104f10dc6e99b7fde.png" alt=""><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2>作者只和OpenPose、CPM做了比较。。。。<br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-8c8f1ddc7edf786ac0fc6c02cefe7f968e295bd0.png" alt=""><br><img src="Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint-35821768d390b09996b5101a7f1b748ccd7366fa.png" alt=""><br>Top Down和Bottom Up分别都有各自比较明显的缺点和优点，目前也陆续有两者结合的研究工作出现，感觉是一个可以去研究的内容。</li>
</ul>
</div></div><a class="button-hover more" href="../../2018/12/24/Bottom-up-Pose-Estimation-of-Multiple-Person-with-Bounding-Box-Constraint/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="../../2018/12/24/MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network/">MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/Skeleton/">Skeleton</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../tags/BottomUp/">BottomUp</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1807.04067" target="_blank" rel="noopener">https://arxiv.org/abs/1807.04067</a><br>ECCV2018一篇利用bottom up方法做pose estimation的论文。论文所提出的方法主要是分别利用两个分支一个分支用来检测人体框，另一个分支用来出关键点的heatmap。然后再利用一个网络来merge 两个分支出的结果，将关键点分别映射到对应的人体框上实现多人的姿态估计。</p>
<p>下图是本文所提方法的大体Pipeline，整个pipeline可以分成三个部分，人体关键点检测、人体框检测和关键点的聚类（PRN）：<br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-b80970b3e1b91db9992796fdada5d5cacddc1531.jpg" alt=""></p>
<ul>
<li><strong>Backbone</strong>：Resnet + FPN</li>
<li><strong>关键点检测</strong>：整个分支结构如下图，比较直观，L2 loss，总共出K + 1 个heatmap，+1为seg的结果：<br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-a42ffc7c2d336f7a095fc898d260ef90c728a7f6.png" alt=""></li>
<li><strong>人体框检测</strong>：就是直接上的RetinaNet<br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-96f37d11a64105bce2440ed6b6955231dd1a6949.png" alt=""></li>
<li><strong>Pose Residual Network (PRN)</strong>：这一部分内容是论文的核心，主要是利用人体框将关键点检测的结果映射到对应的instance上。具体做法是在关键点检测的结果上利用对应人体框的位置crop出一样大小的patch，这样就可以得到K x W x H大小的feature map，K为关键点个数，W、H为对应人体框的size，然后把这个feature map resize到固定大小(36 x 56)作为PRN的输入。那么对于上图的c、d这种同一个框有多个人关键点overlap的情况，主要是利用一个多层感知机（residual multilayer perceptron）来把人体框对应instance的关键点提取出来，具体逻辑如下图：<br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-c340c5ea72c9c75d031dbcba3c8b569d4e393023.jpg" alt=""><br><strong>结果：</strong><br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-9255a515da09097264bb0c17a00cd420a3a16291.png" alt=""><br><img src="MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network-d87795e947cb3e80cf5311605ec05df40db408f6.png" alt=""></li>
</ul>
<p>这篇论文另一个比较重要的点是虽然整个网络的pipeline比较多但是inference的速度还是比较快的，对于典型的COCO图片（～3人）可以达到23FPS的速度，用来merge的PRN网络输入比较小层数也不深。</p>
</div></div><a class="button-hover more" href="../../2018/12/24/MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network/#more">阅读全文</a></div></div><div id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="../2/"><i class="fas fa-angle-left"></i></a><a class="page-number" href="../../">1</a><a class="page-number" href="../2/">2</a><span class="page-number current">3</span><a class="page-number" href="../4/">4</a><a class="extend next" rel="next" href="../4/"><i class="fas fa-angle-right"></i></a></div></div></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fas fa-user"></i></span><span id="busuanzi_value_site_uv"></span><span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fas fa-eye"></i></span><span id="busuanzi_value_site_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Out of Memory</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../js/copy.js"></script><!--script(src=url)--></body></html>