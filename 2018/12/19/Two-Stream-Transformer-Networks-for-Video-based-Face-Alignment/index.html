<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Live and Learn"><title>Two-Stream Transformer Networks for Video-based Face Alignment | Out of Memory</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Two-Stream Transformer Networks for Video-based Face Alignment</h1><a id="logo" href="/.">Out of Memory</a><p class="description">Live and Learn</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Two-Stream Transformer Networks for Video-based Face Alignment</h1><div class="post-meta">Dec 19, 2018<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" href="/2018/12/19/Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment/#vcomment"><span class="valine-comment-count" data-xid="/2018/12/19/Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment/"></span><span> 条评论</span></a><div class="post-content"><p>URL: <a href="https://ieeexplore.ieee.org/document/7999169" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/7999169</a><br>这是一篇TPAMI2018的论文，论文主要的研究内容是视频场景下的facial landmark 定位的问题。论文的motivation也比较直观，和之前看过的RED-Net很类似，就是想借助于视频流提供的temporal信息再加上静态图片的spatial信息来优化视频场景下的facial landmark问题，比如pose、遮挡等。</p>
<p>下面这幅图是论文中给出的整个框架的示意图，画的比较直观论文中所谓的Two-Stream就是Spatial stream 和 Temporal stream这两个branch，两个分支的输出最后通过不同的权重整合到一起作为最后的输出：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-f1e5f5d479db4e5b4c7d609bcd2acce9a43a4002_1_690x241.jpg" alt></p>
<ul>
<li>Spatial Stream: 这个分支通常是和一般的静态图片处理方式是一致的，论文中所提方法分为两个部分，sampler和regression， sampler是从原图中进行采样local patch的过程，local path就是每个landmark点周围的dxd的区域，论文中取d=26。regression则是一个标准的CNN网络，接受local patch为输入回归具体的landmark坐标（实际上回归的是offset），为了达到比较高的精度论文在实际做的时候其实做了两阶段的cascade：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-6c1b39b23cda6d28051df18db474abb7ab3e5c33_1_690x83.png" alt></li>
<li>Temporal Stream:这一个分支通常是视频场景下facial landmark定位特有的处理过程, 为了对视频的时序信息进行处理，不少的论文都是采用RNN模型来进行处理，本文也不例外，temporal stream分支的输入是一段连续的帧，这些帧首先会经过encoder进行处理提取图片的一些context的信息，然后会根据时序依次输入到两层RNN中，第一层RNN layer通常来编码一些整体的特征信息，第二层RNN layer则用来编码一些变化的时序信息例如pose等，最后RNN的输出会经过decoder映射回输入的size保持原有的spatial 信息然后再经过比较小的回归网络得到最后的landmark输出：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-e3ebddb40d960442b71b63296cf3ecd654878f7a_1_376x500.jpg" alt></li>
</ul>
<p>论文主要在300-VM和TF（Talking Face）两个数据集进行了实验，相比较之前的REDN、TCDCN模型都有提升：<br>300-VW上的表现：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-d371c0da62673b1d319fc818b8ca472efe4cf8c1_1_690x187.png" alt><br>视频场景下的facial landmark定位相比静态图片的facial landmark定位增加了时序信息可以利用，目前的研究所用方法也比较类似，之前看过的REDNet和这篇TSTN都是通过CNN+RNN的逻辑来整合spatial信息和temporal信息，感觉这种信息融合的方法还需要仔细的去研究。</p>
</div><div class="tags"><a href="/tags/Landmark/">Landmark</a><a href="/tags/Face/">Face</a></div><div class="post-nav"><a class="pre" href="/2018/12/24/MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network/">MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network</a><a class="next" href="/2018/12/19/DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks/">DropFilter: A Novel Regularization Method for Learning Convolutional Neural Networks</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'true' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'kJG6YxzEHXemVJF63TTH3GrX-gzGzoHsz',
  appKey:'o4lqtg6IRJ10QYXGuUJ2YLEC',
  placeholder:'我来说两句~',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://yoursite.com"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Engineering/">Engineering</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper-Reading/">Paper Reading</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Loss/" style="font-size: 15px;">Loss</a> <a href="/tags/Mobile/" style="font-size: 15px;">Mobile</a> <a href="/tags/Platform/" style="font-size: 15px;">Platform</a> <a href="/tags/Hardware/" style="font-size: 15px;">Hardware</a> <a href="/tags/Detection/" style="font-size: 15px;">Detection</a> <a href="/tags/Landmark/" style="font-size: 15px;">Landmark</a> <a href="/tags/KeyPoint/" style="font-size: 15px;">KeyPoint</a> <a href="/tags/BottomUp/" style="font-size: 15px;">BottomUp</a> <a href="/tags/Bottom-Up/" style="font-size: 15px;">Bottom_Up</a> <a href="/tags/Pose/" style="font-size: 15px;">Pose</a> <a href="/tags/Tools/" style="font-size: 15px;">Tools</a> <a href="/tags/Engineering/" style="font-size: 15px;">Engineering</a> <a href="/tags/Mutual-learning/" style="font-size: 15px;">Mutual learning</a> <a href="/tags/Classification/" style="font-size: 15px;">Classification</a> <a href="/tags/Segmentation/" style="font-size: 15px;">Segmentation</a> <a href="/tags/Regularization/" style="font-size: 15px;">Regularization</a> <a href="/tags/VID/" style="font-size: 15px;">VID</a> <a href="/tags/Face/" style="font-size: 15px;">Face</a> <a href="/tags/SOT/" style="font-size: 15px;">SOT</a> <a href="/tags/Tracking/" style="font-size: 15px;">Tracking</a> <a href="/tags/Track/" style="font-size: 15px;">Track</a> <a href="/tags/Mimick/" style="font-size: 15px;">Mimick</a> <a href="/tags/3D/" style="font-size: 15px;">3D</a> <a href="/tags/Template/" style="font-size: 15px;">Template</a> <a href="/tags/Bottom-UP/" style="font-size: 15px;">Bottom UP</a> <a href="/tags/DeepLab/" style="font-size: 15px;">DeepLab</a> <a href="/tags/Learning-Strategy/" style="font-size: 15px;">Learning Strategy</a> <a href="/tags/Classic/" style="font-size: 15px;">Classic</a> <a href="/tags/Basic/" style="font-size: 15px;">Basic</a> <a href="/tags/Quantization/" style="font-size: 15px;">Quantization</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/06/28/Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm/">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/28/Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection/">Quantization Mimic: Towards Very Tiny CNN for Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/24/Mimicking-Very-Efficient-Network-for-Object-Detection/">Mimicking Very Efficient Network for Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/20/Tone-Mapping/">Tone Mapping</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/15/Lightweight-Real-time-Makeup-Try-on-in-Mobile-Browsers-with-Tiny-CNN-Models-for-Facial-Tracking/">Lightweight Real-time Makeup Try-on in Mobile Browsers with Tiny CNN Models for Facial Tracking</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/10/Improving-Landmark-Localization-with-Semi-Supervised-Learning/">Improving Landmark Localization with Semi-Supervised Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/05/Seeing-Small-Faces-from-Robust-Anchor’s-Perspective/">Seeing Small Faces from Robust Anchor’s Perspective</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/01/Style-Aggregated-Network-for-Facial-Landmark-Detection/">Style Aggregated Network for Facial Landmark Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/25/Deep-Regionlets-for-Object-Detection/">Deep Regionlets for Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/21/Regionlets-for-Generic-Object-Detection/">Regionlets for Generic Object Detection</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/libanghuai" title="Github" target="_blank">Github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Out of Memory.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script type="text/javascript" color="0,0,0" opacity="0.5" zindex="-2" count="50" src="//lib.baomitu.com/canvas-nest.js/2.0.3/canvas-nest.umd.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>