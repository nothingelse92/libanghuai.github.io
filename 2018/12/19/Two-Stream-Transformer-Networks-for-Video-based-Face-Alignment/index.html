<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Two-Stream Transformer Networks for Video-based Face Alignment"><meta name="keywords" content="Landmark,Face"><meta name="author" content="Out of Memory,undefined"><meta name="copyright" content="Out of Memory"><title>Two-Stream Transformer Networks for Video-based Face Alignment【Out of Memory】</title><link rel="stylesheet" href="../../../../css/fan.css"><link rel="stylesheet" href="../../../../css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="../../../../favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="../../../../js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Out of Memory</div><div class="author-info-description">Live and Learn</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/libanghuai" target="_blank">GitHub<i class="icon-dot bg-color5"></i></a><a class="links-button button-hover" href="mailto:libanghuai@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color3"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1185719433&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color7"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="../../../../archives"><span class="pull-top">日志</span><span class="pull-bottom">106</span></a><a class="author-info-articles-tags article-meta" href="../../../../tags"><span class="pull-top">标签</span><span class="pull-bottom">36</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Out of Memory</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">Two-Stream Transformer Networks for Video-based Face Alignment</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2018-12-19 | 更新于 2019-12-23</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../../../tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="../../../../tags/Face/">Face</a></div></div></div><div class="main-content"><p>URL: <a href="https://ieeexplore.ieee.org/document/7999169" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/7999169</a><br>这是一篇TPAMI2018的论文，论文主要的研究内容是视频场景下的facial landmark 定位的问题。论文的motivation也比较直观，和之前看过的RED-Net很类似，就是想借助于视频流提供的temporal信息再加上静态图片的spatial信息来优化视频场景下的facial landmark问题，比如pose、遮挡等。</p>
<p>下面这幅图是论文中给出的整个框架的示意图，画的比较直观论文中所谓的Two-Stream就是Spatial stream 和 Temporal stream这两个branch，两个分支的输出最后通过不同的权重整合到一起作为最后的输出：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-f1e5f5d479db4e5b4c7d609bcd2acce9a43a4002_1_690x241.jpg" alt=""></p>
<ul>
<li>Spatial Stream: 这个分支通常是和一般的静态图片处理方式是一致的，论文中所提方法分为两个部分，sampler和regression， sampler是从原图中进行采样local patch的过程，local path就是每个landmark点周围的dxd的区域，论文中取d=26。regression则是一个标准的CNN网络，接受local patch为输入回归具体的landmark坐标（实际上回归的是offset），为了达到比较高的精度论文在实际做的时候其实做了两阶段的cascade：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-6c1b39b23cda6d28051df18db474abb7ab3e5c33_1_690x83.png" alt=""></li>
<li>Temporal Stream:这一个分支通常是视频场景下facial landmark定位特有的处理过程, 为了对视频的时序信息进行处理，不少的论文都是采用RNN模型来进行处理，本文也不例外，temporal stream分支的输入是一段连续的帧，这些帧首先会经过encoder进行处理提取图片的一些context的信息，然后会根据时序依次输入到两层RNN中，第一层RNN layer通常来编码一些整体的特征信息，第二层RNN layer则用来编码一些变化的时序信息例如pose等，最后RNN的输出会经过decoder映射回输入的size保持原有的spatial 信息然后再经过比较小的回归网络得到最后的landmark输出：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-e3ebddb40d960442b71b63296cf3ecd654878f7a_1_376x500.jpg" alt=""></li>
</ul>
<p>论文主要在300-VM和TF（Talking Face）两个数据集进行了实验，相比较之前的REDN、TCDCN模型都有提升：<br>300-VW上的表现：<br><img src="Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment-d371c0da62673b1d319fc818b8ca472efe4cf8c1_1_690x187.png" alt=""><br>视频场景下的facial landmark定位相比静态图片的facial landmark定位增加了时序信息可以利用，目前的研究所用方法也比较类似，之前看过的REDNet和这篇TSTN都是通过CNN+RNN的逻辑来整合spatial信息和temporal信息，感觉这种信息融合的方法还需要仔细的去研究。</p>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Out of Memory</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="http://libanghuai.com/2018/12/19/Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment/">http://libanghuai.com/2018/12/19/Two-Stream-Transformer-Networks-for-Video-based-Face-Alignment/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://libanghuai.com">Out of Memory</a>！</span></div></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="../../24/MultiPoseNet-Fast-Multi-Person-Pose-Estimation-using-Pose-Residual-Network/"><i class="fas fa-angle-left">&nbsp;</i><span>MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="../DropFilter-A-Novel-Regularization-Method-for-Learning-Convolutional-Neural-Networks/"><span>DropFilter: A Novel Regularization Method for Learning Convolutional Neural Networks</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Out of Memory</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="../../../../js/copy.js"></script><!--script(src=url)--></body></html>