<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Live and Learn"><meta name="keywords" content=""><meta name="author" content="Out of Memory,undefined"><meta name="copyright" content="Out of Memory"><title>Live and Learn【Out of Memory】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="author-info"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Out of Memory</div><div class="author-info-description">Live and Learn</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/libanghuai" target="_blank">GitHub<i class="icon-dot bg-color0"></i></a><a class="links-button button-hover" href="mailto:libanghuai@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color10"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1185719433&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color9"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="archives"><span class="pull-top">日志</span><span class="pull-bottom">109</span></a><a class="author-info-articles-tags article-meta" href="tags"><span class="pull-top">标签</span><span class="pull-bottom">36</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Out of Memory</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><div id="recent-posts"><!-- each post in page.posts.sort('date', -1).limit(10).toArray()--><!-- config中配置按照什么排序--><div class="recent-post-item"><a class="post-title" href="2020/04/10/SimpleShot-Revisiting-Nearest-Neighbor-Classification-for-Few-Shot-Learning/">SimpleShot: Revisiting Nearest-Neighbor Classification for Few-Shot Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-10</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1911.04623.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1911.04623.pdf</a></p>
<p>一篇打脸的论文，现在做few shot任务的方法通常有meta learning/metric learning等方法，作者在论文里claim其实只要对feature extrator输出的feature进行最近邻统计就可以在few shot benchmark上得到很高的指标，这一类简单且高效的few shot方法可以参考另外两篇论文：<em>Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?</em>, 以及做检测的: <em>Frustratingly Simple Few-Shot Object Detection</em>。 论文很简短，做法很简单，效果很不错。</p>
<p>看看论文怎么做的吧，首先数据集上面会划分成D<sup>base</sup>, D<sup>noval</sup>, 然后用一般的CNN网络基于D<sup>base</sup>去训练一个分类器，这里作者也用了五个不同的backbone，Conv-4，WRN-28-10， DenseNet-121，ResNet-10/18，MobileNet.</p>
<p>当分类模型训练好以后变成feature extractor，对novel class里面的test数据提取特征，同时也对novel class里的support数据提取特征，通过计算两者的相似度(最短距离)来match test数据的label，如果support数据集里每一类的图片有多张K-shot，那么就做个平均作为这一类的类别中心，test图片的feature就和这个类别中心算最短距离就好<br>。</p>
<p>然后对于feature extractor的数据作者也做了一些ablation，一是比做任何操作，直接去计算距离，二是L2 norm一下，三是对feature减均值再做norm，作者分别做了比较详细的对比实验：</p>
<p><img src="SimpleShot-Revisiting-Nearest-Neighbor-Classification-for-Few-Shot-Learning-屏幕快照 2020-04-10 下午12.44.47.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/04/10/SimpleShot-Revisiting-Nearest-Neighbor-Classification-for-Few-Shot-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/21/Spatial-aware-Graph-Relation-Network-for-Large-scale-Object-Detection/">Spatial-aware Graph Relation Network for Large-scale Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-21</time></div><div class="post-content"><div class="main-content content"></div></div><a class="button-hover more" href="2020/03/21/Spatial-aware-Graph-Relation-Network-for-Large-scale-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/15/Few-Shot-Object-Detection-with-Attention-RPN-and-Multi-Relation-Detector/">Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1908.01998.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1908.01998.pdf</a></p>
<p>这应该是CVPR2020的论文了, 看完之后我觉得挺失望的。做法上我觉得是完全把用在tracking任务上的siamese网络原封不动的迁移过来了，然后换了一个few shot detection的故事在讲，虽然论文中作者在siamese网络的后面花了不少笔墨写了Patch-Relation Head, Local-Correlation Head, Global-Relation Head 但是注意看Table 3 实际上点上面最大的收益就是siamese网络，加上一堆的trick之后实际上只涨了一个点(68.6% -&gt; 69.8%).</p>
<p><img src="Few-Shot-Object-Detection-with-Attention-RPN-and-Multi-Relation-Detector-截屏2020-03-2118.40.42.png" alt=""></p>
<p>论文所提的网络结构是基于孪生网络来做的，support set和query set分别送入孪生网络的两个分支，两个分支的backbone参数是共享了，两者抽完feature之后会对两个feature做一个相似度计算得到一个新的feature map，这个feature map会送去产生proposal，这一步就是论文里面提到的<strong>attention mechanism</strong>. 然后就是这两组roi pooling之后feature的交互了，分为三种:</p>
<ol>
<li><strong>patch-relation head</strong> 两个分支的roi feature concat到一起送入一个子网络出cls + reg</li>
<li><strong>global-relation head</strong> 两个分支的roi feature concat到一起，然后走一个avg pooling 再经过两层FC后出一个score</li>
<li><strong>local-correlation head</strong> 用和孪生网络差不多的cross correlation两个feature 卷一把走fc后又出了一个分</li>
</ol>
<p>最后的分就是三者之和，reg的结果就用<strong>patch-relation head</strong>出的结果</p>
</div></div><a class="button-hover more" href="2020/03/15/Few-Shot-Object-Detection-with-Attention-RPN-and-Multi-Relation-Detector/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/14/RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection/">RepMet: Representative-based metric learning for classification and few-shot object detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-15</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1806.04728" target="_blank" rel="noopener">https://arxiv.org/abs/1806.04728</a></p>
<p>这是CVPR2019的论文了，用metric learning来做few shot的，作者在论文里主要提了一个<em>插件subnet</em>可以用来替换掉分类任务或者定位任务中的class分支。下面这张图可以比较好的阐述问题：</p>
<p><img src="RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection-截屏2020-03-1519.19.31.png" alt=""></p>
<p>因为论文所提的这个结构是一个subnet，可以无缝的插入到现有的网络结构中，所以它的输入通常是具体的feature，对于检测任务来说就是<strong>ROI的feature</strong>，输入的feature首先会经过一个embedding module，论文中对于分类任务这个embedding module就是2层FC，对于检测任务就是3层FC，这样就可以把输入的feature压缩到很小的维度(e), 再来看论文中提出的核心概念(<em>‘representatives’</em>), 作者把每一个类别抽象成k个mode，每个mode定义为e长度的vector(为了和embedding module保持一致)，具体实现的时候就是一个N x k x e大小的fc在具体操作的时候reshape一把就好，这个fc输入恒为1。</p>
<p>得到这些信息之后就可以计算距离了，embedding之后的结果和representatives直接相乘就可以得到一个N x k的结果，默认服从高斯分布, 这样可以得到每一个class对应的每一个mode的概率:</p>
<p><img src="RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection-截屏2020-03-1519.27.31.png" alt=""></p>
<p>那么embedding之后的feature或者输入feature对应这些具体类别的概率呢, 取mode里最大的概率:</p>
<p><img src="RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection-截屏2020-03-1519.27.36.png" alt=""></p>
<p>然后这篇论文不太好理解的地方是模型的训练，对于few shot检测来说，训练依然服从c way k shot这样的采样逻辑来小规模训练，每次训练的时候会用新类别的embedding结果直接替换掉原来的<strong>representatives</strong>进行fine tune学习。可能这一步就是针对小样本学习设定的。</p>
</div></div><a class="button-hover more" href="2020/03/14/RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/13/Conditional-Convolutions-for-Instance-Segmentation/">Conditional Convolutions for Instance Segmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-14</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/2003.05664.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2003.05664.pdf</a></p>
<p>最新放出来的论文，做Instance Segmentation，和PolarMask一样都是follow FCOS大的框架，不同于Mask RCNN出框crop然后做Seg的方式，论文所提的方法更加类似FCN直接出，整个框架的设计感觉还是比较精巧的。具体看下怎么做：</p>
<p><img src="Conditional-Convolutions-for-Instance-Segmentation-屏幕快照 2020-03-13 下午2.42.24.png" alt=""></p>
<p>我把这个结构分成两个部分:</p>
<ol>
<li>FCOS: 也就是上图的上半部分，整个pipeline和FCOS没啥差别，只是head层的输出略有不一样，那么对于FPN每一个layer的每一个pixel，主要出三个东西:<ol>
<li>Classification Head: 和原版FCOS含义一样</li>
<li>Center-ness Head： 这个定义也是和原版FCOS一致的，用来抑制不太好的预测结果</li>
<li>Controller Head： 这个就是本篇论文的核心了，他负责出Mask FCN Head的参数，假设Mask FCN Head的总参数量是X，那么Controller Head的维度就是X，论文中当X取169的时候性能就比较不错了。</li>
</ol>
</li>
<li>Mask FCN Head: Mask FCN Head是论文的核心点，上图下半部分，它的结构就是一般的FCN，但是它的特点在于FCN的参数是动态的，不同的instance有不同的参数，这就会造成多个Mask FCN Head的感觉，同时功能上也类似Mask RCNN出框的作用-区分Instance. Mask FCN Head接在P3 Layer之后, 经过几层Conv之后得到一个H x W x C的feature map, 论文中C = 8，作者claim C的取值对分割的性能影响不大. 甚至C = 2的时候性能也只是下降0.3%！因为Mask FCN Head负责出instance，而其参数又是由P3 - P7的head层所得，所以为了构建两者的联系，在Mask FCN Head输入层F<sub>mask</sub> Concat了F<sub>mask</sub>到P3 - P7的相对位移，假设F<sub>mask</sub>的维度为H x W x C，P<sub>i</sub>的维度为H<sub>i</sub> x W<sub>i</sub> x C<sub>i</sub>, 那么把F<sub>mask</sub>的每一个pixel映射到P<sub>i</sub>，映射前后坐标的offset就会和原始的F<sub>mask</sub> Concat到一起作为<strong>Mask FCN Head</strong>的输入。</li>
</ol>
<p>最后模型的效果就是instance aware的segmentation:<br><img src="Conditional-Convolutions-for-Instance-Segmentation-屏幕快照 2020-03-13 下午3.51.40.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/03/13/Conditional-Convolutions-for-Instance-Segmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/13/Meta-R-CNN-Towards-General-Solver-for-Instance-level-Low-shot-Learning/">Meta R-CNN : Towards General Solver for Instance-level Low-shot Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-14</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1909.13032" target="_blank" rel="noopener">https://arxiv.org/abs/1909.13032</a></p>
<p>ICCV 2019的论文，论文标题比较直白，meta learning + Faster/Mask RCCN做物体检测/分割。作者在论文里强调了few shot场景下物体分类和物体检测的差异，物体分类通常对全图做，物体检测由于全图有不同的多个物体所以并不适用，那么自然而然的就想到在ROI后面做。那么论文就想办法把meta learning的机制融合进two stage的pipeline里面。</p>
<p><img src="Meta-R-CNN-Towards-General-Solver-for-Instance-level-Low-shot-Learning-截屏2020-03-1423.34.50.png" alt=""></p>
<p>论文所提的网络结构也比较简单，首先保留了Faster/Mask RCNN的整个pipeline，在此基础上增加了一个Predictor-head Remodeling Network(PRN)子网络，这个自网络也是这篇论文的主要贡献所在，出发点的话和很多的few shot的论文很类似就是想办法增强网络对于不同类别的区分度，手段也是一样的就是各种atttention。PRN的作用呢也是用来学习不同class的attention编码(class-attentive vectors)。<strong>这个编码和同是ICCV2019论文的feature reweight方法的不一样点是那篇论文最后学的是channel attention，每一个channel代表一个类，以此来增强某一个或某几个channel，而这篇论文学的是对于给定类别的roi feature学习类内每个channel的权重，所以PRN输出是固定长度的vector代表每个类别对应的attention</strong>。</p>
<p>论文整体的结构就是这样，主要需要注意的应该是模型的过程了，首先模型训练的时候还是会遵循一般meta learning的策略，构建C way k shot的数据集，support集 D<sub>train</sub> 和 query 集D<sub>meta</sub>,其中D<sub>train</sub>作为Faster/Mask RCNN的输入，D<sub>meta</sub>作为PRN的输入，这样两者联动同时加上Loss<sub>cls</sub> / Loss<sub>reg</sub> / Loss<sub>meta</sub>来共同监督网络的训练。Loss<sub>meta</sub>用的是ce loss.</p>
<p>至于在最后的inference可以采取和feature reweight那篇论文一样的策略，class-attentive vector可以根据已有数据直接预处理得到就好。</p>
</div></div><a class="button-hover more" href="2020/03/13/Meta-R-CNN-Towards-General-Solver-for-Instance-level-Low-shot-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/13/Incremental-Few-Shot-Object-Detection/">Incremental Few-Shot Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-13</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Incremental-Learning/">Incremental Learning</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/2003.04668" target="_blank" rel="noopener">https://arxiv.org/abs/2003.04668</a></p>
<p>CVPR2020的论文，论文算是提出了一个新的子任务，Few Shot + Incremental Learning，这个任务论文中也给了明确的定义：</p>
<ol>
<li>首先模型的训练是可以基于量比较大的base class的数据来训练的，这样可以得到一个还不错的检测模型</li>
<li>novel class的数据可以随时<em>注册</em>，同时数据量很少，这样模型可以做到随时部署同时可以cover新增的类别</li>
</ol>
<p><img src="Incremental-Few-Shot-Object-Detection-屏幕快照 2020-03-13 下午12.10.03.png" alt=""></p>
<p>然后我们具体来看看论文是怎么做的, 首先模型方面被拆成两个部分</p>
<ol>
<li>feature extractor: 论文中描述为class-agnostic, 相当于这一块是类别无关的base class和novel class公用这一部分用来特征提取</li>
<li>class code generator: 顾名思义，学习不同类别的编码，和上述提取出来的feature整合作为最后detection的输入</li>
</ol>
<p>模型的学习也按照上面的两个模块分成了两个stage：</p>
<p>stage I: <strong>feature extractor</strong>的学习，这部分内容没有特别的地方就是CenterNet，训练完之后把detection head去掉之后就是feature extractor了</p>
<p>stage II: <strong>class code generator</strong>的学习, 这一部分的理解需要把CenterNet heatmap输出那一层进行一定的分解，假设feature extractor最后输出的feature map维度是H x W x C，总共有N个类别，那么逻辑上来讲heatmap维度应该是h x w x N，这个维度转化可以通过1 x 1的卷积达到目的，那么分解一下对于其中的一层feature map相当于H x W x C经过 1 x 1 x C的conv得到！</p>
<p>作者把这个理解为class independent的一个编码，这也是feature map做法的好处类别之间是相对独立的. <em>class code generator</em> 就被定义成学习这个class independent的编码. 至于怎么学呢, meta learning, 每一轮学习通过sample support集和query集去refine模型，对于给定一组输入，走 <em>feature extractor</em> 得到对应的feature，走 <em>class code generator</em> 得到对应的code，两者一结合就可以得到一个h x w的heatmap，然后拿这个heatmap去和gt做L1 loss从而来监督<em>class code generator</em>的学习</p>
<p>那么在Inference的时候呢对于base class的数据就直接上训练好的CenterNet，对于novel class的数据就分别走一遍<em>feature extractor</em>和<em>class code generator</em>然后得到最后的输出，这就是最后的定位结果。</p>
</div></div><a class="button-hover more" href="2020/03/13/Incremental-Few-Shot-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/12/Objects-as-Points/">Objects as Points</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-12</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL：<a href="https://arxiv.org/abs/1904.07850" target="_blank" rel="noopener">https://arxiv.org/abs/1904.07850</a></p>
<p>Anchor free做检测的论文CenterNet，应该是中了CVPR2019，整体是把做Pose常用的heatmap的做法放到了检测任务上，当然也可以理解为Pose Estimation/Detection的一种统一，毕竟论文里面也是给出了同样框架下在Pose Estimation和3D Detection任务上面的表现。</p>
<p>具体看下在普通的检测任务上是怎么做的吧：</p>
<p><img src="Objects-as-Points-屏幕快照 2020-03-12 下午6.27.05.png" alt=""></p>
<p>对问题的建模比较好理解，通常物体的框标注为(x1, y1, x2, y2)，那么论文中作者把物体框定义为Center Point + Width/Height，整个网络基于heatmap出结果，每一个pixel出C + 4维输出，所以heatmap最后输出的channel应该是(C + 4), C代表类别，C个channel每一个pixel位置的值就代表这个pixel属于这个类别的概率，剩下来的4维分别是基于这个pixel的offset(2维) 和 object的长宽(2维), 分类用的focal loss，回归用的L1 Loss，至于在inference的时候论文里写的也挺清楚的：找出C每个heatmap的峰值(最多100个)，峰值的定义就是比周围8个近邻大的点，找到这100个峰值之后接可以利用对应4个channel的输出还原出对应的100个框，这就是最后的结果，这些框不再需要走NMS，可以通过简单的卡阈值完成模型的输出。</p>
<p>CenterNet整体的思想就是这些，十分简单有效是一篇不错的工作，另外一个好处是这篇论文最后的代码也开源了: <a href="https://github.com/xingyizhou/CenterNet" target="_blank" rel="noopener">https://github.com/xingyizhou/CenterNet</a></p>
</div></div><a class="button-hover more" href="2020/03/12/Objects-as-Points/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/02/28/Few-shot-Object-Detection-via-Feature-Reweighting/">Few-shot Object Detection via Feature Reweighting</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-14</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1812.01866.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1812.01866.pdf</a></p>
<p>ICCV 2019的一篇论文, 做few shot detection，论文主要的贡献点是提出了一个新的few shot pipeline，主要分成三个部分： <strong>meta extractor</strong>，有点类似meta learning里面的逻辑，学习object有区分度的信息,不同的meta信息用最后feature map中不同的channel表示；<strong>reweight module</strong> 类似SE的逻辑来针对性加强不同的meta信息对于不同的输入图片,所以它输出的维度是和channle数一致的(#channel)；最后就是一个<strong>prediction module</strong>论文中用的就是yolo系列的检测框架来完成检测任务的，前面两个模块相乘之后新的feature map会作为<strong>prediction module</strong>的输入，具体的pipeline可以参考论文提供的这张图还是比较详细的：</p>
<p><img src="Few-shot-Object-Detection-via-Feature-Reweighting-截屏2020-03-0723.08.54.png" alt=""></p>
<p>那么接下来让我们看看这些模块以及最后的模型具体是怎么训练的，怎么work的，整个学习过程分为两个阶段:</p>
<ol>
<li><em>base training</em>, 用比较丰富的训练数据(base set)去学习比较好的meta extractor/reweight module/prediction module</li>
<li><em>few-shot fine-tuning</em>，用base set和novel set去一起学习，由于novel class的训练数据很少所以为了数据均衡，base class训练的时候采样和novel class保持一致，然后剩下来的过程就是<em>base training</em>阶段的就一摸一样了，只是训练时间会短很多。其实就是fine tune了。</li>
</ol>
<p>然后论文似乎主要的内容就都在这了，需要说明的几点：</p>
<ol>
<li><strong>meta extractor</strong>和<strong>prediction module</strong>虽然论文是分开说的，但应该是一体的，reweight参数修饰的feature map应该是backbone的输出，prediction module更多应该代表head部分，整个模型是基于yolov2的</li>
<li><strong>reweight module</strong>的输入是roi，也好理解毕竟是要突出具体的object，论文的做法是在rgb3个channel之上再append一个mask channel，有目标target的地方值为1其他地方值为0，在ablation里面还是解释一下直接抠图然后再append到原图和用mask这样做的差别，点上要高一点</li>
<li>论文里面还特意提了一句<strong>reweight module</strong>在inference的时候可以干掉，原因是如果我知道我将要检测哪个类别的数据，我只要把k个sample直接喂进去对weight vector取平均作为最后的weight vector，然后再inference，这个不要理解错，实际上这玩意还是去不掉的，本质和se没看到啥差别。</li>
</ol>
</div></div><a class="button-hover more" href="2020/02/28/Few-shot-Object-Detection-via-Feature-Reweighting/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/02/25/Few-shot-Adaptive-Faster-R-CNN/">Few-shot Adaptive Faster R-CNN</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-12</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Domain-Adaptation/">Domain Adaptation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Few-Shot_Adaptive_Faster_R-CNN_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Few-Shot_Adaptive_Faster_R-CNN_CVPR_2019_paper.pdf</a></p>
<p>CVPR 2019的论文，这篇比较好的地方是在abstract部分就直接阐明了目前few shot问题存在的几个根本问题，我也比较认可论文提到的这几个痛点：</p>
<ol>
<li>target domain数据量很少，从任务本身来说就比较难，从一个domain transfer到另一个domain，在如此受限的数据情况下</li>
<li>同样也因为target domain数据量很少，导致过拟合问题是一个不得不处理的事情</li>
<li>few shot for detection目前来看实际工作不是很多，因为detection任务需要兼顾object定位和分类，所以任务本身上看比较难</li>
</ol>
<p>我们具体看下这篇论文是怎么做的，整体PPL看上去还是比较复杂的:</p>
<p><img src="Few-shot-Adaptive-Faster-R-CNN-屏幕快照 2020-02-28 下午2.41.16.png" alt=""></p>
<p>从大面上论文主要分成2个部分:</p>
<ol>
<li><strong>Image Level Domain Adaptation</strong>: 上图的上半部分就是Image Level Domain Adaptation部分，这里引入了一个SP操作（Split Pooling），这个主要是作者参考已有的论文结论觉得局部的patch可以更好的表示图片的特征。做法呢也比较简单，有一个初始框(w,h),随机再生成两个shift(δ<sub>w</sub>, δ<sub>h</sub>，这是相比较图像左上角的偏移)，这样就可以组成一个明确位置和大小的框了，然后因为是基于anchor的检测框架，所有这里的(w, h)是和anchor scale/anchor ratio保持一致的，论文里是给了9个框(3个ratio x 3个scale)，这样最后可以抽出9个patch的feature。那么论文的<strong>Pair Sampling</strong>则是GAN类似的对抗学习的过程，我们把这9个patch feature按照scale大小分成larege、medium、small三大部分，然后每一个部分分别来对抗学习，对于每一个部分组成两组pair:{s,s}, {s,t}(s代表source image,t代表target image)，然后GAN要学习的就是区分开这两者。</li>
<li><strong>Instance Level Domain Adaptation</strong>: 上图的中间部分可以理解为正常的Faster RCNN逻辑，而下半部分就可以理解为这里的Instance Level Domain Adaptation，这一部分的作用倒也可以理解，adaptation的粒度不一样，这里作者提了一个概念叫<em>Instance ROI Sampling</em>. 相比较普通的ROI Sampling主要的差别就是IOU卡的很高(&gt;0.7)这样得到的ROI更加贴近Install本身，另外就是所有的Positive都会送入到下一阶段做判断，而不是一般的RPN在处理的时候会控制前背景的比例，然后至于怎么去用GAN去区分就和<em>Image Level Domain Adaptation</em>里面提到的<strong>Pair Sampling</strong>逻辑是一摸一样的了。</li>
</ol>
<p>那么针对作者提出的过拟合问题，论文了也给了一个解法:<strong>Source Model Feature Regularization Training</strong>. 想法比较直接，现在论文提出的FAFRCNN方法是针对src + target domain的，作者另外训练一个source domain的detector，然后给定相同的souce domain的数据让这两个模型提取出来的特征尽可能相近：</p>
<p><img src="Few-shot-Adaptive-Faster-R-CNN-屏幕快照 2020-03-10 下午12.57.14.png" alt=""></p>
<p>然后论文的主要内容大概就这些，整体ppl有点太复杂不太实用。</p>
</div></div><a class="button-hover more" href="2020/02/25/Few-shot-Adaptive-Faster-R-CNN/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/02/22/Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation/">Context Contrasted Feature and Gated Multi-scale Aggregation for Scene Segmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-22</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ding_Context_Contrasted_Feature_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/Ding_Context_Contrasted_Feature_CVPR_2018_paper.pdf</a></p>
<p>CVPR2018的一篇语意分割的论文，整体读起来的感觉就是很复杂！主要内容的话还是围绕分割任务中context信息/local信息的获取问题，融合了CNN/RNN整个结构看起来相当的复杂。好在最后的点看上去还可以。</p>
<p><img src="Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation-截屏2020-02-2222.14.40.png" alt=""></p>
<p>作者首先阐述了针对语意分割需要解决的问题，比如上图是论文解释问题的一张图，对于pixel a那么很显然需要把它分类为car，但是在cnn处理的过程中通常整张图的特征会被占主体的一对父子所主导，因此往往对于pixel a的分类不是很准，所以需要特别注意提取pixel a的local特征，但是只有local特征很显然也是不行的，pixel a附近的context信息是很重要的起码可以形成明显的区分度帮助模型学习，所以论文核心解决的问题就是<strong>context信息/local信息的获取/使用问题</strong>。<br>先看下论文所提方法的pipeline然后在具体说一下novelty的点,直观上看就是在一般的分割pipeline基础上增加了一个ccl模块和g+(gated sum)模块,ccl用来处理论文提出的问题关于context和local信息的使用问题，gated sum可以理解为multi scale的加权问题，是对传统的multi scale通过sum/concat方法的优化:</p>
<p><img src="Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation-截屏2020-02-2222.19.47.png" alt=""></p>
<p>论文的两个novelty点:</p>
<ol>
<li><strong>Context Contrasted Local (CCL)</strong>: 简言之就是context和local信息相减来突出local信息同时不失context的信息(CCL = F<sub>l</sub>(F, Θ<sub>l</sub>) − F<sub>c</sub>(F, Θ<sub>c</sub>))，context信息利用dilation conv来解决，local信息就是普通的conv:</li>
</ol>
<p><img src="Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation-截屏2020-02-2222.23.21.png" alt=""><br>然后为了处理multi scale的问题多个ccl block存在一个级联的过程，因为conv操作本身多次重复之后就是对feature有multi scale的作用。</p>
<ol start="2">
<li><p><strong>Gated Sum</strong>: 这个操作作者提出来主要是考虑到一般的分割任务中对multi scale的feature通常采用sum或者concat的方法进行融合，作者claim对于不同的feature有可能一部分是有害的所以需要加以甄别，那么gated sum可以理解为一个对multi feature进行加权融合的过程，实现方法很复杂…用rnn做的，有兴趣的同学可以细看论文，这里不赘述，是RNN的常规应用，只是细节的确很是繁琐:</p>
<p><img src="Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation-截屏2020-02-2222.34.40.png" alt=""><br>最后点还是不错的，在coco上mean iou能到0.357，在pascal context上可以到0.516，另外看了下ablation，gated sum的操作还是可以带来2-3个点的涨幅的。</p>
</li>
</ol>
</div></div><a class="button-hover more" href="2020/02/22/Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/02/18/Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-28</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Label-Assign/">Label Assign</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1912.02424.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1912.02424.pdf</a></p>
<p>Label Assign另一篇比较有代表性的工作, 论文主要想探究Anchor-based方法和Anchor-free方法本质的差异性(这里主要考虑one stage的RetinaNet和FCOS)，结论是<strong>正负样本取样的差异性导致的</strong>。</p>
<p>作者首先对比了一下RetinaNet和FOCS点上面的差异，加上一堆trick之后，RetinaNet AP能到37.0%，而FCOS能到37.8%，之间有0.8%的GAP，那么就其本身这两个方法现在就剩两个不一样的地方了：</p>
<ol>
<li>正负样本定义不一样，RetinaNet是anchor-based，通过卡IOU来区分正负样本，每个pixel有多个不同的anchor，FCOS是基于点的，每个点有一个anchor point(正or负)，取决于gt框的大小和不同layer定义的回归scale。</li>
<li>回归的方式不一样，FCOS从<strong>anchor point</strong>回归，RetinaNet从<strong>anchor box</strong>回归。</li>
</ol>
<p>针对上面提到的不同的两点作者也做了一个实验，RetinaNet采用FCOS的策略可以把点从37.0%涨到37.8%，而如果FCOS采用RetinaNet的策略点就会从37.8%下降到36.9%：</p>
<p><img src="Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection-屏幕快照 2020-02-18 下午4.10.40.png" alt=""></p>
<p>那么横向看上面这张表，可以方面无论是基于anchor point回归还是基于anchor box回归点是差不多的，所以作者得出结论，RetinaNet和FCOS点有差的最大原因就是<strong>正负样本取样的差异性导致的</strong>.</p>
<p>然后就是论文的核心ATSS(Adaptive Training Sample Selection)策略了,解决<em>how to define positive and negative training samples</em>的问题.<br>具体怎么做呢：</p>
<ol>
<li>对于给定的gt框，在FPN的每一个layer上找Top K个离gt框中心最近的anchor box(距离的话就用两个框中心点的L2距离衡量)作为正样本，那么假设FPN有L个layer那么对于一个gt框就有L x K个postive正样本。</li>
<li>然后计算L x K个正样本anchor与gt框的IOU，统计出均值和标注差m和v，那么由此计算出给定gt框的IOU阈值为t = m + v. 那么最后就选择IoU大于等于t的作为正样本其余作为负样本。均值m可以控制某个layer anchor的质量，那么当然大于均值的anchor要更好了，v控制gt更加适合哪个layer。</li>
<li>在实际使用的时候(论文中的伪代码)限制选中的anchor box的中心点需要在gt框内部</li>
<li>如果一个anchor框可以和多个gt框匹配那么就去IOU最大的</li>
</ol>
<p>至于atss的解释论文给了比较多的阐述，实际也是FCOS做法的insight。最后点上面还是可以的。</p>
<p>具体算法的执行步骤论文中给的伪代码看上去会更加直接:</p>
<p><img src="Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection-屏幕快照 2020-02-20 上午11.44.01.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/02/18/Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/21/CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection/">CityPersons: A Diverse Dataset for Pedestrian Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1702.05693" target="_blank" rel="noopener">https://arxiv.org/abs/1702.05693</a></p>
<p>做行人检测比较经典的数据集了，CityPersons，数据集是基于cityscapes的数据集进行refine的，CityPersons选择了CityScapes中精标注的5000张图片进行标注的(来自欧洲27个城市)，所谓的精标注就可以理解为标准的instance segmentation的标注，共有30类的类别标签，per pixel标注。CityPersons只标注CityScapes中Person和Rider两类，并将两类进一步细分为：<strong>pedestrian</strong>(walking, running or standing up), <strong>rider</strong>(riding bi- cycles or motorbikes),<strong>sitting person</strong>,and <strong>other person</strong>(with unusual postures, e.g. stretching)。</p>
<p>标注方法可以参考下图，对于Pedestrian和Rider<strong>可见框就是seg标注的最小外接矩形</strong>,<strong>全身框的话则是先标头顶到两脚中间点的直线，然后按照固定的长宽ratio 0.41拓展出框的宽度从而完成整个全身框的标注</strong>，至于遮挡的比例那就是上述两个面积的比值，而至于<strong>其他两个类别则直接用的seg的最小外接矩形没有再标注全身框</strong>，而对于假人这样的object直接标为ignore：</p>
<p><img src="CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection-屏幕快照 2020-01-21 下午3.24.25.png" alt=""></p>
<p>数据集划分：</p>
<p><img src="CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection-屏幕快照 2020-01-21 下午3.43.44.png" alt=""></p>
<p>那么在后续的论文中其实我们还会比较常见Heavy，Bare，Partial这样的划分，这个划分是Repulsion Loss那篇论文中根据CityPersons的Reseanable子集继续细分出来的，具体的可以参考Repulsion Loss这篇论文，主要是根据遮挡程度来划分的。</p>
</div></div><a class="button-hover more" href="2020/01/21/CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/15/Object-as-Distribution/">Object as Distribution</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-09</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1907.12929" target="_blank" rel="noopener">https://arxiv.org/abs/1907.12929</a></p>
<p>NIPS 2019的一篇论文，论文的内容还是很新颖的，在检测领域传统的物体表示是通过框来表示，那么这种表示有一个很大的弊端是结合后处理NMS针对crowd的场景几乎是无解的，所以作者提出一个思考用一个框来定位一个物体是不是合理的一种表示方式，因此论文中作者提出了用分布来标志物体，作者给的几个sample还是比较有意思的，涵盖了作者claim的crowd的场景：</p>
<p><img src="Object-as-Distribution-屏幕快照 2020-01-15 下午10.33.53.png" alt=""></p>
<p>具体的话论文中选择用二元正态分布来描述一个物体，公式没什么特别的就是标准的二元正态分布的定义:</p>
<p><img src="Object-as-Distribution-屏幕快照 2020-01-20 下午9.57.09.png" alt=""></p>
<p>从公式中我们也可以看到，那么网络需要学习的有5个参数: μ<sub>xi</sub>,μ<sub>yi</sub>,σ<sub>xi</sub>,σ<sub>yi</sub>,ρ<sub>i</sub>, 这五个参数刚好可以恢复出一个分布出来，比如 μ<sub>xi</sub>、μ<sub>yi</sub>两个参数起码已经恢复出物体的中心来了,σ<sub>xi</sub>、σ<sub>yi</sub> 这两个参数又直接和形状相关，所以用分布来表示物体本身就是make sense的，同时因为分布可以严格区分开物体的中心，从逻辑上讲是有利于解遮挡的场景的。那么为了让学习的目标是和位置无关的，对于具体的学习目标作者进行了转换：m−μ<sub>xi</sub>,n−μ<sub>yi</sub>,logσ<sub>xi</sub>,logσ<sub>yi</sub>,tanh<sup>−1</sup>ρi,(m,n)代表一个处于某个物体内的pixel坐标。</p>
<p>那么接下来就说说如何来优化训练模型，首先对于分布来说自然而然就想到用KL散度来监督两个分布之间的距离，同时论文中基于DeepLab的框架选择以联合训练的方式来训练模型，那么总的监督loss就是L<sub>seg</sub> + L<sub>KL</sub> + L<sub>cls</sub>, 那么在具体训练的时候为了加速训练以及节省资源，作者选择downsample之后监督而不是再resize，所以inference的时候就需要有一个upsample的过程，这个过程无法对边缘点有一个很好的判断，有可能会把边缘点误判为另一个分布，所以作者借助(Mixure Density Network<sub>暂时没读过这篇论文</sub>)以一种bagging的逻辑对于一个object预测多个distribution，所以那个L<sub>cls</sub>就来自于这。</p>
<p>另外需要注意的一个细节是L<sub>KL</sub>为：</p>
<p><img src="Object-as-Distribution-屏幕快照 2020-01-20 下午10.56.04.png" alt=""></p>
<p>m<sub>rep</sub>为mask，只有当cls值最高或者KL loss最小的那个分布mask才为1否则都为0！用作者论文中的话说，模型希望最好的distribution(KL Loss最低和目前选择的distribtion(cls值最大)越接近目标distribution！</p>
<p>其他论文就没有啥注意的了，NMS最后用KL距离就好。<br>论文另外有一个比较大的槽点就是实际上最后的点很低！！！</p>
</div></div><a class="button-hover more" href="2020/01/15/Object-as-Distribution/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/07/FCOS-Fully-Convolutional-One-Stage-Object-Detection/">FCOS: Fully Convolutional One-Stage Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-13</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classic/">Classic</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1904.01355.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.01355.pdf</a></p>
<p>2019年备受推崇的一篇anchor free的论文，最后也应该中了ICCV2019，但是对于熟悉检测领域的同学来说，看到这篇论文应该略有眼熟，这篇论文其实和Densebox应该属于一脉相承，都希望利用FCN的逻辑统一检测/分割等任务。</p>
<p>FCOS的具体想法呢是这样的，基于FPN的结构，P3 - P7得到的feature map假设分别是H<sub>i</sub> x W<sub>i</sub> x C<sub>i</sub>，那么基于H x W这么多个pixel，每个pixel都作为一个中心点去回归一个目标bbox，每个bbox的定义同样包含5个值，一个是分类score，一个是回归offset，只是这个offset的值当前这个pixel距离gt框四条边的具体(和anchor based模型一样，feature map的pixel gt的计算只需要按stride映射会原图就好)，具体的示意图如下：</p>
<p><img src="FCOS-Fully-Convolutional-One-Stage-Object-Detection-屏幕快照 2020-01-06 上午12.20.12.png" alt=""></p>
<p>几个需要注意的点:</p>
<ol>
<li>gt的生成，对于feature map上的某个pixel (x, y), 如果(x, y) 映射到原图的点(x<sup>‘</sup>, y<sup>‘</sup>)落在了某个gt框里，那么对应的offset就算(x<sup>‘</sup>, y<sup>‘</sup>)到gt框四条边的offset，分类gt也就沿用这个gt框的class标注。</li>
<li>FPN的好处一是可以fuse feature另外一个不同的layer可以针对性的回归不同scale的框，那么在FCOS中这部分是怎么做的呢，论文将P3 - P7 5个FPN层用6个值进行区间划分 ，论文中是用的区间m = [0, 64, 128, 256, 512 , ∞]，对于一个gt: (l<sup>∗</sup>, t<sup>∗</sup>, r<sup>∗</sup> ,b<sup>∗</sup>),如果满足max(l<sup>∗</sup>, t<sup>∗</sup>, r<sup>∗</sup> ,b<sup>∗</sup>) &gt; m<sub>i</sub> or max(l<sup>∗</sup>, t<sup>∗</sup>, r<sup>∗</sup> ,b<sup>∗</sup>) &lt; m<sub>i - 1</sub>，那么P<sub>3 + i</sub>就会将这个gt视为negative sample，这层不负责回归这个gt框。这么做其实会有一个问题，对于靠近gt框边缘的点有可能会落到其他layer上(也就是一个gt框内所有的pixel不一定都在一个layer里)这其实在某种程度上违背了FPN的初衷，只是在具体的实现的时候似乎可以卡中心的一些ratio来人为的干掉边缘pixel。</li>
<li>FPN另一个好处可以缓解一个pixel对目标回归的不确定性，比如下图，手拿网球拍的运动员，小的蓝色框的大部分pixel同时也落在来橙色的人体框中，这就导致一个问题，这些overlap的pixel具体需要负责去回归哪个框，通过FPN上述的分层处理可以大大缓解这个问题，论文的ablation里是有具体的数据的，感兴趣的同学可以参考原论文，那么假设在这样的情况下还是有少数不确定的pixel，那么这些pixel就负责回顾最小的那个框！</li>
</ol>
<p><img src="FCOS-Fully-Convolutional-One-Stage-Object-Detection-屏幕快照 2020-01-03 下午10.04.06.png" alt=""></p>
<p>论文的最后一段另外还提出了center-ness loss的概念，主要是想解决在具体实验中发现的FCOS会产生大量低质量的框环绕在gt周围（应该都是gt的边缘pixel产生的），因此提出了center-ness loss的概念：</p>
<p><img src="FCOS-Fully-Convolutional-One-Stage-Object-Detection-截屏2020-01-1200.31.54.png" alt=""></p>
<p>我们来看一下这个loss，l<sup>*</sup>和r<sup>*</sup>，t<sup>*</sup>和b<sup>*</sup>是两对相互关联的变量，如果某个pixel越靠近中心点(center)那么这两对值就会越接近，那么center-ness loss就会越趋向于1，如果某个pixel越远离中心点(center)，那么center-ness loss就会越趋向于0.所以可以理解为center-ness是一个度量离中心点越近的单位，inference的时候这个center-ness分支的结果会加权于score从而约束了偏离中心点的pixel，也抑制了大量低质量的框。</p>
<p>论文整体的内容应该就这些了，FCOS对后续的anchor free做法还是很有启发意义的。</p>
</div></div><a class="button-hover more" href="2020/01/07/FCOS-Fully-Convolutional-One-Stage-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/03/Focal-Loss-for-Dense-Object-Detection/">Focal Loss for Dense Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classic/">Classic</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf</a></p>
<p>重读经典系列第三篇：RetinaNet</p>
<p>ICCV 2017的Best Student Paper,也是 He Kaiming一篇很有代表性的工作，论文主要focus在One Stage Detector中样本不均衡这件事上，并且提出了<strong>Focal Loss</strong>来解决这样的问题，同时基于Focal Loss实现了一个One Stage Detector <strong>RetinaNet</strong>,可以达到Two Stage Detector的精度同时可以保持One Stage Detector的速度。</p>
<p>我们知道Two Stage Detector精度高速度慢，One Stage Detector速度快精度低，这几乎是所有人可以脱口而出的特性，那么作者认为One Stage Detector精度低的主要原因就是非常严重的class imbalance问题，基于anchor的检测器动则有10W+的anchor数目，其中Positive的anchor只有几十个，这样正负样本比几乎可以达到1:1000，大量的负样本中有很多的easy negative samples它们对模型的训练几乎无法贡献有效的信息,同时大量的这种样本本身对模型的训练也是很有害的，毕竟它们占据主要部分容易主导模型的训练。因此作者提出了Focal Loss：<br><strong>FL(p<sub>t</sub>) = −α<sub>t</sub>(1 − p<sub>t</sub>)<sup>γ</sup> log(p<sub>t</sub>)</strong><br>那么Focal Loss本身呢是来自于Cross Entropy Loss(以二分类为例):<br><img src="Focal-Loss-for-Dense-Object-Detection-屏幕快照 2020-01-03 下午8.14.46.png" alt=""><br>稍微简化一下：<br><img src="Focal-Loss-for-Dense-Object-Detection-屏幕快照 2020-01-03 下午8.15.32.png" alt=""><br>那么CE(p, y) = CE(p<sub>t</sub>) = − log(p<sub>t</sub>)<br>那么我们再来看看Focal Loss在CE Loss基础上增加的东西：</p>
<ol>
<li>−α<sub>t</sub>: 这就是简单的一个类别权重，比如可以将正样本的权重加大也是缓解class imbalance的一个选择</li>
<li>(1 − p<sub>t</sub>)<sup>γ</sup> : 这个可以理解为Focal Loss的核心吧，会整体通过模型的预测值动态的去调整loss的权重，如果某一个sample模型预测的类别是错误的那就意味着p<sub>t</sub>值会比较小（注意看p<sub>t</sub>的定义，对于每一个类别都是如此），那么Focal Loss整体就会和CE Loss差不多不会有什么影响，如果一个easy sample可以被模型很好的分类那么意味着p<sub>t</sub>值会比较大，那么Loss的权重就会变小从而优化过程中不会刻意处理，因此整个模型训练过程中都会刻意去优化hard sample。其中γ是平滑系数，论文中通过尝试γ = 2效果会比较好.</li>
</ol>
<p>至于论文中提到的RetinaNet整体其实没有什么特殊的，具体结构如下，是一个FPN的结构：<br><img src="Focal-Loss-for-Dense-Object-Detection-屏幕快照 2020-01-03 下午8.42.36.png" alt=""><br>需要注意的是:</p>
<ol>
<li>class subnet 和 box subnet参数在P3 - P7之间是共享的</li>
<li>P3 - P5通过Conv来源于C3 - C5，P6通过Conv来源于P5，P7通过Conv来源于P6</li>
<li>P3 - P7的结果会concat到一起最后一起NMS</li>
<li>为了训练的稳定性初始化有一些trick具体可以参考原论文</li>
</ol>
</div></div><a class="button-hover more" href="2020/01/03/Focal-Loss-for-Dense-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/30/ACNet-Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-Asymmetric-Convolution-Blocks/">ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1908.03930" target="_blank" rel="noopener">https://arxiv.org/abs/1908.03930</a></p>
<p>ICCV2019的一篇工作,论文提出了Asymmetric Convolutional Block这样一个新的模块，出发点是作者发现对于我们常用的dxd的卷积核来说通常对于效果影响最大的是中心十字架状的skeleton：</p>
<p><img src="ACNet-Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-Asymmetric-Convolution-Blocks-截屏2019-12-3022.14.00.png" alt=""></p>
</div></div><a class="button-hover more" href="2019/12/30/ACNet-Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-Asymmetric-Convolution-Blocks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/23/MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning/">MIC: Mining Interclass Characteristics for Improved Metric Learning </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Metric-Learning/">Metric Learning</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Roth_MIC_Mining_Interclass_Characteristics_for_Improved_Metric_Learning_ICCV_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2019/papers/Roth_MIC_Mining_Interclass_Characteristics_for_Improved_Metric_Learning_ICCV_2019_paper.pdf</a></p>
<p>ICCV2019一篇关于Metric Learning的论文，Metric Learning相关的工作是在做特征表示相关的内容，这篇论文的出发点是从object特征中去除非特征主体的特征从而能对于后续的任务更好的学习。论文不是很好懂需要仔细琢磨。</p>
<p><img src="MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning-截屏2019-12-2422.49.45.png" alt=""></p>
<p>对于一般的分类任务我们通常会把特征分成inter-class类间特征、intra-class类内特征, 那么这篇论文中类似，inter-class特征可以解释为区分特征主体的特征，比如区分狗和猫，intra-class特征可以解释为通用的一些特征，比如光照、角度等。对于一般的分类模型通常会忽略intra-class的特征直接用gt label来监督模型训练强行区分不同的类别。<br>那么本论文主要想细分这两个特征从而更精确的分类，假设输入图为img，f为抽特征的CNN，那么img的特征可以表示为f(img), 对于一般的metric learning通常会用一个encoder E在f(img)的基础上进一步对特征进行映射，从而基于E(f(img))做进一步的处理，比如计算相似度等，这个E的输出也可以理解为对这个img在高维特征空间的embedding。</p>
<p>类别一般的metric learning，本论文有两个encoder， encoder E<sub>α</sub> 编码inter-class特征，这个学习很容易，直接用gt的label监督就好，另一个encoder E<sub>β</sub> 编码intra-class特征，这部分不是很容易学习因为没有gt.那么论文中是怎么做的呢？</p>
<ol>
<li>首先对于类别n的所有的图片，计算每一张图的img的f(img)特征值，f通常是在ImageNet上pretrain过的某个model</li>
<li>计算所有f(img<sub>i</sub>)的mean和standard deviation（标准差）</li>
<li>最后利用公式Z<sub>i</sub> = (f(img<sub>i</sub>) - mean<sub>n</sub>) / StandardDeviation<sub>n</sub>进行特征转化</li>
</ol>
<p>这样做的目的是因为即使多张图片具体相同的intra-class特征，但是由于拍摄地点、拍摄行为等因素还是会导致不太一样，所以通过一个基本的转化得到的Z集合一定程度上消除了这种bias。</p>
<p>那么所有的图片经过上述的处理之后对于特征集合Z就可以聚类了，假设可以聚成K类{C<sub>1</sub>……C<sub>k</sub>},那么C集合就可以作为E<sub>β</sub>训练的label了！你可以把C集合想像成角度、光照、遮挡等一系列属性。</p>
<p>因为E<sub>β</sub>和E<sub>α</sub>共享f并且end2end训练，所以两者之前会相互影响，两者学习的特征也难免会有overlap，那么为了限制两者分别去学习inter-class类间特征和intra-class类内特征，作者也做了一些优化（ Minimizing Mutual Information）。具体可以参考下面这个公式，R是一个小网络用来将E<sub>β</sub>编码的信息映射到E<sub>α</sub>的空间上，⊙就是普通的element wise乘，r代表梯度反转，是对抗学习中常用的一种方式，和实际的需求也比较接近：</p>
<p><img src="MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning-截屏2019-12-2423.35.20.png" alt=""></p>
</div></div><a class="button-hover more" href="2019/12/23/MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/22/High-level-Semantic-Feature-Detection-A-New-Perspective-for-Pedestrian-Detection/">High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Face/">Face</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_High-Level_Semantic_Feature_Detection_A_New_Perspective_for_Pedestrian_Detection_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_High-Level_Semantic_Feature_Detection_A_New_Perspective_for_Pedestrian_Detection_CVPR_2019_paper.pdf</a><br>这是CVPR2019的行人检测论文，和Center and Scale Prediction: A Box-free Approach for Object Detection是同一篇论文，倒是后者的标题更能体现出论文所提的方法，论文主要就是异于Anchor Based的方法提出了预测中心点+框的scale的新方法来解决行人检测的问题或者严格来讲解决一般刚性物体的检测问题. 从行人检测的角度来说是一个比较新颖也比较值得去思考的方法。下图是它整体的Pipeline：</p>
<p><img src="High-level-Semantic-Feature-Detection-A-New-Perspective-for-Pedestrian-Detection-屏幕快照 2019-12-22 下午4.35.00.png" alt=""></p>
<p>论文所提的方法三言两语倒是可以说出大概，但是一些细节还是值得去琢磨的：</p>
<ol>
<li>模型结构很简单，Res50，stage 2 - 5通过deconv将高层feature map统一到一个固定的resolution然后concat到一起，其实比较类似FPN了，r倍downsample之后接一层Conv3x3然后再分别接两个Conv1x1引出两个branch，一个预测中心点，一个预测Scale，两者都是用heatmap出.</li>
<li>模型结构很简单，比较重要的就是gt是怎么生成的以及如何去监督了，首先说center的heatmap，它的大小（w x h）和downsample之后的feature map维度是一致的, 给定一个bounding box可以得到一个标注的中心，那么原则上只有这个点的gt为1，其余位置gt都为0，那么论文中为了更加符合实际场景同时也更加便于训练，用了一个比较常用的高斯分布来生成gt。而至于scale map，对于给定的中心点，它的取值是log(h),h为boundingbox的高度，这样可以通过事先定义的ratio反算出具体的框。同样的在实际生成gt的时候会围绕中心点半径2的范围内都置为log(h)其余为0；</li>
<li>Loss方面center的定位可以看为classification任务所以用ce loss，而scale的预测可以理解为regression任务所以用smoothL1 loss。</li>
<li>因为gt的监督是基于r倍downsample的所以为了更精确的预测会自然想到加一个offset分支来更精确的回归：<br>L = λ<sub>c</sub>L<sub>center</sub> + λ<sub>s</sub>L<sub>scale</sub> + λ<sub>o</sub>L<sub>offset</sub></li>
</ol>
<p>最后就是这篇论文刷的点还是比较高的，也会是做检测比较好的一个方向。</p>
</div></div><a class="button-hover more" href="2019/12/22/High-level-Semantic-Feature-Detection-A-New-Perspective-for-Pedestrian-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/22/Bi-box-Regression-for-Pedestrian-Detection-and-Occlusion-Estimation/">Bi-box Regression for Pedestrian Detection and Occlusion Estimation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf</a></p>
<p>ECCV2018的一篇行人检测的问题，论文的做法其实比较简单，就是让模型在学习全身框的同时也出可见框的结果，两者可以起到互补的作用，整体模型结构也比较简单就是在backbone之后接两个平行的业务层，一个出可见框的结果一个出全身框的结构，两个平行业务层的结构是一致的，具体的逻辑可以参考论文里的下面这张图：</p>
<p><img src="Bi-box-Regression-for-Pedestrian-Detection-and-Occlusion-Estimation-屏幕快照 2019-12-22 下午2.40.42.png" alt=""></p>
<p>那么这篇论文需要注意的更多的是一些细节：</p>
<ol>
<li>虽然模型最后是出两个平行的业务层一个出可见框的结果一个出全身框的结果，但是两者的proposal或者anchor是一摸一样的！假设目前对于同一个标注的gt，可见框为Box<sub>visible</sub>,全身框为Box<sub>full body</sub>,那么对于个给定的proposal P, 当IoU(P, Box<sub>full body</sub>) &gt; $\alpha$ &amp;&amp; IoB(P, Box<sub>visible</sub>) &gt;  $\beta$ 是P为正样本，否则P就是负样本。</li>
<li>因为两个branch用的是同一个proposal，然后基于这个proposal去分别回归offset，那么最后在后处理的时候这个proposal就只能有一个score，作者给了三个方法，第一只考虑可见分支，fc出的结果来个softmax就好，第二是只考虑全身分支，同样fc出的结果来个softmax就好，第三就是融合两个分支的结果把两个fc的结果对应相加然后再接个softmax就好，这样类似Bagging的做法来增强模型的鲁棒性。</li>
<li>论文另外一个需要注意的就是训练的细节了，全身框那个分支的回归就是和一般的Faster RCNN一样，直接只回归正样本的offset，但是对于可见框的回归是同时回归正样本和负样本的。这一点还是比较make sense的，因为论文中需要同一个proposal去同时回归全身框和可见框，所以对于那些高度遮挡的case，正样本proposal会比较少，反而是无遮挡或者轻微遮挡的case因为可见框和全身框的Overlap很大一般不会有明显的影响，因此这就会导致最后模型两个分支学的东西几乎是一样的。那么对于可见框分支对负样本怎么学习呢？作者将其学习目标定义为这个proposal的中心，所以它的gt为(0, 0, -INF, -INF)（实际使用的时候是(0, 0, -3, -3)），因为是log-space所以是-INF，这个可以看论文，这样就可以显示的将遮挡情况让模型进行感知。感觉这个做法会对FP的处理会比较有帮助。</li>
</ol>
</div></div><a class="button-hover more" href="2019/12/22/Bi-box-Regression-for-Pedestrian-Detection-and-Occlusion-Estimation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/19/SSA-CNN-Semantic-Self-Attention-CNN-for-Pedestrian-Detection/">SSA-CNN: Semantic Self-Attention CNN for Pedestrian Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-19</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Attention/">Attention</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1902.09080.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1902.09080.pdf</a></p>
<p>关于行人检测的论文，出发点的话感觉和这篇论文是比较类似的<a href="http://libanghuai.com/2019/12/08/Mask-Guided-Attention-Network-for-Occluded-Pedestrian-Detection/">Mask-Guided Attention Network for Occluded Pedestrian Detection</a>。主要想利用mask作为额外的监督来辅助提升检测的效果。两篇论文的Segmentation的标注也都是粗粒度的，MGAN是利用可见框标注作为mask，本篇论文是直接用的全身框作为mask。至于具体的做法可以参考论文里给的这张图：</p>
<p><img src="SSA-CNN-Semantic-Self-Attention-CNN-for-Pedestrian-Detection-屏幕快照 2019-12-19 下午10.04.51.png" alt=""></p>
<p>整体框架结构依然follow的Faster RCNN逻辑，论文所提的SSA-CNN主要分成两个部分：<strong>SSA-RPN</strong>和<strong>SSA-RCNN</strong>:</p>
<ol>
<li>SSA-RPN部分，从backbone（VGG16）的conv4_3和conv5_3分别引出一个segmentation分支，这个seg分支本身用全身框的mask去监督，这个分支的feature map然后再和对应的conv4_3或者conv5_3 feature map concat到一起引出正常的RPN业务分支cls和bbox，然后用预定义的target anchor去监督。有一个需要注意的地方是 <strong>只有conv5_3这个分支出的预测结果才会被接下来的SSA-RCNN使用，conv4_3这个分支出的seg类似一个外挂只做额外的监督，inference的时候不用</strong></li>
<li>SSA-RCNN部分，这个部分和传统的Faster RCNN的Fast RCNN分支处理不完全一样，一般的Fast RCNN是利用RPN的proposal经过ROIPooling得到对应的feature去做cls和reg，但对于SSA-RCNN则是直接利用SSA-RPN的proposal去原图中抠取，然后padding resize之后作为SSA-RCNN的输入，这样做的原因作者也在论文中阐明了: <strong>the pooling bins collapse if ROI’s input resolution is smaller than output</strong>，比如输入112x112映射到ROIPooling那就对应着7x7，但是CityPersons和 Caltech数据集有大量小于112 x 112大小的人，所以这个问题就会变得比较严重。其他SSA-RCNN模型部分就和SSA-RPN很像了，conv4_3和conv5_3引出的seg分支的feature map会先concat到一起，然后再和conv5_3 concat然后接入cls+reg来做行人的定位。</li>
</ol>
<p>这篇论文其他应该就没有需要注意的了。</p>
</div></div><a class="button-hover more" href="2019/12/19/SSA-CNN-Semantic-Self-Attention-CNN-for-Pedestrian-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/19/Pedestrian-Detection-with-Autoregressive-Network-Phases/">Pedestrian Detection with Autoregressive Network Phases</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Brazil_Pedestrian_Detection_With_Autoregressive_Network_Phases_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Brazil_Pedestrian_Detection_With_Autoregressive_Network_Phases_CVPR_2019_paper.pdf</a></p>
<p>不算老的论文，CVPR2019的关于行人检测的论文。</p>
<p><img src="Pedestrian-Detection-with-Autoregressive-Network-Phases-截屏2019-12-1923.59.25.png" alt=""></p>
</div></div><a class="button-hover more" href="2019/12/19/Pedestrian-Detection-with-Autoregressive-Network-Phases/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/16/RetinaFace-Single-stage-Dense-Face-Localisation-in-the-Wild/">RetinaFace: Single-stage Dense Face Localisation in the Wild</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-16</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Face/">Face</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Multi-Task/">Multi-Task</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1905.00641" target="_blank" rel="noopener">https://arxiv.org/abs/1905.00641</a></p>
<p>这篇论文就很糙快猛了，multitask(人脸检测+人脸关键点定位+人脸3D Mesh + 人脸分类)涨点：<br><img src="RetinaFace-Single-stage-Dense-Face-Localisation-in-the-Wild-屏幕快照 2019-05-05 下午5.45.42.jpg" alt=""><br>Loss也比较直接，Lcls为softmax loss，Lbox和Lpts为L1 Loss，Lpixel具体如下，R为3D图片在2D平面的投影，I为gt：<br><img src="RetinaFace-Single-stage-Dense-Face-Localisation-in-the-Wild-截屏2019-12-1622.39.25.png" alt=""><br>其他这篇论文似乎就没有什么可以描述的了…</p>
</div></div><a class="button-hover more" href="2019/12/16/RetinaFace-Single-stage-Dense-Face-Localisation-in-the-Wild/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/16/MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment/">MobileFAN: Transferring Deep Hidden Representation for Face Alignment</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-15</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Face/">Face</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Mimick/">Mimick</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1908.03839" target="_blank" rel="noopener">https://arxiv.org/abs/1908.03839</a></p>
<p>一篇人脸关键点的论文,论文的核心应该是模型蒸馏，但是单纯的基于mobilenetv2网络在多个benchmark也刷了挺不错的点，甚至是好于CVPR2018的LAB，MobileFAN就是mobilenetv2加上出heatmap直接出出来的结果，MobileFAN + KD就是加上知识蒸馏的结果，下表是WFLW上面的点,坦白讲点还是很高的：<br><img src="MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment-屏幕快照 2019-08-16 下午6.41.34.png" alt=""><br>知识蒸馏方面论文中提了两个方面：<br>一是<strong>Feature-Aligned Distillation</strong>，出发点是希望teacher和student网络学习的分布是一致的，两个网络deconv层的spatial维度是一致的差别就在于channel不一致，作者简单的用1x1把两者拉统一，Feature-Aligned Distillation和之前用大模型带小模型的做法基本是一致的，学习的时候直接mse监督两个feature map。<br><img src="MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment-Screenshot from 2019-08-15 10-54-20.png" alt=""><br>另一个是<strong>Feature-Similarity Distillation</strong>，这部分主要想让teacher和student网络表示的空间信息是一致的，比如人脸结构轮廓等，这一部分首先teacher和student网络分别去计算不同pixel之间的相似度：<br><img src="MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment-截屏2019-12-1622.24.41.png" alt=""><br>然后两个网络feature map之间计算相似度：<br><img src="MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment-截屏2019-12-1622.24.46.png" alt=""><br>总之感觉从结果上看还是挺惊艳的，模型蒸馏感觉对于小模型可以好好搞一下。</p>
</div></div><a class="button-hover more" href="2019/12/16/MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/15/Improving-Occlusion-and-Hard-Negative-Handling-for-Single-Stage-Pedestrian-Detectors/">Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Noh_Improving_Occlusion_and_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/Noh_Improving_Occlusion_and_CVPR_2018_paper.pdf</a><br>一篇行人检测的论文，论文选择在single stage架构上做文章，主要解决两个问题，一个是遮挡问题，一个是FP问题，先说遮挡问题，遮挡问题论文提出来的整体解法和zhangshifeng的那篇Occlusion-aware RCNN论文做法算是很像的了，Occlusion-aware RCNN把行人整体分成了5个part，每个part会出一个score来对feature map做attention，这篇论文就是把行人平均分成M x N(论文中去M = 6， N = 3)个part来做和Occlusion-aware RCNN类似的事情，细节的话可以参考下面这张图：</p>
<p><img src="Improving-Occlusion-and-Hard-Negative-Handling-for-Single-Stage-Pedestrian-Detectors-截屏2019-12-1523.50.06.png" alt=""></p>
<p>首先模型会出基本的reg结果，这个没有什么特殊的，这个方案特殊就是特殊在这个”Occlusion-aware Score”, 对于每一个anchor模型会去学一个M x N的矩阵，代表每一个part的confidence，这个矩阵的gt生成也比较直接，如果某一个grid中的可见行人部分(这个解决可见框标注)面积 &gt; t * area(grid)，论文中t取0.4，那么这个part的score就为1，否则就为0，因为对于每一个anchor我们只需要一个score，那么论文中也说出了两种从这个M x N矩阵中生成一个score的方法，1. argmax 2. 接几层网络以这个矩阵为输入去直接学这个score，后一个方法会考虑不同的遮挡pattern(w1….wp)会比第一个更灵活点（w也是学习的,维度也是MxN）.</p>
<p>论文另一个核心就是对FP的处理了，作者的出发点还是增强score的表现力，具体做法可以参考下面这张图，核心是这个”Grid Classifier”,对于网络中间的一些feature map比如是L1…Ln,假设对于第l个feature map，其大小为w<sub>l</sub> x h<sub>l</sub> x c<sub>l</sub>,那么通过一个1x1的conv就可以得到一个w<sub>l</sub> x h<sub>l</sub>的confidence map（至于这个confidence map如何监督也比较直接，gt就是把原来的输入图划分成w<sub>l</sub> x h<sub>l</sub> 大小，每一个小grid的gt值就是这个小grid于gt框的交面积与这个grid面积的比值），那么将这个confidence map插值到原来的输入图大小WxH，所有的1-L层feature map都做类似的事情，那么就会得到L个WxH大小的confidence map,取平均之后就可以得到最终的一个WxH的confidence map，那么对于某一个预测框，最终的score就是这个confidence map上对应预测框内部元素的均值，这个score和前面Occlusion-aware Score想乘就是最终的boundingbox的score。这一步主要是想通过多层的信息共同参与打分来增加模型的鲁棒性。</p>
<p><img src="Improving-Occlusion-and-Hard-Negative-Handling-for-Single-Stage-Pedestrian-Detectors-屏幕快照 2019-12-16 下午8.37.04.png" alt=""></p>
<p>这篇论文给我的感觉就是很复杂，trick的地方很多，分 part算score的做法也有太多类似的论文。另外论文报的点似乎没有和sota比较，在部分数据集上的结果与sota相比还是差距比较大的</p>
</div></div><a class="button-hover more" href="2019/12/15/Improving-Occlusion-and-Hard-Negative-Handling-for-Single-Stage-Pedestrian-Detectors/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/10/How-to-Write-Clean-Code/">How to Write Clean Code(Updating)</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-10</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Engineering/">Engineering</a></div></div><div class="post-content"><div class="main-content content"></div></div><a class="button-hover more" href="2019/12/10/How-to-Write-Clean-Code/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/10/Learning-Efficient-Single-stage-Pedestrian-Detectors-by-Asymptotic-Localization-Fitting/">Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-12</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf</a></p>
<p>ECCV2018行人检测的论文，在清一色的Faster RCNN系列文章中也算是一股清流了，论文用Single Shot的模型来做行人检测，也是为数不多把模型速度作为卖点和核心的论文。论文的整个核心可以理解为Cascade RCNN + RefineDet(ALF思想类似Cascade RCNN、One Stage的解决方案类似RefineDet，毕竟Cascade RCNN和RefineDet本身有些设计理念是很像的).</p>
<p><img src="Learning-Efficient-Single-stage-Pedestrian-Detectors-by-Asymptotic-Localization-Fitting-截屏2019-12-1123.53.08.png" alt=""></p>
<p>论文的核心概念是Asymptotic Localization Fitting(ALFNet), 上图示意图还是比较明显的，3个橙色feature maps是resnet/mobilenet的stage 3、4、5，绿色是直接在stage 5上接conv延伸出来的一层feature map，整体构成了类似FPN的逻辑。<br>在每个stage上都会通过CPB模块渐进式对框进行refine，这一步就和Cascade RCNN的逻辑比较像了，每个CPB都会对anchor进行一次refine，每次正负样本的IOU阈值都逐步提高，从而不断的提高框的回归精度，这里面有一个需要注意的地方就是anchor的生成只用了一个ratio: 0.41…就是CityPerson数据集的统计值，这还是有点hack数据集的意思的…</p>
</div></div><a class="button-hover more" href="2019/12/10/Learning-Efficient-Single-stage-Pedestrian-Detectors-by-Asymptotic-Localization-Fitting/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/08/Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd/">Repulsion Loss: Detecting Pedestrians in a Crowd</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL：<a href="https://zpascal.net/cvpr2018/Wang_Repulsion_Loss_Detecting_CVPR_2018_paper.pdf" target="_blank" rel="noopener">https://zpascal.net/cvpr2018/Wang_Repulsion_Loss_Detecting_CVPR_2018_paper.pdf</a></p>
<p>一篇解决行人检测遮挡场景的论文，切入点是loss，作者认为行人检测问题在遮挡场景一个比较显著的难点是boundingbox的<strong>shift</strong>问题，简言之就是由于多个人密集聚在一起的时候因为个体之间特征过于相似所以网络在预测定位的时候会产生偏移的现象，从而导致位置不够精确甚至会产生FP，论文的figure1给了一个比较简单的示意图(虚线红色框就是描述这个现象的dt框)：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-0923.54.13.png" alt=""></p>
<p>OK,那就承接上面这张图直接说一下这篇论文的核心内容，repulsion Loss:</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.03.37.png" alt=""></p>
<p>Repulsion Loss主要分成三个组成部分，L<sub>Attr</sub>, 就是一般的回归Loss比如L1、L2…之类的，毕竟为了refine定位框必要的监督信息还是需要的：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.06.10.png" alt=""></p>
<p>另一部分叫 L<sub>RepGT</sub>，这一项的目的是希望每一个proposal与相临近的gt框(不是当前proposal match的gt框)相远离，也就是对于第一张图，如果要预测紫色框，那么L<sub>RepGT</sub>就用来控制紫色框与蓝色框相远离从而缓解shift的问题，做法的话很直接，对于给定的proposal选择一个与之IoU最大的gt框然后计算Loss：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.14.24.png" alt=""></p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.14.41.png" alt=""></p>
<p>这里也引入了一个超参数来平滑loss：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.14.47.png" alt=""></p>
<p>Repulsion Loss的最后一项叫L<sub>RepBox</sub>, 其主要目的是希望对于任意match到不同gt的两个proposal之间需要尽可能的远离，然后形式上也比较类似了：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.19.12.png" alt=""></p>
<p>整体感觉这篇论文讲的点都还make sense但是都有点治标不治本</p>
</div></div><a class="button-hover more" href="2019/12/08/Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/08/Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs/">Occluded Pedestrian Detection Through Guided Attention in CNNs</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-08</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Occluded_Pedestrian_Detection_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Occluded_Pedestrian_Detection_CVPR_2018_paper.pdf</a></p>
<p>同样是解决遮挡场景的行人检测问题，同样又是attention的逻辑…但是论文中对于基于attention去做的insight还是比较有意思的。作者在做实验可视化的时候发现网络输出的feature map对于人体不同的部位分别有不同的几组channel会对其高响应，从而也就引出了论文为什么要对channel进行attention的潜在原因，论文给的示意图还是比较直接的：</p>
<p><img src="Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs-截屏2019-12-0823.07.25.png" alt=""></p>
<p>方法上还是万年不变的faster rcnn框架再辅助一个attention分支（论文中示意的attention net），attention分支会出一个fc，维度和roi pooling之后的feature map channel数保持一致：</p>
<p><img src="Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs-截屏2019-12-0823.06.03.png" alt=""></p>
<p>至于如何attention作者也提出了三个想法，第一个就是SE Net的做法，roi pooling之后的feature直接作为attention net的输入，参数的更新直接自学习（下图的第一个Self attention net），第二个做法是利用标注的可见框，做法比较trick，需要去统计数据集，作者统计了CityPerson数据集然后把行人框分成(1) fully visible; (2) upper body visi- ble; (3) left body visible; (4) right body visible四个pattern，然后attention net核心就是一个分类模型，在学习过程中一旦pattern被确认会用conv再抽一波特征（论文没有说清楚我猜是类似ROI Pooling去crop pattern对应部位的特征），示意图就是下图的第二个Visible-box attention net。第三个做法…emm感觉更trick了，作者直接拿来一个预训练好的skeleton模型，既然模型不同channel对不同part高响应那就干脆把高响应的heatmap作为guidance来指导模型训练，做法很直接，但是个人觉得这种做法最后去分析问题的时候是不是还得去优化skeleton模型……</p>
<p><img src="Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs-截屏2019-12-0823.06.54.png" alt=""></p>
<p>然后这篇论文就没什么可以介绍的了。</p>
</div></div><a class="button-hover more" href="2019/12/08/Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/08/Occlusion-aware-R-CNN-Detecting-Pedestrians-in-a-Crowd/">Occlusion-aware R-CNN - Detecting Pedestrians in a Crowd</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-08</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Shifeng_Zhang_Occlusion-aware_R-CNN_Detecting_ECCV_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/papers/Shifeng_Zhang_Occlusion-aware_R-CNN_Detecting_ECCV_2018_paper.pdf</a></p>
<p>这篇论文算是去年去参加ECCV2018 poster展台最火的一个了，论文同样是做遮挡场景的行人检测的，方法也是花样attention。</p>
<p>论文同样基于faster rcnn的框架来做行人检测，作者所提方法有两个核心，一个是<strong>Part Occlusion aware RoI Pooling Unit</strong>，作者把人的body分成5个part分别过ROI Pooling得到固定的输出大小，论文中是7x7，然后经过<strong>Occlusion process unit</strong>得到每个part的可见与否的置信度c，c再分别和对应的part feature相乘得到加权后的feature，最后连同body本身大part的feature通过element wise sum得到最后attention的feature：</p>
<p><img src="Occlusion-aware-R-CNN-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-0822.21.19.png" alt=""></p>
<p>论文所提方法的另外一个核心是<strong>Aggregation Loss</strong>，这玩意其实和<strong>Repulsion Loss</strong>这篇论文的理论算是很像的了，核心想法是希望同一个gt的proposals(anchors)之间需要尽可能的近:</p>
<p><img src="Occlusion-aware-R-CNN-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-0822.34.14.png" alt=""></p>
<p>{t<sub>1</sub><sup>*</sup>, t<sub>2</sub><sup>*</sup>…}是涉及多个proposal（anchor）的gt集合，集合长度为ρ，{Φ1, · · · , Φρ}是上述对应的gt相对应的anchors的集合，公式写的其实很直白不赘述。</p>
</div></div><a class="button-hover more" href="2019/12/08/Occlusion-aware-R-CNN-Detecting-Pedestrians-in-a-Crowd/#more">阅读全文</a></div></div><div id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="page/4/">4</a><a class="extend next" rel="next" href="page/2/"><i class="fas fa-angle-right"></i></a></div></div></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fas fa-user"></i></span><span id="busuanzi_value_site_uv"></span><span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fas fa-eye"></i></span><span id="busuanzi_value_site_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Out of Memory</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--></body></html>