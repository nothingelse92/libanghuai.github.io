<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Live and Learn"><title>Out of Memory | Live and Learn</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Out of Memory</h1><a id="logo" href="/.">Out of Memory</a><p class="description">Live and Learn</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title"><a href="/2019/11/18/Towards-Universal-Object-Detection-by-Domain-Attention/">Towards Universal Object Detection by Domain Attention</a></h1><div class="post-meta">2019-11-18</div><div class="post-content"><p><img src="" alt=""></p>
</div><p class="readmore"><a href="/2019/11/18/Towards-Universal-Object-Detection-by-Domain-Attention/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/06/28/Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm/">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</a></h1><div class="post-meta">2019-06-28</div><div class="post-content"><p>URL:<a href="https://wywu.github.io/projects/LAB/LAB.html" target="_blank" rel="noopener">https://wywu.github.io/projects/LAB/LAB.html</a><br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image002.png" alt=""><br>本论文提出了一种基于边界信息的landmark定位方法，通过回归landmark 构成的boundary可以一定程度上解决遮挡等一些问题，boundary的一般性也得以融合多个不同的landmark标注数据集进行一同训练。此外论文也贡献了包含1w张图片的数据集WFLW。</p></div><p class="readmore"><a href="/2019/06/28/Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/06/28/Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection/">Quantization Mimic: Towards Very Tiny CNN for Object Detection</a></h1><div class="post-meta">2019-06-28</div><div class="post-content"><p>URL:<a href="https://arxiv.org/pdf/1805.02152.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1805.02152.pdf</a><br>关于模型压缩的论文，论文致力于研究更加小型化的模型，论文定义“Very Tiny”为压缩模型的每一层channel数是原来模型的1/16或更小.从论文的标题也可以看出，论文提出的方法是结合目前比较常见的模型小型化方式：quantization 和 mimic。具体的模型结构在论文中给出了比较通俗的图例：<br><img src="Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection-image002.png" alt=""></p></div><p class="readmore"><a href="/2019/06/28/Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/06/24/Mimicking-Very-Efficient-Network-for-Object-Detection/">Mimicking Very Efficient Network for Object Detection</a></h1><div class="post-meta">2019-06-24</div><div class="post-content"><p>URL:<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf</a><br>这篇论文可以理解为关于模型压缩的论文，论文采取大模型来训练小模型的方法，作者claim这是第一次将mimic方法运用到物体检测领域。<br>之前mimic方法通常用在分类任务中，Mimic方法的出发点是希望大模型学习到的特征可以传递给小模型，这篇论文主要有如下的contribution：</p></div><p class="readmore"><a href="/2019/06/24/Mimicking-Very-Efficient-Network-for-Object-Detection/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/06/20/Tone-Mapping/">Tone Mapping</a></h1><div class="post-meta">2019-06-20</div><div class="post-content"><p>Tone Mapping 可以简单理解为将HDR（High Dynamic Range） 的图像映射到LDR（Low Dynamic Range）的图像中，DR的定义可以理解为同一张图片中所有像素点最大亮度和最小亮度的log的差值，RGB场景下亮度的定义为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y = 0.2126R + 0.7152G + 0.0722B</span><br></pre></td></tr></table></figure></p></div><p class="readmore"><a href="/2019/06/20/Tone-Mapping/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/06/15/Lightweight-Real-time-Makeup-Try-on-in-Mobile-Browsers-with-Tiny-CNN-Models-for-Facial-Tracking/">Lightweight Real-time Makeup Try-on in Mobile Browsers with Tiny CNN Models for Facial Tracking</a></h1><div class="post-meta">2019-06-15</div><div class="post-content"><p>URL:<a href="https://arxiv.org/abs/1906.02260" target="_blank" rel="noopener">https://arxiv.org/abs/1906.02260</a><br>一篇主要做轻量级Landmark应用的论文，整体novelty有限，主要是在MobileNetV2的基础上实现了一个轻量级的Facial Landmark模型，在iPhone XR上可以实现20ms的inference速度</p></div><p class="readmore"><a href="/2019/06/15/Lightweight-Real-time-Makeup-Try-on-in-Mobile-Browsers-with-Tiny-CNN-Models-for-Facial-Tracking/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/06/10/Improving-Landmark-Localization-with-Semi-Supervised-Learning/">Improving Landmark Localization with Semi-Supervised Learning</a></h1><div class="post-meta">2019-06-10</div><div class="post-content"><p>URL:<a href="https://arxiv.org/abs/1709.01591?context=cs" target="_blank" rel="noopener">https://arxiv.org/abs/1709.01591?context=cs</a><br>这篇论文是关于landmark检测的，作者认为目前公开的数据集中标注landmark终究量比较少，但是标注属性（比如分类）的数据集实际上有很多，因此本论文提出半监督的神经网络模型结合标注landmark的数据集和标注属性的数据集来提高landmark定位的准确性。</p></div><p class="readmore"><a href="/2019/06/10/Improving-Landmark-Localization-with-Semi-Supervised-Learning/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/06/05/Seeing-Small-Faces-from-Robust-Anchor’s-Perspective/">Seeing Small Faces from Robust Anchor’s Perspective</a></h1><div class="post-meta">2019-06-05</div><div class="post-content"><p>URL:<a href="https://arxiv.org/pdf/1802.09058.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1802.09058.pdf</a><br>这篇论文是关于anchor的设计，论文提出了一些anchor设计的策略来检测小脸。下图(a)是作者统计的传统基于anchor的模型在不同人脸大小下的recall，下图(b)则是作者将所有的人脸按大小分组，然后计算每一个组里每张人脸与anchor的最大IoU，对group中所有的max IoU取均值就是图(b)的average IoU。通过统计分析作者认为之所以对于小脸recall比较低的情况是因为小脸和初始化的anchor IoU较小，因此论文提出EMO Score来评估gt和anchor之间的联系并提出了一些anchor设计的策略。</p></div><p class="readmore"><a href="/2019/06/05/Seeing-Small-Faces-from-Robust-Anchor’s-Perspective/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/06/01/Style-Aggregated-Network-for-Facial-Landmark-Detection/">Style Aggregated Network for Facial Landmark Detection</a></h1><div class="post-meta">2019-06-01</div><div class="post-content"><p>URL:<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Style_Aggregated_Network_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Style_Aggregated_Network_CVPR_2018_paper.pdf</a><br>这篇论文主要想解决的问题是不同的图片风格对landmark定位的影响，比如论文中给出的例子对于同一张图片的不同风格（原图、灰度图、以及加入光照的图片）通过嘴部特写可以看到明显的差别。因此论文提出 Style-Aggregated Network (SAN) 整合不同的图片风格来更好的检测人脸lmk.<br><img src="Style-Aggregated-Network-for-Facial-Landmark-Detection-image002.png" alt=""></p></div><p class="readmore"><a href="/2019/06/01/Style-Aggregated-Network-for-Facial-Landmark-Detection/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/05/25/Deep-Regionlets-for-Object-Detection/">Deep Regionlets for Object Detection</a></h1><div class="post-meta">2019-05-25</div><div class="post-content"><p>URL:<a href="http://arxiv.org/abs/1712.02408" target="_blank" rel="noopener">http://arxiv.org/abs/1712.02408</a><br>论文主要将regionlet的概念引入到CNN网络结构中提出了一个新的物体检测模型，检测模型主要分为两个部分：</p></div><p class="readmore"><a href="/2019/05/25/Deep-Regionlets-for-Object-Detection/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/05/21/Regionlets-for-Generic-Object-Detection/">Regionlets for Generic Object Detection</a></h1><div class="post-meta">2019-05-21</div><div class="post-content"><p>URL:<a href="http://www.xiaoyumu.com/s/PDF/Regionlets.pdf" target="_blank" rel="noopener">http://www.xiaoyumu.com/s/PDF/Regionlets.pdf</a><br>这是一个传统的用于物体检测的方法，论文的主要贡献是提出了regionlet的概念以及基于regionlet的物体检测方法论文定义物体检测中有三个范围概念：Bounding Box、Region、Regionlet，Bounding Box就是目标候选框，Region是用于Bounding Box的特征提取，位于Bounding Box内，作者认为Region的粒度过大不足以表示局部的特征，因此在Region内部提出更小的范围Regionlet：<br><img src="Regionlets-for-Generic-Object-Detection-image002.png" alt=""></p></div><p class="readmore"><a href="/2019/05/21/Regionlets-for-Generic-Object-Detection/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/05/17/Object-Detection-in-Video-with-Spatiotemporal-Sampling-Networks/">Object Detection in Video with Spatiotemporal Sampling Networks</a></h1><div class="post-meta">2019-05-17</div><div class="post-content"><p>URL:<a href="https://arxiv.org/pdf/1803.05549.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1803.05549.pdf</a><br>这是ECCV2018 关于物体检测的一篇论文,论文主要提出一种时空采样网络STSN来提高视频中的物体检测效果。论文出发点还是整合多帧的信息来提高当前帧的检测效果，论文提出的STSN结构和FGFA比较类似，基本都会涉及到backbone网络提取特征、特征聚合等操作.</p></div><p class="readmore"><a href="/2019/05/17/Object-Detection-in-Video-with-Spatiotemporal-Sampling-Networks/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/05/12/Flow-Guided-Feature-Aggregation-for-Video-Object-Detection/">Flow-Guided Feature Aggregation for Video Object Detection </a></h1><div class="post-meta">2019-05-12</div><div class="post-content"><p>URL:<a href="https://arxiv.org/pdf/1703.10025.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.10025.pdf</a><br>这篇论文是MSRA daijifeng组的研究工作，主要提出了一种神经网络结构来进行视频中的物体识别，视频中的物体检测典型的特征就是有些帧的物体因为运动模糊、遮挡、奇怪的pose导致难以检测，但是这个帧的附近帧中可能物体是处于一个正常的状态，因此论文考虑通过整合多帧信息来提高物体检测的效果，从而提出了FGFA (flow-guided feature aggregation) 网络。</p></div><p class="readmore"><a href="/2019/05/12/Flow-Guided-Feature-Aggregation-for-Video-Object-Detection/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/05/10/Flownet-Learning-optical-flow-with-convolutional-networks/">Flownet: Learning optical flow with convolutional networks</a></h1><div class="post-meta">2019-05-10</div><div class="post-content"><p>URL:<a href="http://openaccess.thecvf.com/content_iccv_2015/papers/Dosovitskiy_FlowNet_Learning_Optical_ICCV_2015_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_iccv_2015/papers/Dosovitskiy_FlowNet_Learning_Optical_ICCV_2015_paper.pdf</a><br>这是一篇ICCV2015的论文，主要是利用CNN来进行光流的计算，在视频的相关应用中通常会涉及到光流的概念，可参考Stanford CS131了解相关内容。论文主要提出了两种CNN结构：</p></div><p class="readmore"><a href="/2019/05/10/Flownet-Learning-optical-flow-with-convolutional-networks/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/05/01/Pose-Invariant-3D-Face-Alignment/">Pose-Invariant 3D Face Alignment </a></h1><div class="post-meta">2019-05-01</div><div class="post-content"><p>URL:<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Jourabloo_Pose-Invariant_3D_Face_ICCV_2015_paper.pdf" target="_blank" rel="noopener">https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Jourabloo_Pose-Invariant_3D_Face_ICCV_2015_paper.pdf</a><br>利用3D模型来处理大Pose Landmark问题的一篇文章，整体还是follow 3DMM的那一套pipeline，只是因为这篇论文出的相对比较早，对于参数的学习是利用一般的回归模型来级联回归，整体在不同Pose下还是有一定效果的。不过和整个pipeline的设计有关速度比较慢。</p></div><p class="readmore"><a href="/2019/05/01/Pose-Invariant-3D-Face-Alignment/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/04/20/Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression/">Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression</a></h1><div class="post-meta">2019-04-20</div><div class="post-content"><p>URL:<a href="https://arxiv.org/abs/1904.07399" target="_blank" rel="noopener">https://arxiv.org/abs/1904.07399</a><br>最新挂出来的关于人脸landmark的论文，可以理解为整合wing loss + look at boundary进行的优化，wing loss在cvpr2018提出的时候是直接应用在回归landmark点坐标，作者想将其应用到heatmap出点的逻辑上因而提出了adaptive wing loss，同时在网络中引入boundary的信息来辅助模型的训练，在benchmark上的表现还是很不错的，部分指标可以和wing loss、lab拉开比较大的差距。</p></div><p class="readmore"><a href="/2019/04/20/Adaptive-Wing-Loss-for-Robust-Face-Alignment-via-Heatmap-Regression/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/04/10/Spatial-Transformer-Networks/">Spatial Transformer Networks</a></h1><div class="post-meta">2019-04-10</div><div class="post-content"><p>URL:<a href="https://arxiv.org/pdf/1506.02025.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.02025.pdf</a><br>这是一篇NIPS 2015的文章，主要提出了STN网络结构直接的赋予了网络对于各种变换的不变性。<br>STN网络主要分为三个部分：</p></div><p class="readmore"><a href="/2019/04/10/Spatial-Transformer-Networks/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/04/04/Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection/">Semantic Alignment: Finding Semantically Consistent Ground-truth for Facial Landmark Detection</a></h1><div class="post-meta">2019-04-04</div><div class="post-content"><p>URL:<a href="https://arxiv.org/abs/1903.10661" target="_blank" rel="noopener">https://arxiv.org/abs/1903.10661</a><br>CVPR2019最新挂出来的一篇关于人脸landmark的论文，论文的出发点是觉得目前landmark定位精度受限于部分标注点”语意”模糊有关,比如说脸部轮廓点或者眼部轮廓点不像眼球、鼻尖这些点有明确的语意定义，因此标注引入的误差就相对影响比较大。所以作者从这方面入手在模型每次迭代的时候去寻找这样一个“真正”的gt来监督网络的训练，此外为了来修正一些偏移比较厉害的点，作者又引入了一个子网络来refine整体的landmark。这篇论文整体个人感觉很有意义。</p></div><p class="readmore"><a href="/2019/04/04/Semantic-Alignment-Finding-Semantically-Consistent-Ground-truth-for-Facial-Landmark-Detection/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/03/25/Object-Detection-based-on-Region-Decomposition-and-Assembly/">Object Detection based on Region Decomposition and Assembly</a></h1><div class="post-meta">2019-03-25</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1901.08225" target="_blank" rel="noopener">https://arxiv.org/abs/1901.08225</a><br>AAAI2019的一篇关于检测的论文，论文主要的出发点是想解决遮挡场景下的物体检测问题，整个逻辑基于Faster RCNN的框架来做，主要思路是先把proposal分part分别来提取特征然后再通过一定的方法将其merge到一起来突出可见部分的特征，从而得到更可信的信息。</p></div><p class="readmore"><a href="/2019/03/25/Object-Detection-based-on-Region-Decomposition-and-Assembly/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/03/19/Mask-Scoring-R-CNN/">Mask Scoring R-CNN</a></h1><div class="post-meta">2019-03-19</div><div class="post-content"><p>URL:<a href="https://arxiv.org/abs/1903.00241" target="_blank" rel="noopener">https://arxiv.org/abs/1903.00241</a><br>论文的出发点很直观，就是为了优化在目前的一些instance segmentation的方法中用classification score来标注一个mask的质量，这个其实很显然和实际应用场景是完全不一致的，比如论文中给出的例子：<br><img src="Mask-Scoring-R-CNN-屏幕快照 2019-03-19 下午10.56.07.png" alt=""></p></div><p class="readmore"><a href="/2019/03/19/Mask-Scoring-R-CNN/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/03/18/Region-Proposal-by-Guided-Anchoring/">Region Proposal by Guided Anchoring</a></h1><div class="post-meta">2019-03-18</div><div class="post-content"><p>URL:<a href="https://arxiv.org/abs/1901.03278" target="_blank" rel="noopener">https://arxiv.org/abs/1901.03278</a><br>CVPR2019的一篇对anchor进行优化的论文，主要将原来需要预先定义的anchor改成直接end2end学习anchor位置和size。首先anchor的定义通常为(x, y, w, h) (x, y为中心点)，formulate一下：<br><img src="Region-Proposal-by-Guided-Anchoring-屏幕快照 2019-03-18 下午10.33.58.png" alt=""><br>因此本文所提的guided anchoring利用两个branch分别预测anchor的位置和w、h：<br><img src="Region-Proposal-by-Guided-Anchoring-屏幕快照 2019-03-18 下午10.28.29.png" alt=""></p></div><p class="readmore"><a href="/2019/03/18/Region-Proposal-by-Guided-Anchoring/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/03/15/Grid-RCNN/">Grid RCNN</a></h1><div class="post-meta">2019-03-15</div><div class="post-content"><p>URL:<a href="https://arxiv.org/abs/1811.12030" target="_blank" rel="noopener">https://arxiv.org/abs/1811.12030</a><br>CVPR2018的一篇论文，从某种程度上来说是借鉴Bottom Up的方法来优化目前检测方面的一些问题，主要出发点还是希望检测器出的框能尽可能的准，所以相比较一般的检测器直接出四维的坐标信息，Grid RCNN则是出9个点，用9个点的信息来表示一个bbox。<br>具体的PipeLine如下：<br><img src="Grid-RCNN-屏幕快照 2019-03-16 下午3.04.35.png" alt=""></p></div><p class="readmore"><a href="/2019/03/15/Grid-RCNN/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/03/03/Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks/">Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks</a></h1><div class="post-meta">2019-03-03</div><div class="post-content"><p>URL:<a href="https://arxiv.org/abs/1711.06753" target="_blank" rel="noopener">https://arxiv.org/abs/1711.06753</a><br>CVPR2018一篇关于人脸Landmark的论文，这篇论文主要是关于人脸关键点的定位，因为论文的重点是loss function和data augmentation所以论文所实验的模型结构是比较简单的CNN结构来实验：<br><img src="Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks-image002.png" alt=""></p></div><p class="readmore"><a href="/2019/03/03/Wing-Loss-for-Robust-Facial-Landmark-Localisation-with-Convolutional-Neural-Networks/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/03/03/Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression/">Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression</a></h1><div class="post-meta">2019-03-03</div><div class="post-content"><p>URL：<a href="https://arxiv.org/abs/1902.09630" target="_blank" rel="noopener">https://arxiv.org/abs/1902.09630</a><br>这是CVPR2019的一篇论文。本论文主要提出了GIoU的概念来优化IoU在评估或者IoU Loss在训练中的一些问题<br>这是利用Ln-Loss来优化bbox回归问题的常见bug，不同的overlap程度在Ln-loss看来都一样，但是实际上对于IoU或者GIoU确实不一样的，很显然后者更合理：</p></div><p class="readmore"><a href="/2019/03/03/Generalized-Intersection-over-Union-A-Metric-and-A-Loss-for-Bounding-Box-Regression/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/03/02/Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points/">Bottom-up Object Detection by Grouping Extreme and Center Points</a></h1><div class="post-meta">2019-03-02</div><div class="post-content"><p>URL:<a href="https://arxiv.org/pdf/1901.08043.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.08043.pdf</a><br>Codebase:<a href="https://github.com/xingyizhou/ExtremeNet" target="_blank" rel="noopener">https://github.com/xingyizhou/ExtremeNet</a><br>一篇比较有意义的论文，主要是用bottom up的方法来做检测的问题，整个工作是基于ECCV2018的cornernet来做的，对于一个框ExtremeNet会出5个点，四个边界点和一个中心点，中心点主要是用来做group。下图是标注的示例图：</p></div><p class="readmore"><a href="/2019/03/02/Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/03/02/PFLD-A-Practical-Facial-Landmark-Detector/">PFLD: A Practical Facial Landmark Detector</a></h1><div class="post-meta">2019-03-02</div><div class="post-content"><p>URL: <a href="https://arxiv.org/abs/1902.10859" target="_blank" rel="noopener">https://arxiv.org/abs/1902.10859</a><br>相关主页：<a href="https://sites.google.com/view/xjguo/fld" target="_blank" rel="noopener">https://sites.google.com/view/xjguo/fld</a><br>这两天刚挂出来的关于landmark的论文，论文中对300w数据集报的点是要比LAB、SAN等CVPR2018论文的点是要高的，在845手机上也可以达到140FPS的速度，论文所提方法主要在设计加权的loss，比如考虑人脸的yaw、pitch、roll等信息来加权loss。</p></div><p class="readmore"><a href="/2019/03/02/PFLD-A-Practical-Facial-Landmark-Detector/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/02/28/S3FD-Single-Shot-Scale-invariant-Face-Detector/">S3FD: Single Shot Scale-invariant Face Detector</a></h1><div class="post-meta">2019-02-28</div><div class="post-content"><p>URL: <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_S3FD_Single_Shot_ICCV_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_S3FD_Single_Shot_ICCV_2017_paper.pdf</a><br>ICCV2017的一篇论文，主要研究小人脸的检测，作者针对目前基于anchor的检测器对小人脸表现不好的现象分析了几个可能的原因，并针对性的提出了具体的解决方法，比如新的anchor匹配策略、max out backgroud label等方法。</p></div><p class="readmore"><a href="/2019/02/28/S3FD-Single-Shot-Scale-invariant-Face-Detector/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/02/22/谈一谈模型量化/">谈一谈模型量化</a></h1><div class="post-meta">2019-02-22</div><div class="post-content"><p>说到量化，首先需要弄清楚我们为什么需要量化？为什么可以量化？<br>对于为什么需要量化可以从两个实际的需求来说，第一，对于一般float32存储的模型来说通常硬盘占用相对会比较大，那么在现有的一些端上比如手机端对模型的内存占用是有很明显的限制的，将float32的模型转化为int8的模型就可以带来75%存储的节约，因此在某些场景下模型的量化也是一个必然的选择。第二，从模型的运算效率上来说，int8只有一个字节，float32有四个字节，所以int8参数的获取只需要25%float32的内存带宽，因此可以更好地使用缓存，同时每个时钟周期执行更多操作的SIMD操作。另外DSP等计算单元本身对int8计算很友好，int8模型的使用可以提高速度同时可以降低功耗。<br>至于为什么可以量化，可以这么理解，我们知道CNN网络在对图片进行各种处理的时候比如识别、比如分类、比如检测，都有比较强的鲁棒性，对噪声的兼容性相对比较高。那么落到量化这件事情上来说，可能带来的问题就是精度的损失，那么其实我们可以把这一部分精度的损失理解为外界带给网络的<strong>“噪声”</strong>,所以结合CNN在具体任务的表现我们有理由相信量化操作给网络带来的精度影响不会那么严重。当然了这算是比较理想的状态了，其实在具体的实验过程中对于目前已经做的比较轻量的网络，量化对模型的精度还是会有比较大的影响的。所以量化通常也是根据具体的任务需要做的一个选择。</p></div><p class="readmore"><a href="/2019/02/22/谈一谈模型量化/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/02/03/You-Only-Look-Once-Unified-Real-Time-Object-Detection/">You Only Look Once: Unified, Real-Time Object Detection</a></h1><div class="post-meta">2019-02-03</div><div class="post-content"><p>URL: <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" target="_blank" rel="noopener">https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf</a><br><strong>重读经典系列第一篇：YOLO</strong><br>YOLO系列是另一个经典的Single Stage的检测器：<br><img src="You-Only-Look-Once-Unified-Real-Time-Object-Detection-屏幕快照 2019-02-03 下午10.49.49.png" alt=""></p></div><p class="readmore"><a href="/2019/02/03/You-Only-Look-Once-Unified-Real-Time-Object-Detection/">阅读全文</a></p></div><div class="post"><h1 class="post-title"><a href="/2019/02/03/SSD-Single-Shot-MultiBox-Detector/">SSD: Single Shot MultiBox Detector</a></h1><div class="post-meta">2019-02-03</div><div class="post-content"><p>URL: <a href="https://arxiv.org/pdf/1512.02325.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1512.02325.pdf</a><br>Code: <a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">https://github.com/weiliu89/caffe/tree/ssd</a><br><strong>重读经典系列第一篇：SSD</strong><br>SSD是很经典的Single Stage检测网络，至今仍有很多的工作是基于SSD在改进。<br>不同于典型的Two stage检测网络将proposal的生成放在RPN网络来做，然后后续的网络branch基于RPN的结果进行Refine，SSD将Proposal直接放到网络后面的feature map上来做，基本逻辑如下图，实际和FasterRCNN网络生成anchor的逻辑是一模一样的：<br><img src="SSD-Single-Shot-MultiBox-Detector-屏幕快照 2019-02-03 下午10.33.52.png" alt=""></p></div><p class="readmore"><a href="/2019/02/03/SSD-Single-Shot-MultiBox-Detector/">阅读全文</a></p></div><nav class="page-navigator"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">下一页</a></nav></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="http://yoursite.com"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Engineering/">Engineering</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper-Reading/">Paper Reading</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Mobile/" style="font-size: 15px;">Mobile</a> <a href="/tags/Platform/" style="font-size: 15px;">Platform</a> <a href="/tags/Hardware/" style="font-size: 15px;">Hardware</a> <a href="/tags/Bottom-Up/" style="font-size: 15px;">Bottom_Up</a> <a href="/tags/Detection/" style="font-size: 15px;">Detection</a> <a href="/tags/Landmark/" style="font-size: 15px;">Landmark</a> <a href="/tags/Pose/" style="font-size: 15px;">Pose</a> <a href="/tags/BottomUp/" style="font-size: 15px;">BottomUp</a> <a href="/tags/KeyPoint/" style="font-size: 15px;">KeyPoint</a> <a href="/tags/Tools/" style="font-size: 15px;">Tools</a> <a href="/tags/Engineering/" style="font-size: 15px;">Engineering</a> <a href="/tags/Loss/" style="font-size: 15px;">Loss</a> <a href="/tags/Mutual-learning/" style="font-size: 15px;">Mutual learning</a> <a href="/tags/Classification/" style="font-size: 15px;">Classification</a> <a href="/tags/Segmentation/" style="font-size: 15px;">Segmentation</a> <a href="/tags/Regularization/" style="font-size: 15px;">Regularization</a> <a href="/tags/Face/" style="font-size: 15px;">Face</a> <a href="/tags/VID/" style="font-size: 15px;">VID</a> <a href="/tags/SOT/" style="font-size: 15px;">SOT</a> <a href="/tags/Track/" style="font-size: 15px;">Track</a> <a href="/tags/Tracking/" style="font-size: 15px;">Tracking</a> <a href="/tags/Mimick/" style="font-size: 15px;">Mimick</a> <a href="/tags/3D/" style="font-size: 15px;">3D</a> <a href="/tags/Template/" style="font-size: 15px;">Template</a> <a href="/tags/Bottom-UP/" style="font-size: 15px;">Bottom UP</a> <a href="/tags/Learning-Strategy/" style="font-size: 15px;">Learning Strategy</a> <a href="/tags/DeepLab/" style="font-size: 15px;">DeepLab</a> <a href="/tags/Classic/" style="font-size: 15px;">Classic</a> <a href="/tags/Basic/" style="font-size: 15px;">Basic</a> <a href="/tags/Quantization/" style="font-size: 15px;">Quantization</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/11/18/Towards-Universal-Object-Detection-by-Domain-Attention/">Towards Universal Object Detection by Domain Attention</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/28/Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm/">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/28/Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection/">Quantization Mimic: Towards Very Tiny CNN for Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/24/Mimicking-Very-Efficient-Network-for-Object-Detection/">Mimicking Very Efficient Network for Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/20/Tone-Mapping/">Tone Mapping</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/15/Lightweight-Real-time-Makeup-Try-on-in-Mobile-Browsers-with-Tiny-CNN-Models-for-Facial-Tracking/">Lightweight Real-time Makeup Try-on in Mobile Browsers with Tiny CNN Models for Facial Tracking</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/10/Improving-Landmark-Localization-with-Semi-Supervised-Learning/">Improving Landmark Localization with Semi-Supervised Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/05/Seeing-Small-Faces-from-Robust-Anchor’s-Perspective/">Seeing Small Faces from Robust Anchor’s Perspective</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/01/Style-Aggregated-Network-for-Facial-Landmark-Detection/">Style Aggregated Network for Facial Landmark Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/25/Deep-Regionlets-for-Object-Detection/">Deep Regionlets for Object Detection</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Out of Memory.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>