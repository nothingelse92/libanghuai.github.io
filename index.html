<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Live and Learn"><meta name="keywords" content=""><meta name="author" content="Out of Memory,undefined"><meta name="copyright" content="Out of Memory"><title>Live and Learn【Out of Memory】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="author-info"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Out of Memory</div><div class="author-info-description">Live and Learn</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/libanghuai" target="_blank">GitHub<i class="icon-dot bg-color3"></i></a><a class="links-button button-hover" href="mailto:libanghuai@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color10"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1185719433&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color5"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="archives"><span class="pull-top">日志</span><span class="pull-bottom">123</span></a><a class="author-info-articles-tags article-meta" href="tags"><span class="pull-top">标签</span><span class="pull-bottom">39</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Out of Memory</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><div id="recent-posts"><!-- each post in page.posts.sort('date', -1).limit(10).toArray()--><!-- config中配置按照什么排序--><div class="recent-post-item"><a class="post-title" href="2020/05/24/Deep-Representation-Learning-on-Long-tailed-Data-A-Learnable-Embedding-Augmentation-Perspective/">Deep Representation Learning on Long-tailed Data: A Learnable Embedding Augmentation Perspective</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-05-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Metric-Learning/">Metric Learning</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Long-Tail/">Long Tail</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/2002.10826" target="_blank" rel="noopener">https://arxiv.org/abs/2002.10826</a></p>
<p>CVPR2020 Metric Learning 解决Long Tail的问题，motivation来自head class和tail class在高维空间的分布统计:</p>
<p><img src="Deep-Representation-Learning-on-Long-tailed-Data-A-Learnable-Embedding-Augmentation-Perspective-屏幕快照 2020-05-24 下午3.32.10.png" alt=""></p>
<p>这一张图中:</p>
<ol>
<li>(a) DukeMTMC-reID数据集中选择top 8的head class，可以看到intra-class和inter-class差距都比较明显，所以在分类的时候会比较容易的区分开来，这也就是head class表现比较好的根本原因.</li>
<li>(b) (a)中的8的head class随机取3个class缩减他们的样本数量形成一个head + tail class的区分，可以发现tail class的样本都挤在一个很窄的范围内，intra-class diversity很小，因为本身样本很少所以可以学习的区分度当然也很小，同样也因为样本比较少比较难拉开和其他样本的差异.</li>
<li>(c) 加上论文中提出来的feature cloud达到的效果</li>
</ol>
<p>论文的想法，把head class的分布(variance)迁移到tail class从而实现intra-class diversity能变大一点:</p>
<p><img src="Deep-Representation-Learning-on-Long-tailed-Data-A-Learnable-Embedding-Augmentation-Perspective-屏幕快照 2020-05-24 下午3.38.13.png" alt=""></p>
<p>看一下具体做法:</p>
<ol>
<li><p><strong>Intra-class Angular Distribution</strong>： 因为我们需要做的是增加tail class的intra-class diversity, 所以需要定义一下intra-class diversity(用类别i的所有样本到类别中心的角度分布的variance来衡量intra-class diversity):</p>
<p> <img src="Deep-Representation-Learning-on-Long-tailed-Data-A-Learnable-Embedding-Augmentation-Perspective-屏幕快照 2020-05-24 下午3.41.37.png" alt=""></p>
<p> f是类别i的某一个样本的feature，c为类别i的类别中心feature，计算的时候用EMA在训练过程中动态改变:</p>
<p> <img src="Deep-Representation-Learning-on-Long-tailed-Data-A-Learnable-Embedding-Augmentation-Perspective-屏幕快照 2020-05-24 下午3.44.02.png" alt=""></p>
</li>
<li><p><strong>Featue Cloud</strong>: 给定一个tail class的样本feature f, 用head class的angular distribution生成f周围的一群虚拟的feature cloud, head class的angular distribution的计算是不区分具体是head class中的哪一个而是直接把所有的head class取平均就好了:</p>
<p> <img src="Deep-Representation-Learning-on-Long-tailed-Data-A-Learnable-Embedding-Augmentation-Perspective-屏幕快照 2020-05-24 下午3.49.25.png" alt=""></p>
</li>
</ol>
<p><strong>那么假设head class的angular distribution服从N(µ<sub>h</sub> and σ<sup>2</sup><sub>h</sub>), tail class的angular distribution服从N(µ<sub>t</sub> and σ<sup>2</sup><sub>t</sub>), 那么为了达到把head class的分布迁移过来，也就是把σ<sup>2</sup><sub>h</sub>迁移过来，在构建的feature cloud的时候需要保证，对于给定的一个tail class样本的feature x与随机从x的feature cloud中挑选的feature f，两者之间的角度α<sub>x</sub>需要服从N(0, σ<sup>2</sup><sub>h</sub> - σ<sup>2</sup><sub>t</sub>),这是两个独立同分布的性质.</strong></p>
<p>所以最后在训练的时候cosface和arcface的loss就可以更新为, 多出来一个α<sub>y</sub>就是一个<strong>类似data augmentation的存在来增加样本以此增加diversity</strong>:</p>
<p><img src="Deep-Representation-Learning-on-Long-tailed-Data-A-Learnable-Embedding-Augmentation-Perspective-屏幕快照 2020-05-24 下午3.55.53.png" alt=""></p>
<p>那么关于head class/tail class的划分，论文中给了两种方式，一个就是通过卡样本数目T，如果某一个类别的样本数超过T就是head class否则就是tail class，另一种方式就是直接用样本数来加权得到整个数据集的variance, 那么很显然在计算过程中那些样本数比较多的类别会贡献更多的variance，所以再挨个算一遍每个类别的variance，如果单个类别的variance小于总体的variance说明就是tail class:</p>
<p><img src="Deep-Representation-Learning-on-Long-tailed-Data-A-Learnable-Embedding-Augmentation-Perspective-屏幕快照 2020-05-24 下午3.58.46.png" alt=""></p>
<p>贴一个点, 能到sota，优势不大:</p>
<p><img src="Deep-Representation-Learning-on-Long-tailed-Data-A-Learnable-Embedding-Augmentation-Perspective-屏幕快照 2020-05-24 下午4.00.55.png" alt=""></p>
<p>方法大体就是这个样子，感觉想法挺好的</p>
</div></div><a class="button-hover more" href="2020/05/24/Deep-Representation-Learning-on-Long-tailed-Data-A-Learnable-Embedding-Augmentation-Perspective/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/05/24/Cross-domain-Object-Detection-through-Coarse-to-Fine-Feature-Adaptation/">Cross-domain Object Detection through Coarse-to-Fine Feature Adaptation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-05-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Domain-Adaptation/">Domain Adaptation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/2003.10275" target="_blank" rel="noopener">https://arxiv.org/abs/2003.10275</a></p>
<p>做DA场景的物体检测，同样还是分image level和instance level，image level的改进是不再关注全图而是只关注图片的前景区域，instance level的改进是参考few shot领域的protonet维护一些类别中心，然后监督不同domain的同一类的类别中心. 整体框架没有太大变化。</p>
<p><img src="Cross-domain-Object-Detection-through-Coarse-to-Fine-Feature-Adaptation-0287be73879cbd725975da6eddc81f07dc744fd8.png" alt=""></p>
<p>Coarse Feature Adaptation: ART Block, 在Image Level做DA，不同的地方是把注意力放在全图的前景区域而不是全图所有区域, 一般Image Level的DA是做一个Domain Classifier用对抗学习的方式去去除Domain相关的信息，ART则是用RPN的feature map得到一个前景的mask拍到loss上做attention, mask的生成比较简单用RPN的feature取平均得到hxwx1的mask，然后去mask的平均mean然后突出mask上大于mean的pixel:</p>
<p><img src="Cross-domain-Object-Detection-through-Coarse-to-Fine-Feature-Adaptation-a236eab386eb0d3781983759910eee424033e6ff.png" alt=""></p>
<p>ART会作用于不同的level的feature map，所以scale不一致的情况下直接up sample就好:</p>
<p><img src="Cross-domain-Object-Detection-through-Coarse-to-Fine-Feature-Adaptation-c614cc0ea94f981321bc8392be6a99a04ed0856d.png" alt=""></p>
<p>Fine Feature Adaptation: PSA Block, 参考Few Shot任务里的ProtoNet，对于每一类的数据都维护一个类别中心, Fr是Fast RCNN那一支的第二个FC，对于source/target domain处理方式略有不同，对于source domain因为有标注所以直接上gt框去获得类别表示，target domain因为没有标注所以就用的dt框去获得类别表示:</p>
<p><img src="Cross-domain-Object-Detection-through-Coarse-to-Fine-Feature-Adaptation-屏幕快照 2020-05-24 下午1.43.40.png" alt=""></p>
<p>然后考虑到det任务单卡的batch size比较小，所以作者维护了一个全局proto, 每一个mini batch下都会动态的去更新这个类别中心(EMA):</p>
<p><img src="Cross-domain-Object-Detection-through-Coarse-to-Fine-Feature-Adaptation-屏幕快照 2020-05-24 下午1.44.22.png" alt=""></p>
<p>贴一下Foggy Cityscapes上的点，似乎还是比较高的:</p>
<p><img src="Cross-domain-Object-Detection-through-Coarse-to-Fine-Feature-Adaptation-32b6fa820b25d7ec2a9e967f9beee22eb4a018b2.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/05/24/Cross-domain-Object-Detection-through-Coarse-to-Fine-Feature-Adaptation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/05/24/Distance-IoU-Loss-Faster-and-Better-Learning-for-Bounding-Box-Regression/">Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-05-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1911.08287" target="_blank" rel="noopener">https://arxiv.org/abs/1911.08287</a></p>
<p>AAAI 2020的一篇论文，算是对IoU系列的一个总结同时提出了新的基于IoU的Loss, DIoU和CIoU</p>
<p>论文上来先分析一下目前IoU系列Loss的缺点:</p>
<ol>
<li>IoU : 对non-overlap的框不友好，无法处理这一类的问题</li>
<li><p>GIoU: GIoU是在IoU的基础上增加了最小外接矩形的惩罚, C是最小外接矩形的面积, B是预测框，B<sup>gt</sup>就是对应的gt框了, 所以解决了IoU对non-overlap框的忽略:</p>
<p> <img src="Distance-IoU-Loss-Faster-and-Better-Learning-for-Bounding-Box-Regression-屏幕快照 2020-05-24 下午1.26.08.png" alt=""></p>
<p> GIoU的两个问题:</p>
<p> a. GIoU做监督的情况下模型倾向于先出一个比较大的BoundingBox，这样两个框的non-overlap的概率会很小，然后再走IoU Loss的逻辑一步步优化，这样的问题会导致GIoU模型训练收敛速度变慢</p>
<p> <img src="Distance-IoU-Loss-Faster-and-Better-Learning-for-Bounding-Box-Regression-屏幕快照 2020-05-24 下午1.29.11.png" alt=""></p>
<p> b. GIoU对于包含关系的两个框无法处理，因为会退化到IoU的情况，比如下图，这种情况是不合理的:</p>
<p> <img src="Distance-IoU-Loss-Faster-and-Better-Learning-for-Bounding-Box-Regression-屏幕快照 2020-05-24 下午1.30.22.png" alt=""></p>
</li>
</ol>
<p>所以作者在论文里提出了DIoU来处理GIoU的缺点，解法也很简单，引入两个框中心点之间的距离来处理GIoU的缺点, b代表框的中心点，c代表最小外接矩形的对角线长度，ρ就代表欧式距离:</p>
<p><img src="Distance-IoU-Loss-Faster-and-Better-Learning-for-Bounding-Box-Regression-屏幕快照 2020-05-24 下午1.31.32.png" alt=""></p>
<p>示意图:</p>
<p><img src="Distance-IoU-Loss-Faster-and-Better-Learning-for-Bounding-Box-Regression-屏幕快照 2020-05-24 下午1.33.00.png" alt=""></p>
<p>在DIoU的基础上作者又提出了CIoU(Complete IoU Loss), 综合考虑两个框的Overlap/center/ratio，所以在DIoU的基础上又加上了ratio的约束:</p>
<p><img src="Distance-IoU-Loss-Faster-and-Better-Learning-for-Bounding-Box-Regression-屏幕快照 2020-05-24 下午1.34.47.png" alt=""></p>
<p>v是对角度的约束，希望dt和gt框w/h比也趋于一致:</p>
<p><img src="Distance-IoU-Loss-Faster-and-Better-Learning-for-Bounding-Box-Regression-屏幕快照 2020-05-24 下午1.35.06.png" alt=""></p>
<p>α 是一个trade off的超参数,原则很简单，框的回归最重要，如果两个框的overlap足够大再去处理一下ratio的问题:</p>
<p><img src="Distance-IoU-Loss-Faster-and-Better-Learning-for-Bounding-Box-Regression-屏幕快照 2020-05-24 下午1.35.59.png" alt=""></p>
<p>最后再看下作者做的几组模拟实验，结果也验证了作者的一些解释:</p>
<p><img src="Distance-IoU-Loss-Faster-and-Better-Learning-for-Bounding-Box-Regression-屏幕快照 2020-05-24 下午1.37.21.png" alt=""></p>
<p><img src="Distance-IoU-Loss-Faster-and-Better-Learning-for-Bounding-Box-Regression-屏幕快照 2020-05-24 下午1.37.39.png" alt=""></p>
<p>点上面也是还可以的:</p>
<p><img src="Distance-IoU-Loss-Faster-and-Better-Learning-for-Bounding-Box-Regression-屏幕快照 2020-05-24 下午1.38.22.png" alt=""></p>
<p>D代表用DIoU的计算方式取代IoU的计算方式去处理NMS问题，不过看上去这个小trick的提升很有限</p>
</div></div><a class="button-hover more" href="2020/05/24/Distance-IoU-Loss-Faster-and-Better-Learning-for-Bounding-Box-Regression/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/05/13/Meta-RCNN-Meta-learning-for-few-shot-object-detection/">Meta-RCNN: Meta learning for few-shot object detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-05-13</time></div><div class="post-content"><div class="main-content content"></div></div><a class="button-hover more" href="2020/05/13/Meta-RCNN-Meta-learning-for-few-shot-object-detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/05/12/Diversity-Transfer-Network-for-Few-Shot-Learning/">Diversity Transfer Network for Few-Shot Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-05-13</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1912.13182.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1912.13182.pdf</a></p>
<p>AAAI 2020的一篇做Few shot的论文，论文的思想可以理解为对feature做augmentation来增强模型的多样性，希望在novel class也达到比较好的效果.</p>
<p><img src="Diversity-Transfer-Network-for-Few-Shot-Learning-394d9577106a1e1ebc5b29013c25f2a0f415a56c.png" alt=""></p>
<p>上面这张图就是论文所提的方法了，主要两个部分：</p>
<ol>
<li><p>橙色的Meta Task: 输入有三种图，Query Image和 Support Image就是标注的episode设置，在此基础上额外加了一组Reference Image，它由H组类别相同的图片对组成。那么首先这些所有的图都会过一个Feature Extractor进行特征提取(特征会经过Norm)，然后一组Reference Image 图片对的输出feature会相减和Support Image的feature再相加送入到一个Generator里面进行encoding，那么作者认为这个Encoding之后的feature和原始的support image的feature表征的是同一类物体(毕竟相同类别的两张图相减了嘛)，作者通过这样的操作把intra-class的diversity显式的encode到网络的训练过程中，希望模型可以学习到这种多样性，至于meta class的分类就和protonet一样了, 只是proxy的计算是H个encoding的feature再加上原始的support image的feature取平均得到：</p>
<p><img src="Diversity-Transfer-Network-for-Few-Shot-Learning-d5f6b0f3151d960602c16b9bbb15e45b4a56e689.png" alt=""></p>
<ol start="2">
<li>灰色的分支就是一个普通的分类任务，目的是加速模型的收敛，作者在基础上作了一些优化。</li>
</ol>
</li>
</ol>
<p>贴一下miniImageNet的点, 相比现在的SOTA就不是很高了:</p>
<p><img src="Diversity-Transfer-Network-for-Few-Shot-Learning-f659232dcbd0bec01b5f4feacd93cbba6178b503.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/05/12/Diversity-Transfer-Network-for-Few-Shot-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/05/09/Bounding-Box-Regression-with-Uncertainty-for-Accurate-Object-Detection/">Bounding Box Regression with Uncertainty for Accurate Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-05-09</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1809.08545" target="_blank" rel="noopener">https://arxiv.org/abs/1809.08545</a></p>
<p>CVPR 2019的论文，论文核心就是引入KL Loss，用分布对BoundingBox进行建模，同时利用分布学习到的variance去辅助NMS阶段对BoundingBox框的修正，挺不错的一篇论文。</p>
<p>首先来说它的motivation，目前检测任务的标注通常都是希望把物体的边界框标的越精准越好，但是实际情况下避免不了会引入很多的标注误差，因为有很多的case物体的边界框都很模棱两可，如下图，所以作者就想到用分布来建模BoundingBox，这样做的好处是对边界框不会那么敏感同时学习到的variance可以用于后续的后处理</p>
<p><img src="Bounding-Box-Regression-with-Uncertainty-for-Accurate-Object-Detection-屏幕快照 2020-05-09 下午3.07.34.png" alt=""></p>
<p>简化起见，作者就用高斯分布来建模框, 那么就涉及到2组变量x 和 σ，x代表(x1, y1, x2, y2)这样一组边界坐标，σ用来表征定位精度（σ同样是4维对应前面提到的(x1, y1, x2, y2)）的certainty，σ = 0就代表模型觉得当前这个x预测值足够的准:</p>
<p><img src="Bounding-Box-Regression-with-Uncertainty-for-Accurate-Object-Detection-屏幕快照 2020-05-09 下午3.10.07.png" alt=""></p>
<p>那么在模型结构上就可以很简单的用FC出这4维的σ了, (x1, y1, x2, y2)还是用原来的回归分支出:</p>
<p><img src="Bounding-Box-Regression-with-Uncertainty-for-Accurate-Object-Detection-屏幕快照 2020-05-09 下午3.10.13.png" alt=""></p>
<p>那么既然用分布来建模BoundingBox那么比较直接的想法就是用KL Loss来监督这个分布预测，最后通过一系列的转化本文用的回归Loss就是下图了，至于如何转换就可以直接参考原文了比较简单:</p>
<p><img src="Bounding-Box-Regression-with-Uncertainty-for-Accurate-Object-Detection-屏幕快照 2020-05-09 下午3.15.56.png" alt=""></p>
<p>那么这样的模型输出之后其实就可以出结果了，因为回归分支还是有4维的输出的，同时也会被KL Loss监督，论文中作者又利用高斯分布出的variance在NMS过程中Refine模型最终出的框, 伪代码如下:</p>
<p><img src="Bounding-Box-Regression-with-Uncertainty-for-Accurate-Object-Detection-屏幕快照 2020-05-09 下午3.17.53.png" alt=""></p>
<p>上图伪代码中标绿的地方对应下图的具体公式:</p>
<p><img src="Bounding-Box-Regression-with-Uncertainty-for-Accurate-Object-Detection-屏幕快照 2020-05-09 下午3.18.28.png" alt=""></p>
<p>我们可以先看伪代码：</p>
<ol>
<li>idx ← IoU(b<sub>m</sub>, B) &gt; 0 =&gt;这一行最后的idx实际上就是与b<sub>m</sub>有交集的”邻居”框(IoU&gt;0), 所以idx是一个集合</li>
<li>p ← exp(−(1 − IoU(b<sub>m</sub>, B[idx]))2/σ<sub>t</sub>) =&gt; 这一行最后的p实际上就是上述”邻居”的对应的权重，σ<sub>t</sub>没有特殊的含义就是一个可以随时调节的变量，那么这个计算公式也比较直观，IoU越大的“邻居”p越大.</li>
<li>b<sub>m</sub> ← p(B[idx]/C[idx])/p(1/C[idx]) =&gt; 这一行的C就是variance的集合，所以实际就是又利用“邻居”预测框的variance来加权框，那么很显然C越大对当前框坐标x的影响越小。</li>
</ol>
<p>最后可以看一下各个component具体的影响:</p>
<p><img src="Bounding-Box-Regression-with-Uncertainty-for-Accurate-Object-Detection-屏幕快照 2020-05-09 下午3.25.28.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/05/09/Bounding-Box-Regression-with-Uncertainty-for-Accurate-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/05/06/CenterMask-single-shot-instance-segmentation-with-point-representation/">CenterMask: single shot instance segmentation with point representation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-05-07</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/2004.04446" target="_blank" rel="noopener">https://arxiv.org/abs/2004.04446</a></p>
<p>和上一篇论文重名了，也中了CVPR 2020，论文中作者将Instance Segmentation归结为两个问题，一个是如果区分Instance，另一个是如果得到更加精确的分割结果，作者提出的网络CenterMask相关设计也对应着这两部分:</p>
<p><img src="CenterMask-single-shot-instance-segmentation-with-point-representation-屏幕快照 2020-05-06 下午9.23.10.png" alt=""></p>
<p>论文中的网络结构设计主体follow CenterNet，模型总共有5个分支:</p>
<ol>
<li>Heatmap: 用来出物体的中心和类别</li>
<li>Offset: 用来refine Heatmap出的中心位置</li>
<li>Size: 用来出物体的H和W</li>
<li><p>Shape：利用Heatmap和Offset出的中心点对应的feature(长度是1 x 1 x S<sup>2</sup>)出粗分割的结果，这样Size + Shape就可以表示不同大小形态的物体了, 具体做法下图描述的挺清楚的:</p>
<p> <img src="CenterMask-single-shot-instance-segmentation-with-point-representation-屏幕快照 2020-05-06 下午9.26.37.png" alt=""></p>
</li>
<li><p>Saliency: Channel为1的输出，和Semantic Segmentation的作用一致，只是这里只需要区分前后背景不用区分具体类别</p>
</li>
<li>那么最后分割的结果就是 Mask<sub><em>result</em></sub> = Sigmoid(Mask<sub><em>Size + Shape</em></sub>) <em> Sigmoid(Mask<sub></sub></em>Saliency*)，加上Heatmap的预测结果就可以实现Instance Segmentation了</li>
<li><p>然后网络会直接监督Mask<sub><em>result</em></sub>，下图中的M<sub>k</sub>:</p>
<p> <img src="CenterMask-single-shot-instance-segmentation-with-point-representation-屏幕快照 2020-05-06 下午9.38.13.png" alt=""></p>
</li>
</ol>
<p>这篇论文的主要内容就是这些了，loss什么的细节可以具体看论文。这篇论文看到这的话我一开始有一个疑问就是既然Size + Shape目的是区分instance，那么比如框/点这样很粗的定位信息就够了…但是对于遮挡问题分割会更加友好一点..</p>
</div></div><a class="button-hover more" href="2020/05/06/CenterMask-single-shot-instance-segmentation-with-point-representation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/05/06/CenterMask-Real-Time-Anchor-Free-Instance-Segmentation/">CenterMask : Real-Time Anchor-Free Instance Segmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-05-07</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1911.06667" target="_blank" rel="noopener">https://arxiv.org/abs/1911.06667</a></p>
<p>CVPR2020的一篇分割的论文，整体novelty有限，相当于将FCOS应用到分割领域中:</p>
<p><img src="CenterMask-Real-Time-Anchor-Free-Instance-Segmentation-屏幕快照 2020-05-06 下午12.45.45.png" alt=""></p>
<p>上图就是论文所提的方法了，前半截就是标准的FCOS的流程，一个Regression分支+一个Classification分支+一个Centerness分支，后半截就是新加入的Mask分支，做法和Mask RCNN做法是一致的，首先利用Regression分支出的框去Crop Feature，这里面用的还是ROIAlign只是具体去到哪个FPN Layer上去Crop Feature作者在论文中做了一些说明，但实际上和Mask RCNN还是没差到哪去，只是在FCOS的框架下做了一下适配。</p>
<p>Mask分支的做法是这样的，首先Crop出来的14x14的Feature<strong>经过4层conv</strong>送入到一个叫SAM的Attention模块中，SAM模块呢做了这么几个事：</p>
<ol>
<li>分别经过两个Pooling，一个是Average Pooling ，一个是Max Pooling，然后把这两个Pooling的输出Concat到一起得到F<sub>concat</sub>，至于这样做有啥Insight就不太清楚了。</li>
<li>F<sub>concat</sub>再经过一个Conv得到新的Feature F<sub>new</sub>。</li>
<li>F<sub>new</sub>通过Sigmoid之后和原始的Feature相乘得到Attention之后的Feature F<sub>attention</sub>。</li>
<li>F<sub>attention</sub> 2倍Up-Sample之后再1x1去分类得到最终的结果.</li>
</ol>
<p>作者另外的改进就是VoVNetV2 backbone的改进了，两处:</p>
<ol>
<li>VoVNet + residual connection</li>
<li>SENet -&gt; eSENet（SENet原来是2层FC，第一层FC会砍channel数，第二层FC会还原回去，eSENet就省掉了第一个FC直接上第二个FC增加了不少的计算量，原因是作者认为第一个FC降channel会丢失channel的信息）</li>
</ol>
<p>各个模块的ablation(mask scoring就是Mask Score RCNN里面的方法):</p>
<p><img src="CenterMask-Real-Time-Anchor-Free-Instance-Segmentation-屏幕快照 2020-05-06 下午12.58.31.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/05/06/CenterMask-Real-Time-Anchor-Free-Instance-Segmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/24/LSTD-A-low-shot-transfer-detector-for-object-detection/">LSTD: A low-shot transfer detector for object detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1803.01529" target="_blank" rel="noopener">https://arxiv.org/abs/1803.01529</a></p>
<p>AAAI 2018的一篇论文，应该是最早做few shot for detection的论文，也是目前few shot detection论文都会引用对比的一篇论文，论文的一些想法还是比较make sense的。</p>
<p><img src="LSTD-A-low-shot-transfer-detector-for-object-detection-屏幕快照 2020-04-25 下午3.30.19.png" alt=""></p>
<p>这张图基本涵盖LSTD所有的内容了，首先从结构上来看下LSTD是怎么做的，LSTD把分类回归分成了两步，第一步做回归，分类信息只涉及是否是物体这样的二分类，然后在第二步对第一步的回归框进行细分类别而不再做回归的refine了，</p>
<ol>
<li><p>第一步的回归用的SSD，作者给出了原因:</p>
<ol>
<li><strong>SSD很显式的在做multi scale，对于小样本训练数据来说可以很好的缓解multi scale的问题</strong></li>
<li><strong>Faster RCNN最后回归的时候是N个分类器，所以它所有的分类都是类别相关的，但是SSD不是，SSD都是类别无关的相互share的，所以呢这种情况下在source domain下训练好的模型可以直接拿来做为target domain下模型的初始化</strong></li>
</ol>
</li>
<li><p>第二步最后的分类回归，主要是将Faster RCNN最后的FC层换成Conv层来实现分类，<strong>好处是减少了模型参数，一定程度上缓解了过拟合的情况</strong>。</p>
</li>
</ol>
<p>在LSTD的基础上为了更好的进行transfer learning作者还提出了两个regulazation的方法, 这两个方法主要在target domain fine tune的时候使用:</p>
<ol>
<li><p>第一个是背景抑制，这样做的好处是消除掉背景的干扰让模型专注的去处理目标信息:<br>做法很简单，用gt框生成一个mask拍到conv feature map上得到背景的特征F<sub>BD</sub>. L<sub>BD</sub> = ||F<sub>BD</sub>||<sub>2</sub><br>效果看上去还是比较明显的:</p>
<p> <img src="LSTD-A-low-shot-transfer-detector-for-object-detection-屏幕快照 2020-04-25 下午4.02.29.png" alt=""></p>
</li>
<li><p>第二个是source domain到target domain的知识迁移，出发点是source domain的信息和target domain的信息很多情况下是互通的，比如牛马羊之类的都是有一定的相似度的，所以在target domain训练数据比较缺失的情况下借助source domain的知识辅助进行学习是很有必要的。那么做法呢也很直接:</p>
<p>   首先我们有两个LSTD模型: LSTD<sub>source</sub>，这个是利用全量的source domain的数据训练的. 另一个LSTD<sub>target</sub>,它是用LSTD<sub>source</sub>进行初始化的，当然最后的分类层是随机初始化的，在一般的分类层之外还会加一个额外的分类层来把LSTD<sub>source</sub>的知识迁移过来，具体看上图很好理解。然后在学习的时候对于一张输入训练图会分别送入到LSTD<sub>source</sub>和LSTD<sub>target</sub>，得到的分类logits进行CE监督，这样就显式的将source domain的信息来监督target domain的学习:</p>
<p>   <img src="LSTD-A-low-shot-transfer-detector-for-object-detection-屏幕快照 2020-04-25 下午4.10.22.png" alt=""></p>
</li>
</ol>
<p>伪代码:</p>
<p><img src="LSTD-A-low-shot-transfer-detector-for-object-detection-屏幕快照 2020-04-25 下午4.14.32.png" alt=""></p>
<p>那么LSTD的全部就是这些内容了核心呢是降低模型学习的难度毕竟少量数据下很容易过拟合。</p>
</div></div><a class="button-hover more" href="2020/04/24/LSTD-A-low-shot-transfer-detector-for-object-detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/24/Meta-Learning-to-Detect-Rare-Objects/">Meta-Learning to Detect Rare Objects</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Meta-Learning_to_Detect_Rare_Objects_ICCV_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Meta-Learning_to_Detect_Rare_Objects_ICCV_2019_paper.pdf</a></p>
<p>ICCV 2019的论文，做few shot detection. 论文的insight作者说的比较清楚 <em>“we introduce a parameterized weight prediction meta-model that is trained on the space of model parameters to predict a category’s large-sample bounding box detection parameters from its few-shot parameters. The”</em>, 相同类别的一些数据当数据量比较小的时候(few shot)以及数据量比较大的时候学习到的分类信息肯定是不一样的，那么作者的出发点呢就是bridge这两者的差异，学习一个<strong>weight prediction meta-model</strong>来实现从few shot参数到large shot的参数转化。</p>
<p><img src="Meta-Learning-to-Detect-Rare-Objects-屏幕快照 2020-04-27 下午12.29.27.png" alt=""></p>
<p>作者将CNN模型里面的所有参数区分为类别有关的和类别无关的两种，比如对于Faster RCNN，backbone和RPN是类别无关的，而Fast RCNN部分是类别相关的。对于类别无关的参数, 直接在fine tune的时候用来初始化就好了，因为特征是类间共享的。而对于类别有关的参数，就学习一个<strong>weight prediction meta-model T</strong>来实现few shot参数到large shot参数的的转化。</p>
<p><img src="Meta-Learning-to-Detect-Rare-Objects-屏幕快照 2020-04-27 下午12.35.11.png" alt=""></p>
<p>上图呢就是在一个episode里面学习的loss监督，假设对于类别c，w<sub>det</sub><sup>c,*</sup>代表基于large shot训练出来的参数(fast rcnn的分类+定位参数), w<sub>det</sub><sup>c</sup>代表基于few shot训练出来的对应的参数，那么作者参考以前的论文将w<sub>det</sub><sup>c</sup>到w<sub>det</sub><sup>c,*</sup>的转化定义为一个回归任务φ,φ就是用一个FC实现的，所以loss上的前半截就是直接约束这个回归任务，loss的后半截就是一个标准的cls+reg的loss。</p>
<p>那么最终在训练的时候呢，第一步在全量数据上先训练一个基本的检测器，第二步呢fix住模型无关的参数(backbone + rpn), 放开模型相关的参数在MAML配置下联合训练w<sub>det</sub><sup>c,*</sup>和w<sub>det</sub><sup>c</sup>，有点蒸馏的感觉。<br>在Meta-testing阶段，meta training阶段学到的weight prediction meta-model T参数不变作为一个regulization项来监督novel class的fine tune.</p>
<p>实验结果, 放到现在点也不算高了，那篇直接fine tune head的论文可以把点刷到30+了:</p>
<p><img src="Meta-Learning-to-Detect-Rare-Objects-屏幕快照 2020-04-27 下午12.45.09.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/04/24/Meta-Learning-to-Detect-Rare-Objects/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/19/Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks/">Large-Margin Softmax Loss for Convolutional Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://arxiv.org/abs/1612.02295" target="_blank" rel="noopener">http://arxiv.org/abs/1612.02295</a></p>
<p>ICLR2016的一篇论文，出发点挺直接的一篇论文，主要在优化Softmax Loss，提出了Large-Margin Softmax Loss来促使网络学到更加有区分度的Feature.</p>
<p>首先来分解一下Softmax Loss的计算:</p>
<p><img src="Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks-屏幕快照 2020-04-19 下午7.52.31.png" alt=""></p>
<p>这里的f就是最后一层FC的输出： f<sub>yi</sub> = W<sub>yi</sub><sup>T</sup> * x<sub>i</sub>, x<sub>i</sub>代表第i张图抽取出来的feature，W<sub>yi</sub>就是最后一层FC连接到前一层feature的weights，<strong>其实可以理解为y<sub>i</sub>类在高维空间的类中心</strong>. 又因为向量的内积可以直接转化成cosine运算:</p>
<p>W<sub>yi</sub><sup>T</sup> * x<sub>i</sub> = ||W<sub>j</sub>|| ||x<sub>i</sub>|| cos(θ<sub>j</sub>), θ<sub>j</sub>当然就是这两个向量之间的夹角了. 所以Softmax Loss又可以转化成:</p>
<p><img src="Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks-屏幕快照 2020-04-19 下午7.59.03.png" alt=""></p>
<p>那么Large-Margin Softmax Loss提出了和SVM类似的分类平面的问题，它希望inter-class之间的margin可以大一些，所以在Softmax Loss的基础上引入一个变量m来控制分类平面的margin.</p>
<p>在原始的Softmax Loss中针对一个二分类问题，假设样本x属于类别1，那么我们希望||W<sub>1</sub>|| ||x|| cos(θ<sub>1</sub>) &gt; ||W<sub>2</sub>|| ||x|| cos(θ<sub>2</sub>), (cosine在[0,π]区间内是一个单调递减的函数，同时指数函数还是一个单调递增的函数)，那么在Large-Margin Softmax Loss上就转化成||W<sub>1</sub>|| ||x|| cos(mθ<sub>1</sub>) &gt; ||W<sub>2</sub>|| ||x|| cos(θ<sub>2</sub>)，所以Large-Margin Softmax Loss：</p>
<p><img src="Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks-屏幕快照 2020-04-19 下午8.06.10.png" alt=""></p>
<p>其中:</p>
<p><img src="Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks-屏幕快照 2020-04-19 下午8.06.33.png" alt=""></p>
<p>同时满足D(π/m) = cos(π/m). 论文里面取:</p>
<p><img src="Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks-屏幕快照 2020-04-19 下午8.07.44.png" alt=""></p>
<p>论文里给的一张示意图挺能说明Large-Margin Softmax Loss的insight的：</p>
<p><img src="Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks-屏幕快照 2020-04-19 下午8.08.13.png" alt=""></p>
<p>效果上面当然也挺好了:</p>
<p><img src="Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks-屏幕快照 2020-04-19 下午8.09.06.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/04/19/Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/18/FeatureNMS-Non-Maximum-Suppression-by-Learning-Feature-Embeddings/">FeatureNMS: Non-Maximum Suppression by Learning Feature Embeddings</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/NMS/">NMS</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/2002.07662.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2002.07662.pdf</a></p>
<p>从NMS的角度解决行人crowd的问题，解决方法很直接，抛开IoU从框面积的角度去解overlap问题，论文直接对每一个anchor再学习一个embedding feature去辅助判断不同的物体, 伪代码写的比较清楚,集合D就是最后的输出了:</p>
<p><img src="FeatureNMS-Non-Maximum-Suppression-by-Learning-Feature-Embeddings-屏幕快照 2020-04-18 下午5.39.53.png" alt=""></p>
<p>做法上也很直接，在RetinaNet输出head，cls head + reg head之外再加一个embedding head，纬度为32维，当然这个值可以随意定了，监督方式Margin Loss:</p>
<p><img src="FeatureNMS-Non-Maximum-Suppression-by-Learning-Feature-Embeddings-屏幕快照 2020-04-18 下午5.41.28.png" alt=""><br><img src="FeatureNMS-Non-Maximum-Suppression-by-Learning-Feature-Embeddings-屏幕快照 2020-04-18 下午5.41.56.png" alt=""></p>
<p>f<sub>i</sub>和f<sub>j</sub>是两个不同的anchor对应的embeding feature，至于obj(i)就是用来判断anchor<sub>i</sub>属于哪一个object(gt box). NMS距离计算方面就是普通的L2.</p>
<p>β 和 α为两个超参数 : <em>α determines the margin between positive and negative examples, and the parameter β determines the decision threshold. (α = 0.2,β = 1.0)</em></p>
<p>从结果上看当然是涨点了，但是感觉没有说服力，所有的对比没有和sota的方法进行一个公平对比，模型学习的设置也是和sota的方法完全不一样的，比如论文里应该只用了visible box来训练：</p>
<p><img src="FeatureNMS-Non-Maximum-Suppression-by-Learning-Feature-Embeddings-屏幕快照 2020-04-18 下午5.48.08.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/04/18/FeatureNMS-Non-Maximum-Suppression-by-Learning-Feature-Embeddings/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/18/Prototypical-networks-for-few-shot-learning/">Prototypical networks for few-shot learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1703.05175.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.05175.pdf</a></p>
<p>NIPS的一篇论文，few shot learning比较早期的工作了，这个论文的做法和simple shot是很类似的，当然simple shot在后了，首先作者定义一个embedding function f可以将经过feature extractor的图片映射到高维空间,在这个高维空间上找到各个class的类别中心(prototype)，通过距离计算将query图片assign到对应的类别中心完成分类：</p>
<p>c<sub>k 就是类别k的prototype, S<sub>k</sub>为类别属于k的样本集合，f<sub>φ</sub>就是上面说的那个embedding function，x<sub>i</sub>就是输入图片经过feature extractor得到的特征了.</sub></p>
<p><img src="Prototypical-networks-for-few-shot-learning-屏幕快照 2020-04-18 下午4.52.53.png" alt=""></p>
<p>类别归属的判断, d就是一种距离的度量，论文里面就直接用的欧式距离:</p>
<p><img src="Prototypical-networks-for-few-shot-learning-屏幕快照 2020-04-18 下午4.56.27.png" alt=""></p>
<p>论文提供的伪代码还是比较清楚的:</p>
<p><img src="Prototypical-networks-for-few-shot-learning-屏幕快照 2020-04-18 下午4.58.37.png" alt=""></p>
<p>感觉整个论文的核心就在这了，因为是投的nips论文另外的笔墨都花在了机制的解释和论证上了，有兴趣可以直接看原文</p>
</div></div><a class="button-hover more" href="2020/04/18/Prototypical-networks-for-few-shot-learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/16/Meta-Transfer-Learning-for-Few-Shot-Learning/">Meta-Transfer Learning for Few-Shot Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Meta-Learning/">Meta Learning</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1812.02391.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1812.02391.pdf</a></p>
<p>MAML一族的解决few shot learning的问题，对MAML的基本做法也做了比较详细的了解，论文的motivation呢是解决目前神网络在few shot任务下容易过拟合的问题，所以在fix backbone的基础上学了一个比较轻量的scale/shift网络来meta learning. 论文所提的方法有两大主要内容:</p>
<ol>
<li>Meta-Transfer Learning (MTL), 方法主要的Pipeline</li>
<li>Hard Task (HT) Meta-Batch, meta learning形式下的hard mining，做法比较简单粗暴</li>
</ol>
<p><img src="Meta-Transfer-Learning-for-Few-Shot-Learning-屏幕快照 2020-04-16 下午3.08.46.png" alt=""></p>
<p>整个Pipeline:</p>
<ol>
<li>首先需要明确整个pipeline里面涉及到的模型: <strong>feature extractor(图片的特征提取),base learner(一个标注的分类模型), SS(一个用于学习kernel的scale和bias的shift的小子网络)</strong></li>
<li>先用base class的所有数据训练一个feature extractor，这算是常规的操作, <strong>这个feature extractor在后续所有的操作中都会被fix住</strong></li>
<li><p>Scaling and Shifting (SS), 这是Meta Transfer的核心，在每一轮meta task上它会和base learner同步一起学习，下图可以很好的解释SS的操作，为了避免过拟合，SS选择在feature extractor的基础上额外加一个小网络来学习transfer能力，而保证原来的feature extractor fix住，Scale学习的是kernel的scale，而Shift学习的是bias的shift，另外一种对现有网络的学习方式，下图也给了和一般的全面fine tune方式的比较：</p>
<p> <img src="Meta-Transfer-Learning-for-Few-Shot-Learning-屏幕快照 2020-04-16 下午3.21.15.png" alt=""></p>
<p> 当前这种考虑我的第一反应是SKNet之类对Kernel进行attention的方法感觉有点类似</p>
</li>
<li><p>HT Meta Batch: hide mining, 在每个meta test任务上选择acc最低的类别，然后把这些数据或者这些数据对应的类别拿出来再重点refine</p>
</li>
<li>最后在meta test阶段fix住SS只在noval class上fine tune base learner.</li>
</ol>
<p>伪代码:</p>
<p><img src="Meta-Transfer-Learning-for-Few-Shot-Learning-屏幕快照 2020-04-16 下午3.27.24.png" alt=""></p>
<p><img src="Meta-Transfer-Learning-for-Few-Shot-Learning-屏幕快照 2020-04-16 下午3.27.53.png" alt=""></p>
<p>贴一下点, 放到现在点就不算很高了:</p>
<p><img src="Meta-Transfer-Learning-for-Few-Shot-Learning-屏幕快照 2020-04-16 下午3.29.39.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/04/16/Meta-Transfer-Learning-for-Few-Shot-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/14/Learning-to-Compare-Relation-Network-for-Few-Shot-Learning/">Learning to Compare: Relation Network for Few-Shot Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf</a></p>
<p>CVPR2018的论文，整体pipeline很简单，做法也比较make sense，感觉比较不错的一篇论文，论文解决few shot的出发点是构建test image和support image之间的关系来给test image赋label。本质上和那些基于最近邻的逻辑是一样的，只是它把这个compare的过程放到网络里显式的去学习了。</p>
<p>![](Learning-to-Compare-Relation-Network-for-Few-Shot-</p>
<p>Learning-屏幕快照 2020-04-14 下午5.57.44.png)<br>上面这张图其实画的挺清楚的（5 way 1 shot），整个pipeline主要有两个模块，一个embedding module，一堆conv对image抽一下特征，另一个是relation module，这个主要是用来构建support image和query image的关系，细节做法：</p>
<ol>
<li>所有的图片过embedding module得到对应的feature map，对于C shot的C &gt; 1的话就把所有同类图片的feature map做一下element wise sum得到一个代表这个类别的feature map</li>
<li>得到C个feature map之后依次和query image的feature map concat到一起送入到一个conv block里面做回归，la</li>
<li>bel的话就是如果query image属于某个类，那个位置就为1否则为0，让网络去学习图片与类别之间的关系</li>
</ol>
<p>注意点：</p>
<ol>
<li>模型的训练还是去fine tune，先在训练集上构建episode训练然后再到support集上fine tune</li>
<li>对于zero shot，一般会给出关于类别的语意信息，那么就把原先support set产生的feature map直接改成类别的semantic embedding就好，然后其他ppl就和K shot保持一致</li>
</ol>
<p>下面这张图就是RN网络的细化：</p>
<p><img src="Learning-to-Compare-Relation-Network-for-Few-Shot-Learning-屏幕快照 2020-04-14 下午6.12.30.png" alt=""></p>
<p>贴一下结果吧,放到现在在miniImageNet上的结果就不高了:</p>
<p><img src="Learning-to-Compare-Relation-Network-for-Few-Shot-Learning-屏幕快照 2020-04-14 下午7.13.04.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/04/14/Learning-to-Compare-Relation-Network-for-Few-Shot-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/10/SimpleShot-Revisiting-Nearest-Neighbor-Classification-for-Few-Shot-Learning/">SimpleShot: Revisiting Nearest-Neighbor Classification for Few-Shot Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-10</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1911.04623.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1911.04623.pdf</a></p>
<p>一篇打脸的论文，现在做few shot任务的方法通常有meta learning/metric learning等方法，作者在论文里claim其实只要对feature extrator输出的feature进行最近邻统计就可以在few shot benchmark上得到很高的指标，这一类简单且高效的few shot方法可以参考另外两篇论文：<em>Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?</em>, 以及做检测的: <em>Frustratingly Simple Few-Shot Object Detection</em>。 论文很简短，做法很简单，效果很不错。</p>
<p>看看论文怎么做的吧，首先数据集上面会划分成D<sup>base</sup>, D<sup>noval</sup>, 然后用一般的CNN网络基于D<sup>base</sup>去训练一个分类器，这里作者也用了五个不同的backbone，Conv-4，WRN-28-10， DenseNet-121，ResNet-10/18，MobileNet.</p>
<p>当分类模型训练好以后变成feature extractor，对novel class里面的test数据提取特征，同时也对novel class里的support数据提取特征，通过计算两者的相似度(最短距离)来match test数据的label，如果support数据集里每一类的图片有多张K-shot，那么就做个平均作为这一类的类别中心，test图片的feature就和这个类别中心算最短距离就好<br>。</p>
<p>然后对于feature extractor的数据作者也做了一些ablation，一是比做任何操作，直接去计算距离，二是L2 norm一下，三是对feature减均值再做norm，作者分别做了比较详细的对比实验：</p>
<p><img src="SimpleShot-Revisiting-Nearest-Neighbor-Classification-for-Few-Shot-Learning-屏幕快照 2020-04-10 下午12.44.47.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/04/10/SimpleShot-Revisiting-Nearest-Neighbor-Classification-for-Few-Shot-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/15/Few-Shot-Object-Detection-with-Attention-RPN-and-Multi-Relation-Detector/">Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1908.01998.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1908.01998.pdf</a></p>
<p>这应该是CVPR2020的论文了, 看完之后我觉得挺失望的。做法上我觉得是完全把用在tracking任务上的siamese网络原封不动的迁移过来了，然后换了一个few shot detection的故事在讲，虽然论文中作者在siamese网络的后面花了不少笔墨写了Patch-Relation Head, Local-Correlation Head, Global-Relation Head 但是注意看Table 3 实际上点上面最大的收益就是siamese网络，加上一堆的trick之后实际上只涨了一个点(68.6% -&gt; 69.8%).</p>
<p><img src="Few-Shot-Object-Detection-with-Attention-RPN-and-Multi-Relation-Detector-截屏2020-03-2118.40.42.png" alt=""></p>
<p>论文所提的网络结构是基于孪生网络来做的，support set和query set分别送入孪生网络的两个分支，两个分支的backbone参数是共享了，两者抽完feature之后会对两个feature做一个相似度计算得到一个新的feature map，这个feature map会送去产生proposal，这一步就是论文里面提到的<strong>attention mechanism</strong>. 然后就是这两组roi pooling之后feature的交互了，分为三种:</p>
<ol>
<li><strong>patch-relation head</strong> 两个分支的roi feature concat到一起送入一个子网络出cls + reg</li>
<li><strong>global-relation head</strong> 两个分支的roi feature concat到一起，然后走一个avg pooling 再经过两层FC后出一个score</li>
<li><strong>local-correlation head</strong> 用和孪生网络差不多的cross correlation两个feature 卷一把走fc后又出了一个分</li>
</ol>
<p>最后的分就是三者之和，reg的结果就用<strong>patch-relation head</strong>出的结果</p>
</div></div><a class="button-hover more" href="2020/03/15/Few-Shot-Object-Detection-with-Attention-RPN-and-Multi-Relation-Detector/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/14/RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection/">RepMet: Representative-based metric learning for classification and few-shot object detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-15</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1806.04728" target="_blank" rel="noopener">https://arxiv.org/abs/1806.04728</a></p>
<p>这是CVPR2019的论文了，用metric learning来做few shot的，作者在论文里主要提了一个<em>插件subnet</em>可以用来替换掉分类任务或者定位任务中的class分支。下面这张图可以比较好的阐述问题：</p>
<p><img src="RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection-截屏2020-03-1519.19.31.png" alt=""></p>
<p>因为论文所提的这个结构是一个subnet，可以无缝的插入到现有的网络结构中，所以它的输入通常是具体的feature，对于检测任务来说就是<strong>ROI的feature</strong>，输入的feature首先会经过一个embedding module，论文中对于分类任务这个embedding module就是2层FC，对于检测任务就是3层FC，这样就可以把输入的feature压缩到很小的维度(e), 再来看论文中提出的核心概念(<em>‘representatives’</em>), 作者把每一个类别抽象成k个mode，每个mode定义为e长度的vector(为了和embedding module保持一致)，具体实现的时候就是一个N x k x e大小的fc在具体操作的时候reshape一把就好，这个fc输入恒为1。</p>
<p>得到这些信息之后就可以计算距离了，embedding之后的结果和representatives直接相乘就可以得到一个N x k的结果，默认服从高斯分布, 这样可以得到每一个class对应的每一个mode的概率:</p>
<p><img src="RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection-截屏2020-03-1519.27.31.png" alt=""></p>
<p>那么embedding之后的feature或者输入feature对应这些具体类别的概率呢, 取mode里最大的概率:</p>
<p><img src="RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection-截屏2020-03-1519.27.36.png" alt=""></p>
<p>然后这篇论文不太好理解的地方是模型的训练，对于few shot检测来说，训练依然服从c way k shot这样的采样逻辑来小规模训练，每次训练的时候会用新类别的embedding结果直接替换掉原来的<strong>representatives</strong>进行fine tune学习。可能这一步就是针对小样本学习设定的。</p>
</div></div><a class="button-hover more" href="2020/03/14/RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/13/Conditional-Convolutions-for-Instance-Segmentation/">Conditional Convolutions for Instance Segmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-14</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/2003.05664.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2003.05664.pdf</a></p>
<p>最新放出来的论文，做Instance Segmentation，和PolarMask一样都是follow FCOS大的框架，不同于Mask RCNN出框crop然后做Seg的方式，论文所提的方法更加类似FCN直接出，整个框架的设计感觉还是比较精巧的。具体看下怎么做：</p>
<p><img src="Conditional-Convolutions-for-Instance-Segmentation-屏幕快照 2020-03-13 下午2.42.24.png" alt=""></p>
<p>我把这个结构分成两个部分:</p>
<ol>
<li>FCOS: 也就是上图的上半部分，整个pipeline和FCOS没啥差别，只是head层的输出略有不一样，那么对于FPN每一个layer的每一个pixel，主要出三个东西:<ol>
<li>Classification Head: 和原版FCOS含义一样</li>
<li>Center-ness Head： 这个定义也是和原版FCOS一致的，用来抑制不太好的预测结果</li>
<li>Controller Head： 这个就是本篇论文的核心了，他负责出Mask FCN Head的参数，假设Mask FCN Head的总参数量是X，那么Controller Head的维度就是X，论文中当X取169的时候性能就比较不错了。</li>
</ol>
</li>
<li>Mask FCN Head: Mask FCN Head是论文的核心点，上图下半部分，它的结构就是一般的FCN，但是它的特点在于FCN的参数是动态的，不同的instance有不同的参数，这就会造成多个Mask FCN Head的感觉，同时功能上也类似Mask RCNN出框的作用-区分Instance. Mask FCN Head接在P3 Layer之后, 经过几层Conv之后得到一个H x W x C的feature map, 论文中C = 8，作者claim C的取值对分割的性能影响不大. 甚至C = 2的时候性能也只是下降0.3%！因为Mask FCN Head负责出instance，而其参数又是由P3 - P7的head层所得，所以为了构建两者的联系，在Mask FCN Head输入层F<sub>mask</sub> Concat了F<sub>mask</sub>到P3 - P7的相对位移，假设F<sub>mask</sub>的维度为H x W x C，P<sub>i</sub>的维度为H<sub>i</sub> x W<sub>i</sub> x C<sub>i</sub>, 那么把F<sub>mask</sub>的每一个pixel映射到P<sub>i</sub>，映射前后坐标的offset就会和原始的F<sub>mask</sub> Concat到一起作为<strong>Mask FCN Head</strong>的输入。</li>
</ol>
<p>最后模型的效果就是instance aware的segmentation:<br><img src="Conditional-Convolutions-for-Instance-Segmentation-屏幕快照 2020-03-13 下午3.51.40.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/03/13/Conditional-Convolutions-for-Instance-Segmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/13/Meta-R-CNN-Towards-General-Solver-for-Instance-level-Low-shot-Learning/">Meta R-CNN : Towards General Solver for Instance-level Low-shot Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-14</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1909.13032" target="_blank" rel="noopener">https://arxiv.org/abs/1909.13032</a></p>
<p>ICCV 2019的论文，论文标题比较直白，meta learning + Faster/Mask RCCN做物体检测/分割。作者在论文里强调了few shot场景下物体分类和物体检测的差异，物体分类通常对全图做，物体检测由于全图有不同的多个物体所以并不适用，那么自然而然的就想到在ROI后面做。那么论文就想办法把meta learning的机制融合进two stage的pipeline里面。</p>
<p><img src="Meta-R-CNN-Towards-General-Solver-for-Instance-level-Low-shot-Learning-截屏2020-03-1423.34.50.png" alt=""></p>
<p>论文所提的网络结构也比较简单，首先保留了Faster/Mask RCNN的整个pipeline，在此基础上增加了一个Predictor-head Remodeling Network(PRN)子网络，这个自网络也是这篇论文的主要贡献所在，出发点的话和很多的few shot的论文很类似就是想办法增强网络对于不同类别的区分度，手段也是一样的就是各种atttention。PRN的作用呢也是用来学习不同class的attention编码(class-attentive vectors)。<strong>这个编码和同是ICCV2019论文的feature reweight方法的不一样点是那篇论文最后学的是channel attention，每一个channel代表一个类，以此来增强某一个或某几个channel，而这篇论文学的是对于给定类别的roi feature学习类内每个channel的权重，所以PRN输出是固定长度的vector代表每个类别对应的attention</strong>。</p>
<p>论文整体的结构就是这样，主要需要注意的应该是模型的过程了，首先模型训练的时候还是会遵循一般meta learning的策略，构建C way k shot的数据集，support集 D<sub>train</sub> 和 query 集D<sub>meta</sub>,其中D<sub>train</sub>作为Faster/Mask RCNN的输入，D<sub>meta</sub>作为PRN的输入，这样两者联动同时加上Loss<sub>cls</sub> / Loss<sub>reg</sub> / Loss<sub>meta</sub>来共同监督网络的训练。Loss<sub>meta</sub>用的是ce loss.</p>
<p>至于在最后的inference可以采取和feature reweight那篇论文一样的策略，class-attentive vector可以根据已有数据直接预处理得到就好。</p>
</div></div><a class="button-hover more" href="2020/03/13/Meta-R-CNN-Towards-General-Solver-for-Instance-level-Low-shot-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/13/Incremental-Few-Shot-Object-Detection/">Incremental Few-Shot Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-13</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Incremental-Learning/">Incremental Learning</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/2003.04668" target="_blank" rel="noopener">https://arxiv.org/abs/2003.04668</a></p>
<p>CVPR2020的论文，论文算是提出了一个新的子任务，Few Shot + Incremental Learning，这个任务论文中也给了明确的定义：</p>
<ol>
<li>首先模型的训练是可以基于量比较大的base class的数据来训练的，这样可以得到一个还不错的检测模型</li>
<li>novel class的数据可以随时<em>注册</em>，同时数据量很少，这样模型可以做到随时部署同时可以cover新增的类别</li>
</ol>
<p><img src="Incremental-Few-Shot-Object-Detection-屏幕快照 2020-03-13 下午12.10.03.png" alt=""></p>
<p>然后我们具体来看看论文是怎么做的, 首先模型方面被拆成两个部分</p>
<ol>
<li>feature extractor: 论文中描述为class-agnostic, 相当于这一块是类别无关的base class和novel class公用这一部分用来特征提取</li>
<li>class code generator: 顾名思义，学习不同类别的编码，和上述提取出来的feature整合作为最后detection的输入</li>
</ol>
<p>模型的学习也按照上面的两个模块分成了两个stage：</p>
<p>stage I: <strong>feature extractor</strong>的学习，这部分内容没有特别的地方就是CenterNet，训练完之后把detection head去掉之后就是feature extractor了</p>
<p>stage II: <strong>class code generator</strong>的学习, 这一部分的理解需要把CenterNet heatmap输出那一层进行一定的分解，假设feature extractor最后输出的feature map维度是H x W x C，总共有N个类别，那么逻辑上来讲heatmap维度应该是h x w x N，这个维度转化可以通过1 x 1的卷积达到目的，那么分解一下对于其中的一层feature map相当于H x W x C经过 1 x 1 x C的conv得到！</p>
<p>作者把这个理解为class independent的一个编码，这也是feature map做法的好处类别之间是相对独立的. <em>class code generator</em> 就被定义成学习这个class independent的编码. 至于怎么学呢, meta learning, 每一轮学习通过sample support集和query集去refine模型，对于给定一组输入，走 <em>feature extractor</em> 得到对应的feature，走 <em>class code generator</em> 得到对应的code，两者一结合就可以得到一个h x w的heatmap，然后拿这个heatmap去和gt做L1 loss从而来监督<em>class code generator</em>的学习</p>
<p>那么在Inference的时候呢对于base class的数据就直接上训练好的CenterNet，对于novel class的数据就分别走一遍<em>feature extractor</em>和<em>class code generator</em>然后得到最后的输出，这就是最后的定位结果。</p>
</div></div><a class="button-hover more" href="2020/03/13/Incremental-Few-Shot-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/12/Objects-as-Points/">Objects as Points</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-12</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL：<a href="https://arxiv.org/abs/1904.07850" target="_blank" rel="noopener">https://arxiv.org/abs/1904.07850</a></p>
<p>Anchor free做检测的论文CenterNet，应该是中了CVPR2019，整体是把做Pose常用的heatmap的做法放到了检测任务上，当然也可以理解为Pose Estimation/Detection的一种统一，毕竟论文里面也是给出了同样框架下在Pose Estimation和3D Detection任务上面的表现。</p>
<p>具体看下在普通的检测任务上是怎么做的吧：</p>
<p><img src="Objects-as-Points-屏幕快照 2020-03-12 下午6.27.05.png" alt=""></p>
<p>对问题的建模比较好理解，通常物体的框标注为(x1, y1, x2, y2)，那么论文中作者把物体框定义为Center Point + Width/Height，整个网络基于heatmap出结果，每一个pixel出C + 4维输出，所以heatmap最后输出的channel应该是(C + 4), C代表类别，C个channel每一个pixel位置的值就代表这个pixel属于这个类别的概率，剩下来的4维分别是基于这个pixel的offset(2维) 和 object的长宽(2维), 分类用的focal loss，回归用的L1 Loss，至于在inference的时候论文里写的也挺清楚的：找出C每个heatmap的峰值(最多100个)，峰值的定义就是比周围8个近邻大的点，找到这100个峰值之后接可以利用对应4个channel的输出还原出对应的100个框，这就是最后的结果，这些框不再需要走NMS，可以通过简单的卡阈值完成模型的输出。</p>
<p>CenterNet整体的思想就是这些，十分简单有效是一篇不错的工作，另外一个好处是这篇论文最后的代码也开源了: <a href="https://github.com/xingyizhou/CenterNet" target="_blank" rel="noopener">https://github.com/xingyizhou/CenterNet</a></p>
</div></div><a class="button-hover more" href="2020/03/12/Objects-as-Points/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/02/28/Few-shot-Object-Detection-via-Feature-Reweighting/">Few-shot Object Detection via Feature Reweighting</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-14</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1812.01866.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1812.01866.pdf</a></p>
<p>ICCV 2019的一篇论文, 做few shot detection，论文主要的贡献点是提出了一个新的few shot pipeline，主要分成三个部分： <strong>meta extractor</strong>，有点类似meta learning里面的逻辑，学习object有区分度的信息,不同的meta信息用最后feature map中不同的channel表示；<strong>reweight module</strong> 类似SE的逻辑来针对性加强不同的meta信息对于不同的输入图片,所以它输出的维度是和channle数一致的(#channel)；最后就是一个<strong>prediction module</strong>论文中用的就是yolo系列的检测框架来完成检测任务的，前面两个模块相乘之后新的feature map会作为<strong>prediction module</strong>的输入，具体的pipeline可以参考论文提供的这张图还是比较详细的：</p>
<p><img src="Few-shot-Object-Detection-via-Feature-Reweighting-截屏2020-03-0723.08.54.png" alt=""></p>
<p>那么接下来让我们看看这些模块以及最后的模型具体是怎么训练的，怎么work的，整个学习过程分为两个阶段:</p>
<ol>
<li><em>base training</em>, 用比较丰富的训练数据(base set)去学习比较好的meta extractor/reweight module/prediction module</li>
<li><em>few-shot fine-tuning</em>，用base set和novel set去一起学习，由于novel class的训练数据很少所以为了数据均衡，base class训练的时候采样和novel class保持一致，然后剩下来的过程就是<em>base training</em>阶段的就一摸一样了，只是训练时间会短很多。其实就是fine tune了。</li>
</ol>
<p>然后论文似乎主要的内容就都在这了，需要说明的几点：</p>
<ol>
<li><strong>meta extractor</strong>和<strong>prediction module</strong>虽然论文是分开说的，但应该是一体的，reweight参数修饰的feature map应该是backbone的输出，prediction module更多应该代表head部分，整个模型是基于yolov2的</li>
<li><strong>reweight module</strong>的输入是roi，也好理解毕竟是要突出具体的object，论文的做法是在rgb3个channel之上再append一个mask channel，有目标target的地方值为1其他地方值为0，在ablation里面还是解释一下直接抠图然后再append到原图和用mask这样做的差别，点上要高一点</li>
<li>论文里面还特意提了一句<strong>reweight module</strong>在inference的时候可以干掉，原因是如果我知道我将要检测哪个类别的数据，我只要把k个sample直接喂进去对weight vector取平均作为最后的weight vector，然后再inference，这个不要理解错，实际上这玩意还是去不掉的，本质和se没看到啥差别。</li>
</ol>
</div></div><a class="button-hover more" href="2020/02/28/Few-shot-Object-Detection-via-Feature-Reweighting/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/02/25/Few-shot-Adaptive-Faster-R-CNN/">Few-shot Adaptive Faster R-CNN</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-12</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Domain-Adaptation/">Domain Adaptation</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Few-Shot_Adaptive_Faster_R-CNN_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Few-Shot_Adaptive_Faster_R-CNN_CVPR_2019_paper.pdf</a></p>
<p>CVPR 2019的论文，这篇比较好的地方是在abstract部分就直接阐明了目前few shot问题存在的几个根本问题，我也比较认可论文提到的这几个痛点：</p>
<ol>
<li>target domain数据量很少，从任务本身来说就比较难，从一个domain transfer到另一个domain，在如此受限的数据情况下</li>
<li>同样也因为target domain数据量很少，导致过拟合问题是一个不得不处理的事情</li>
<li>few shot for detection目前来看实际工作不是很多，因为detection任务需要兼顾object定位和分类，所以任务本身上看比较难</li>
</ol>
<p>我们具体看下这篇论文是怎么做的，整体PPL看上去还是比较复杂的:</p>
<p><img src="Few-shot-Adaptive-Faster-R-CNN-屏幕快照 2020-02-28 下午2.41.16.png" alt=""></p>
<p>从大面上论文主要分成2个部分:</p>
<ol>
<li><strong>Image Level Domain Adaptation</strong>: 上图的上半部分就是Image Level Domain Adaptation部分，这里引入了一个SP操作（Split Pooling），这个主要是作者参考已有的论文结论觉得局部的patch可以更好的表示图片的特征。做法呢也比较简单，有一个初始框(w,h),随机再生成两个shift(δ<sub>w</sub>, δ<sub>h</sub>，这是相比较图像左上角的偏移)，这样就可以组成一个明确位置和大小的框了，然后因为是基于anchor的检测框架，所有这里的(w, h)是和anchor scale/anchor ratio保持一致的，论文里是给了9个框(3个ratio x 3个scale)，这样最后可以抽出9个patch的feature。那么论文的<strong>Pair Sampling</strong>则是GAN类似的对抗学习的过程，我们把这9个patch feature按照scale大小分成larege、medium、small三大部分，然后每一个部分分别来对抗学习，对于每一个部分组成两组pair:{s,s}, {s,t}(s代表source image,t代表target image)，然后GAN要学习的就是区分开这两者。</li>
<li><strong>Instance Level Domain Adaptation</strong>: 上图的中间部分可以理解为正常的Faster RCNN逻辑，而下半部分就可以理解为这里的Instance Level Domain Adaptation，这一部分的作用倒也可以理解，adaptation的粒度不一样，这里作者提了一个概念叫<em>Instance ROI Sampling</em>. 相比较普通的ROI Sampling主要的差别就是IOU卡的很高(&gt;0.7)这样得到的ROI更加贴近Install本身，另外就是所有的Positive都会送入到下一阶段做判断，而不是一般的RPN在处理的时候会控制前背景的比例，然后至于怎么去用GAN去区分就和<em>Image Level Domain Adaptation</em>里面提到的<strong>Pair Sampling</strong>逻辑是一摸一样的了。</li>
</ol>
<p>那么针对作者提出的过拟合问题，论文了也给了一个解法:<strong>Source Model Feature Regularization Training</strong>. 想法比较直接，现在论文提出的FAFRCNN方法是针对src + target domain的，作者另外训练一个source domain的detector，然后给定相同的souce domain的数据让这两个模型提取出来的特征尽可能相近：</p>
<p><img src="Few-shot-Adaptive-Faster-R-CNN-屏幕快照 2020-03-10 下午12.57.14.png" alt=""></p>
<p>然后论文的主要内容大概就这些，整体ppl有点太复杂不太实用。</p>
</div></div><a class="button-hover more" href="2020/02/25/Few-shot-Adaptive-Faster-R-CNN/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/02/22/Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation/">Context Contrasted Feature and Gated Multi-scale Aggregation for Scene Segmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-22</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ding_Context_Contrasted_Feature_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/Ding_Context_Contrasted_Feature_CVPR_2018_paper.pdf</a></p>
<p>CVPR2018的一篇语意分割的论文，整体读起来的感觉就是很复杂！主要内容的话还是围绕分割任务中context信息/local信息的获取问题，融合了CNN/RNN整个结构看起来相当的复杂。好在最后的点看上去还可以。</p>
<p><img src="Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation-截屏2020-02-2222.14.40.png" alt=""></p>
<p>作者首先阐述了针对语意分割需要解决的问题，比如上图是论文解释问题的一张图，对于pixel a那么很显然需要把它分类为car，但是在cnn处理的过程中通常整张图的特征会被占主体的一对父子所主导，因此往往对于pixel a的分类不是很准，所以需要特别注意提取pixel a的local特征，但是只有local特征很显然也是不行的，pixel a附近的context信息是很重要的起码可以形成明显的区分度帮助模型学习，所以论文核心解决的问题就是<strong>context信息/local信息的获取/使用问题</strong>。<br>先看下论文所提方法的pipeline然后在具体说一下novelty的点,直观上看就是在一般的分割pipeline基础上增加了一个ccl模块和g+(gated sum)模块,ccl用来处理论文提出的问题关于context和local信息的使用问题，gated sum可以理解为multi scale的加权问题，是对传统的multi scale通过sum/concat方法的优化:</p>
<p><img src="Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation-截屏2020-02-2222.19.47.png" alt=""></p>
<p>论文的两个novelty点:</p>
<ol>
<li><strong>Context Contrasted Local (CCL)</strong>: 简言之就是context和local信息相减来突出local信息同时不失context的信息(CCL = F<sub>l</sub>(F, Θ<sub>l</sub>) − F<sub>c</sub>(F, Θ<sub>c</sub>))，context信息利用dilation conv来解决，local信息就是普通的conv:</li>
</ol>
<p><img src="Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation-截屏2020-02-2222.23.21.png" alt=""><br>然后为了处理multi scale的问题多个ccl block存在一个级联的过程，因为conv操作本身多次重复之后就是对feature有multi scale的作用。</p>
<ol start="2">
<li><p><strong>Gated Sum</strong>: 这个操作作者提出来主要是考虑到一般的分割任务中对multi scale的feature通常采用sum或者concat的方法进行融合，作者claim对于不同的feature有可能一部分是有害的所以需要加以甄别，那么gated sum可以理解为一个对multi feature进行加权融合的过程，实现方法很复杂…用rnn做的，有兴趣的同学可以细看论文，这里不赘述，是RNN的常规应用，只是细节的确很是繁琐:</p>
<p><img src="Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation-截屏2020-02-2222.34.40.png" alt=""><br>最后点还是不错的，在coco上mean iou能到0.357，在pascal context上可以到0.516，另外看了下ablation，gated sum的操作还是可以带来2-3个点的涨幅的。</p>
</li>
</ol>
</div></div><a class="button-hover more" href="2020/02/22/Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/02/18/Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-28</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Label-Assign/">Label Assign</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1912.02424.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1912.02424.pdf</a></p>
<p>Label Assign另一篇比较有代表性的工作, 论文主要想探究Anchor-based方法和Anchor-free方法本质的差异性(这里主要考虑one stage的RetinaNet和FCOS)，结论是<strong>正负样本取样的差异性导致的</strong>。</p>
<p>作者首先对比了一下RetinaNet和FOCS点上面的差异，加上一堆trick之后，RetinaNet AP能到37.0%，而FCOS能到37.8%，之间有0.8%的GAP，那么就其本身这两个方法现在就剩两个不一样的地方了：</p>
<ol>
<li>正负样本定义不一样，RetinaNet是anchor-based，通过卡IOU来区分正负样本，每个pixel有多个不同的anchor，FCOS是基于点的，每个点有一个anchor point(正or负)，取决于gt框的大小和不同layer定义的回归scale。</li>
<li>回归的方式不一样，FCOS从<strong>anchor point</strong>回归，RetinaNet从<strong>anchor box</strong>回归。</li>
</ol>
<p>针对上面提到的不同的两点作者也做了一个实验，RetinaNet采用FCOS的策略可以把点从37.0%涨到37.8%，而如果FCOS采用RetinaNet的策略点就会从37.8%下降到36.9%：</p>
<p><img src="Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection-屏幕快照 2020-02-18 下午4.10.40.png" alt=""></p>
<p>那么横向看上面这张表，可以方面无论是基于anchor point回归还是基于anchor box回归点是差不多的，所以作者得出结论，RetinaNet和FCOS点有差的最大原因就是<strong>正负样本取样的差异性导致的</strong>.</p>
<p>然后就是论文的核心ATSS(Adaptive Training Sample Selection)策略了,解决<em>how to define positive and negative training samples</em>的问题.<br>具体怎么做呢：</p>
<ol>
<li>对于给定的gt框，在FPN的每一个layer上找Top K个离gt框中心最近的anchor box(距离的话就用两个框中心点的L2距离衡量)作为正样本，那么假设FPN有L个layer那么对于一个gt框就有L x K个postive正样本。</li>
<li>然后计算L x K个正样本anchor与gt框的IOU，统计出均值和标注差m和v，那么由此计算出给定gt框的IOU阈值为t = m + v. 那么最后就选择IoU大于等于t的作为正样本其余作为负样本。均值m可以控制某个layer anchor的质量，那么当然大于均值的anchor要更好了，v控制gt更加适合哪个layer。</li>
<li>在实际使用的时候(论文中的伪代码)限制选中的anchor box的中心点需要在gt框内部</li>
<li>如果一个anchor框可以和多个gt框匹配那么就去IOU最大的</li>
</ol>
<p>至于atss的解释论文给了比较多的阐述，实际也是FCOS做法的insight。最后点上面还是可以的。</p>
<p>具体算法的执行步骤论文中给的伪代码看上去会更加直接:</p>
<p><img src="Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection-屏幕快照 2020-02-20 上午11.44.01.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/02/18/Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/21/CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection/">CityPersons: A Diverse Dataset for Pedestrian Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1702.05693" target="_blank" rel="noopener">https://arxiv.org/abs/1702.05693</a></p>
<p>做行人检测比较经典的数据集了，CityPersons，数据集是基于cityscapes的数据集进行refine的，CityPersons选择了CityScapes中精标注的5000张图片进行标注的(来自欧洲27个城市)，所谓的精标注就可以理解为标准的instance segmentation的标注，共有30类的类别标签，per pixel标注。CityPersons只标注CityScapes中Person和Rider两类，并将两类进一步细分为：<strong>pedestrian</strong>(walking, running or standing up), <strong>rider</strong>(riding bi- cycles or motorbikes),<strong>sitting person</strong>,and <strong>other person</strong>(with unusual postures, e.g. stretching)。</p>
<p>标注方法可以参考下图，对于Pedestrian和Rider<strong>可见框就是seg标注的最小外接矩形</strong>,<strong>全身框的话则是先标头顶到两脚中间点的直线，然后按照固定的长宽ratio 0.41拓展出框的宽度从而完成整个全身框的标注</strong>，至于遮挡的比例那就是上述两个面积的比值，而至于<strong>其他两个类别则直接用的seg的最小外接矩形没有再标注全身框</strong>，而对于假人这样的object直接标为ignore：</p>
<p><img src="CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection-屏幕快照 2020-01-21 下午3.24.25.png" alt=""></p>
<p>数据集划分：</p>
<p><img src="CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection-屏幕快照 2020-01-21 下午3.43.44.png" alt=""></p>
<p>那么在后续的论文中其实我们还会比较常见Heavy，Bare，Partial这样的划分，这个划分是Repulsion Loss那篇论文中根据CityPersons的Reseanable子集继续细分出来的，具体的可以参考Repulsion Loss这篇论文，主要是根据遮挡程度来划分的。</p>
</div></div><a class="button-hover more" href="2020/01/21/CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/15/Object-as-Distribution/">Object as Distribution</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-09</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1907.12929" target="_blank" rel="noopener">https://arxiv.org/abs/1907.12929</a></p>
<p>NIPS 2019的一篇论文，论文的内容还是很新颖的，在检测领域传统的物体表示是通过框来表示，那么这种表示有一个很大的弊端是结合后处理NMS针对crowd的场景几乎是无解的，所以作者提出一个思考用一个框来定位一个物体是不是合理的一种表示方式，因此论文中作者提出了用分布来标志物体，作者给的几个sample还是比较有意思的，涵盖了作者claim的crowd的场景：</p>
<p><img src="Object-as-Distribution-屏幕快照 2020-01-15 下午10.33.53.png" alt=""></p>
<p>具体的话论文中选择用二元正态分布来描述一个物体，公式没什么特别的就是标准的二元正态分布的定义:</p>
<p><img src="Object-as-Distribution-屏幕快照 2020-01-20 下午9.57.09.png" alt=""></p>
<p>从公式中我们也可以看到，那么网络需要学习的有5个参数: μ<sub>xi</sub>,μ<sub>yi</sub>,σ<sub>xi</sub>,σ<sub>yi</sub>,ρ<sub>i</sub>, 这五个参数刚好可以恢复出一个分布出来，比如 μ<sub>xi</sub>、μ<sub>yi</sub>两个参数起码已经恢复出物体的中心来了,σ<sub>xi</sub>、σ<sub>yi</sub> 这两个参数又直接和形状相关，所以用分布来表示物体本身就是make sense的，同时因为分布可以严格区分开物体的中心，从逻辑上讲是有利于解遮挡的场景的。那么为了让学习的目标是和位置无关的，对于具体的学习目标作者进行了转换：m−μ<sub>xi</sub>,n−μ<sub>yi</sub>,logσ<sub>xi</sub>,logσ<sub>yi</sub>,tanh<sup>−1</sup>ρi,(m,n)代表一个处于某个物体内的pixel坐标。</p>
<p>那么接下来就说说如何来优化训练模型，首先对于分布来说自然而然就想到用KL散度来监督两个分布之间的距离，同时论文中基于DeepLab的框架选择以联合训练的方式来训练模型，那么总的监督loss就是L<sub>seg</sub> + L<sub>KL</sub> + L<sub>cls</sub>, 那么在具体训练的时候为了加速训练以及节省资源，作者选择downsample之后监督而不是再resize，所以inference的时候就需要有一个upsample的过程，这个过程无法对边缘点有一个很好的判断，有可能会把边缘点误判为另一个分布，所以作者借助(Mixure Density Network<sub>暂时没读过这篇论文</sub>)以一种bagging的逻辑对于一个object预测多个distribution，所以那个L<sub>cls</sub>就来自于这。</p>
<p>另外需要注意的一个细节是L<sub>KL</sub>为：</p>
<p><img src="Object-as-Distribution-屏幕快照 2020-01-20 下午10.56.04.png" alt=""></p>
<p>m<sub>rep</sub>为mask，只有当cls值最高或者KL loss最小的那个分布mask才为1否则都为0！用作者论文中的话说，模型希望最好的distribution(KL Loss最低和目前选择的distribtion(cls值最大)越接近目标distribution！</p>
<p>其他论文就没有啥注意的了，NMS最后用KL距离就好。<br>论文另外有一个比较大的槽点就是实际上最后的点很低！！！</p>
</div></div><a class="button-hover more" href="2020/01/15/Object-as-Distribution/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/07/FCOS-Fully-Convolutional-One-Stage-Object-Detection/">FCOS: Fully Convolutional One-Stage Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-05-07</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classic/">Classic</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1904.01355.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.01355.pdf</a></p>
<p>2019年备受推崇的一篇anchor free的论文，最后也应该中了ICCV2019，但是对于熟悉检测领域的同学来说，看到这篇论文应该略有眼熟，这篇论文其实和Densebox应该属于一脉相承，都希望利用FCN的逻辑统一检测/分割等任务。</p>
<p>FCOS的具体想法呢是这样的，基于FPN的结构，P3 - P7得到的feature map假设分别是H<sub>i</sub> x W<sub>i</sub> x C<sub>i</sub>，那么基于H x W这么多个pixel，每个pixel都作为一个中心点去回归一个目标bbox，每个bbox的定义同样包含5个值，一个是分类score，一个是回归offset，只是这个offset的值当前这个pixel距离gt框四条边的具体(和anchor based模型一样，feature map的pixel gt的计算只需要按stride映射会原图就好)，具体的示意图如下：</p>
<p><img src="FCOS-Fully-Convolutional-One-Stage-Object-Detection-屏幕快照 2020-01-06 上午12.20.12.png" alt=""></p>
<p>几个需要注意的点:</p>
<ol>
<li>gt的生成，对于feature map上的某个pixel (x, y), 如果(x, y) 映射到原图的点(x<sup>‘</sup>, y<sup>‘</sup>)落在了某个gt框里，那么对应的offset就算(x<sup>‘</sup>, y<sup>‘</sup>)到gt框四条边的offset，分类gt也就沿用这个gt框的class标注。</li>
<li>FPN的好处一是可以fuse feature另外一个不同的layer可以针对性的回归不同scale的框，那么在FCOS中这部分是怎么做的呢，论文将P3 - P7 5个FPN层用6个值进行区间划分 ，论文中是用的区间m = [0, 64, 128, 256, 512 , ∞]，对于一个gt: (l<sup>∗</sup>, t<sup>∗</sup>, r<sup>∗</sup> ,b<sup>∗</sup>),如果满足max(l<sup>∗</sup>, t<sup>∗</sup>, r<sup>∗</sup> ,b<sup>∗</sup>) &gt; m<sub>i</sub> or max(l<sup>∗</sup>, t<sup>∗</sup>, r<sup>∗</sup> ,b<sup>∗</sup>) &lt; m<sub>i - 1</sub>，那么P<sub>3 + i</sub>就会将这个gt视为negative sample，这层不负责回归这个gt框。这么做其实会有一个问题，对于靠近gt框边缘的点有可能会落到其他layer上(也就是一个gt框内所有的pixel不一定都在一个layer里)这其实在某种程度上违背了FPN的初衷，只是在具体的实现的时候似乎可以卡中心的一些ratio来人为的干掉边缘pixel。</li>
<li>FPN另一个好处可以缓解一个pixel对目标回归的不确定性，比如下图，手拿网球拍的运动员，小的蓝色框的大部分pixel同时也落在来橙色的人体框中，这就导致一个问题，这些overlap的pixel具体需要负责去回归哪个框，通过FPN上述的分层处理可以大大缓解这个问题，论文的ablation里是有具体的数据的，感兴趣的同学可以参考原论文，那么假设在这样的情况下还是有少数不确定的pixel，那么这些pixel就负责回顾最小的那个框！</li>
</ol>
<p><img src="FCOS-Fully-Convolutional-One-Stage-Object-Detection-屏幕快照 2020-01-03 下午10.04.06.png" alt=""></p>
<p>通过上面的各种优化之后作者发现FCOS相比较anchor based的方法指标上还是有差距，问题主要是一些低质量的框环绕在gt周围（应该都是gt的边缘pixel产生的),那么为了解决这个问题，作者在cls和reg分支的基础上平行的增加了一个centerness分支，用来预测当前location(anchor point)想较gt中心的距离，centerness的gt定义如下：</p>
<p><img src="FCOS-Fully-Convolutional-One-Stage-Object-Detection-截屏2020-01-1200.31.54.png" alt=""></p>
<p>我们来看一下这个表达式，l<sup>*</sup>和r<sup>*</sup>，t<sup>*</sup>和b<sup>*</sup>是两对相互关联的变量，如果某个pixel越靠近中心点(center)那么这两对值就会越接近，那么center-ness值就会越趋向于1，如果某个pixel越远离中心点(center)，那么center-ness就会越趋向于0.所以可以理解为center-ness是一个度量离中心点越近的单位，inference的时候这个center-ness分支的结果会加权于score从而约束了偏离中心点的pixel，也抑制了大量低质量的框。因为上述的公式将centerness的值约束到了(0,1)范围内，所以最后centerness分支的学习就是用的binary cross entropy (BCE) Loss.</p>
<p>论文整体的内容应该就这些了，FCOS对后续的anchor free做法还是很有启发意义的。</p>
</div></div><a class="button-hover more" href="2020/01/07/FCOS-Fully-Convolutional-One-Stage-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/03/Focal-Loss-for-Dense-Object-Detection/">Focal Loss for Dense Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classic/">Classic</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf</a></p>
<p>重读经典系列第三篇：RetinaNet</p>
<p>ICCV 2017的Best Student Paper,也是 He Kaiming一篇很有代表性的工作，论文主要focus在One Stage Detector中样本不均衡这件事上，并且提出了<strong>Focal Loss</strong>来解决这样的问题，同时基于Focal Loss实现了一个One Stage Detector <strong>RetinaNet</strong>,可以达到Two Stage Detector的精度同时可以保持One Stage Detector的速度。</p>
<p>我们知道Two Stage Detector精度高速度慢，One Stage Detector速度快精度低，这几乎是所有人可以脱口而出的特性，那么作者认为One Stage Detector精度低的主要原因就是非常严重的class imbalance问题，基于anchor的检测器动则有10W+的anchor数目，其中Positive的anchor只有几十个，这样正负样本比几乎可以达到1:1000，大量的负样本中有很多的easy negative samples它们对模型的训练几乎无法贡献有效的信息,同时大量的这种样本本身对模型的训练也是很有害的，毕竟它们占据主要部分容易主导模型的训练。因此作者提出了Focal Loss：<br><strong>FL(p<sub>t</sub>) = −α<sub>t</sub>(1 − p<sub>t</sub>)<sup>γ</sup> log(p<sub>t</sub>)</strong><br>那么Focal Loss本身呢是来自于Cross Entropy Loss(以二分类为例):<br><img src="Focal-Loss-for-Dense-Object-Detection-屏幕快照 2020-01-03 下午8.14.46.png" alt=""><br>稍微简化一下：<br><img src="Focal-Loss-for-Dense-Object-Detection-屏幕快照 2020-01-03 下午8.15.32.png" alt=""><br>那么CE(p, y) = CE(p<sub>t</sub>) = − log(p<sub>t</sub>)<br>那么我们再来看看Focal Loss在CE Loss基础上增加的东西：</p>
<ol>
<li>−α<sub>t</sub>: 这就是简单的一个类别权重，比如可以将正样本的权重加大也是缓解class imbalance的一个选择</li>
<li>(1 − p<sub>t</sub>)<sup>γ</sup> : 这个可以理解为Focal Loss的核心吧，会整体通过模型的预测值动态的去调整loss的权重，如果某一个sample模型预测的类别是错误的那就意味着p<sub>t</sub>值会比较小（注意看p<sub>t</sub>的定义，对于每一个类别都是如此），那么Focal Loss整体就会和CE Loss差不多不会有什么影响，如果一个easy sample可以被模型很好的分类那么意味着p<sub>t</sub>值会比较大，那么Loss的权重就会变小从而优化过程中不会刻意处理，因此整个模型训练过程中都会刻意去优化hard sample。其中γ是平滑系数，论文中通过尝试γ = 2效果会比较好.</li>
</ol>
<p>至于论文中提到的RetinaNet整体其实没有什么特殊的，具体结构如下，是一个FPN的结构：<br><img src="Focal-Loss-for-Dense-Object-Detection-屏幕快照 2020-01-03 下午8.42.36.png" alt=""><br>需要注意的是:</p>
<ol>
<li>class subnet 和 box subnet参数在P3 - P7之间是共享的</li>
<li>P3 - P5通过Conv来源于C3 - C5，P6通过Conv来源于P5，P7通过Conv来源于P6</li>
<li>P3 - P7的结果会concat到一起最后一起NMS</li>
<li>为了训练的稳定性初始化有一些trick具体可以参考原论文</li>
</ol>
</div></div><a class="button-hover more" href="2020/01/03/Focal-Loss-for-Dense-Object-Detection/#more">阅读全文</a></div></div><div id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="page/5/">5</a><a class="extend next" rel="next" href="page/2/"><i class="fas fa-angle-right"></i></a></div></div></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fas fa-user"></i></span><span id="busuanzi_value_site_uv"></span><span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fas fa-eye"></i></span><span id="busuanzi_value_site_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Out of Memory</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--></body></html>