<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Live and Learn"><meta name="keywords" content=""><meta name="author" content="Out of Memory,undefined"><meta name="copyright" content="Out of Memory"><title>Live and Learn【Out of Memory】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="author-info"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Out of Memory</div><div class="author-info-description">Live and Learn</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/libanghuai" target="_blank">GitHub<i class="icon-dot bg-color8"></i></a><a class="links-button button-hover" href="mailto:libanghuai@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color0"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1185719433&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color7"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="archives"><span class="pull-top">日志</span><span class="pull-bottom">99</span></a><a class="author-info-articles-tags article-meta" href="tags"><span class="pull-top">标签</span><span class="pull-bottom">34</span></a><a class="author-info-articles-categories article-meta" href="categories"><span class="pull-top">分类</span><span class="pull-bottom">2</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Out of Memory</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><div id="recent-posts"><!-- each post in page.posts.sort('date', -1).limit(10).toArray()--><!-- config中配置按照什么排序--><div class="recent-post-item"><a class="post-title" href="2020/02/18/Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Label-Assign/">Label Assign</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1912.02424.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1912.02424.pdf</a></p>
<p>Label Assign另一篇比较有代表性的工作, 论文主要想探究Anchor-based方法和Anchor-free方法本质的差异性(这里主要考虑one stage的RetinaNet和FCOS)，结论是<strong>正负样本取样的差异性导致的</strong>。</p>
<p>作者首先对比了一下RetinaNet和FOCS点上面的差异，加上一堆trick之后，RetinaNet AP能到37.0%，而FCOS能到37.8%，之间有0.8%的GAP，那么就其本身这两个方法现在就剩两个不一样的地方了：</p>
<ol>
<li>正负样本定义不一样，RetinaNet是anchor-based，通过卡IOU来区分正负样本，每个pixel有多个不同的anchor，FCOS是基于点的，每个点有一个anchor point(正or负)，取决于gt框的大小和不同layer定义的回归scale。</li>
<li>回归的方式不一样，FCOS从<strong>anchor point</strong>回归，RetinaNet从<strong>anchor box</strong>回归。</li>
</ol>
<p>针对上面提到的不同的两点作者也做了一个实验，RetinaNet采用FCOS的策略可以把点从37.0%涨到37.8%，而如果FCOS采用RetinaNet的策略点就会从37.8%下降到36.9%：</p>
<p><img src="Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection-屏幕快照 2020-02-18 下午4.10.40.png" alt=""></p>
<p>那么横向看上面这张表，可以方面无论是基于anchor point回归还是基于anchor box回归点是差不多的，所以作者得出结论，RetinaNet和FCOS点有差的最大原因就是<strong>正负样本取样的差异性导致的</strong>.</p>
<p>然后就是论文的核心ATSS(Adaptive Training Sample Selection)策略了,解决<em>how to define positive and negative training samples</em>的问题.<br>具体怎么做呢：</p>
<ol>
<li>对于给定的gt框，在FPN的每一个layer上找Top K个离gt框中心最近的anchor box(距离的话就用两个框中心点的L2距离衡量)作为正样本，那么假设FPN有L个layer那么对于一个gt框就有L x K个postive正样本。</li>
<li>然后计算L x K个正样本anchor与gt框的IOU，统计出均值和标注差m和v，那么由此计算出给定gt框的IOU阈值为t = m + v. 那么最后就选择IoU大于等于t的作为正样本其余作为负样本。</li>
<li>在实际使用的时候(论文中的伪代码)限制选中的anchor box的中心点需要在gt框内部</li>
<li>如果一个anchor框可以和多个gt框匹配那么就去IOU最大的</li>
</ol>
<p>至于atss的解释论文给了比较多的阐述，实际也是FCOS做法的insight。最后点上面还是可以的。</p>
</div></div><a class="button-hover more" href="2020/02/18/Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/02/11/FreeAnchor-Learning-to-Match-Anchors-for-Visual-Object-Detection/">FreeAnchor: Learning to Match Anchors for Visual Object Detection </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-18</time></div><div class="post-content"><div class="main-content content"></div></div><a class="button-hover more" href="2020/02/11/FreeAnchor-Learning-to-Match-Anchors-for-Visual-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/21/CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection/">CityPersons: A Diverse Dataset for Pedestrian Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1702.05693" target="_blank" rel="noopener">https://arxiv.org/abs/1702.05693</a></p>
<p>做行人检测比较经典的数据集了，CityPersons，数据集是基于cityscapes的数据集进行refine的，CityPersons选择了CityScapes中精标注的5000张图片进行标注的(来自欧洲27个城市)，所谓的精标注就可以理解为标准的instance segmentation的标注，共有30类的类别标签，per pixel标注。CityPersons只标注CityScapes中Person和Rider两类，并将两类进一步细分为：<strong>pedestrian</strong>(walking, running or standing up), <strong>rider</strong>(riding bi- cycles or motorbikes),<strong>sitting person</strong>,and <strong>other person</strong>(with unusual postures, e.g. stretching)。</p>
<p>标注方法可以参考下图，对于Pedestrian和Rider<strong>可见框就是seg标注的最小外接矩形</strong>,<strong>全身框的话则是先标头顶到两脚中间点的直线，然后按照固定的长宽ratio 0.41拓展出框的宽度从而完成整个全身框的标注</strong>，至于遮挡的比例那就是上述两个面积的比值，而至于<strong>其他两个类别则直接用的seg的最小外接矩形没有再标注全身框</strong>，而对于假人这样的object直接标为ignore：</p>
<p><img src="CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection-屏幕快照 2020-01-21 下午3.24.25.png" alt=""></p>
<p>数据集划分：</p>
<p><img src="CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection-屏幕快照 2020-01-21 下午3.43.44.png" alt=""></p>
<p>那么在后续的论文中其实我们还会比较常见Heavy，Bare，Partial这样的划分，这个划分是Repulsion Loss那篇论文中根据CityPersons的Reseanable子集继续细分出来的，具体的可以参考Repulsion Loss这篇论文，主要是根据遮挡程度来划分的。</p>
</div></div><a class="button-hover more" href="2020/01/21/CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/15/Object-as-Distribution/">Object as Distribution</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-09</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1907.12929" target="_blank" rel="noopener">https://arxiv.org/abs/1907.12929</a></p>
<p>NIPS 2019的一篇论文，论文的内容还是很新颖的，在检测领域传统的物体表示是通过框来表示，那么这种表示有一个很大的弊端是结合后处理NMS针对crowd的场景几乎是无解的，所以作者提出一个思考用一个框来定位一个物体是不是合理的一种表示方式，因此论文中作者提出了用分布来标志物体，作者给的几个sample还是比较有意思的，涵盖了作者claim的crowd的场景：</p>
<p><img src="Object-as-Distribution-屏幕快照 2020-01-15 下午10.33.53.png" alt=""></p>
<p>具体的话论文中选择用二元正态分布来描述一个物体，公式没什么特别的就是标准的二元正态分布的定义:</p>
<p><img src="Object-as-Distribution-屏幕快照 2020-01-20 下午9.57.09.png" alt=""></p>
<p>从公式中我们也可以看到，那么网络需要学习的有5个参数: μ<sub>xi</sub>,μ<sub>yi</sub>,σ<sub>xi</sub>,σ<sub>yi</sub>,ρ<sub>i</sub>, 这五个参数刚好可以恢复出一个分布出来，比如 μ<sub>xi</sub>、μ<sub>yi</sub>两个参数起码已经恢复出物体的中心来了,σ<sub>xi</sub>、σ<sub>yi</sub> 这两个参数又直接和形状相关，所以用分布来表示物体本身就是make sense的，同时因为分布可以严格区分开物体的中心，从逻辑上讲是有利于解遮挡的场景的。那么为了让学习的目标是和位置无关的，对于具体的学习目标作者进行了转换：m−μ<sub>xi</sub>,n−μ<sub>yi</sub>,logσ<sub>xi</sub>,logσ<sub>yi</sub>,tanh<sup>−1</sup>ρi,(m,n)代表一个处于某个物体内的pixel坐标。</p>
<p>那么接下来就说说如何来优化训练模型，首先对于分布来说自然而然就想到用KL散度来监督两个分布之间的距离，同时论文中基于DeepLab的框架选择以联合训练的方式来训练模型，那么总的监督loss就是L<sub>seg</sub> + L<sub>KL</sub> + L<sub>cls</sub>, 那么在具体训练的时候为了加速训练以及节省资源，作者选择downsample之后监督而不是再resize，所以inference的时候就需要有一个upsample的过程，这个过程无法对边缘点有一个很好的判断，有可能会把边缘点误判为另一个分布，所以作者借助(Mixure Density Network<sub>暂时没读过这篇论文</sub>)以一种bagging的逻辑对于一个object预测多个distribution，所以那个L<sub>cls</sub>就来自于这。</p>
<p>另外需要注意的一个细节是L<sub>KL</sub>为：</p>
<p><img src="Object-as-Distribution-屏幕快照 2020-01-20 下午10.56.04.png" alt=""></p>
<p>m<sub>rep</sub>为mask，只有当cls值最高或者KL loss最小的那个分布mask才为1否则都为0！用作者论文中的话说，模型希望最好的distribution(KL Loss最低和目前选择的distribtion(cls值最大)越接近目标distribution！</p>
<p>其他论文就没有啥注意的了，NMS最后用KL距离就好。<br>论文另外有一个比较大的槽点就是实际上最后的点很低！！！</p>
</div></div><a class="button-hover more" href="2020/01/15/Object-as-Distribution/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/07/FCOS-Fully-Convolutional-One-Stage-Object-Detection/">FCOS: Fully Convolutional One-Stage Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-12</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classic/">Classic</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1904.01355.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.01355.pdf</a></p>
<p>2019年备受推崇的一篇anchor free的论文，最后也应该中了ICCV2019，但是对于熟悉检测领域的同学来说，看到这篇论文应该略有眼熟，这篇论文其实和Densebox应该属于一脉相承，都希望利用FCN的逻辑统一检测/分割等任务。</p>
<p>FCOS的具体想法呢是这样的，基于FPN的结构，P3 - P7得到的feature map假设分别是H<sub>i</sub> x W<sub>i</sub> x C<sub>i</sub>，那么基于H x W这么多个pixel，每个pixel都作为一个中心点去回归一个目标bbox，每个bbox的定义同样包含5个值，一个是分类score，一个是回归offset，只是这个offset的值当前这个pixel距离gt框四条边的具体(和anchor based模型一样，feature map的pixel gt的计算只需要按stride映射会原图就好)，具体的示意图如下：</p>
<p><img src="FCOS-Fully-Convolutional-One-Stage-Object-Detection-屏幕快照 2020-01-06 上午12.20.12.png" alt=""></p>
<p>几个需要注意的点:</p>
<ol>
<li>gt的生成，对于feature map上的某个pixel (x, y), 如果(x, y) 映射到原图的点(x<sup>‘</sup>, y<sup>‘</sup>)落在了某个gt框里，那么对应的offset就算(x<sup>‘</sup>, y<sup>‘</sup>)到gt框四条边的offset，分类gt也就沿用这个gt框的class标注。</li>
<li>FPN的好处一是可以fuse feature另外一个不同的layer可以针对性的回归不同scale的框，那么在FCOS中这部分是怎么做的呢，论文将P3 - P7 5个FPN层用6个值进行区间划分 ，论文中是用的区间m = [0, 64, 128, 256, 512 , ∞]，对于一个gt: (l<sup>∗</sup>, t<sup>∗</sup>, r<sup>∗</sup> ,b<sup>∗</sup>),如果满足<br>max(l<sup>∗</sup>, t<sup>∗</sup>, r<sup>∗</sup> ,b<sup>∗</sup>) &gt; m<sub>i</sub> or max(l<sup>∗</sup>, t<sup>∗</sup>, r<sup>∗</sup> ,b<sup>∗</sup>) &lt; m<sub>i - 1</sub>，那么P<sub>3 + i</sub>就会将这个gt视为negative sample，这层不负责回归这个gt框。这么做其实会有一个问题，对于靠近gt框边缘的点有可能会落到其他layer上(也就是一个gt框内所有的pixel不一定都在一个layer里)这其实在某种程度上违背了FPN的初衷，只是在具体的实现的时候似乎可以卡中心的一些ratio来人为的干掉边缘pixel。</li>
<li>FPN另一个好处可以缓解一个pixel对目标回归的不确定性，比如下图，手拿网球拍的运动员，小的蓝色框的大部分pixel同时也落在来橙色的人体框中，这就导致一个问题，这些overlap的pixel具体需要负责去回归哪个框，通过FPN上述的分层处理可以大大缓解这个问题，论文的ablation里是有具体的数据的，感兴趣的同学可以参考原论文，那么假设在这样的情况下还是有少数不确定的pixel，那么这些pixel就负责回顾最小的那个框！</li>
</ol>
<p><img src="FCOS-Fully-Convolutional-One-Stage-Object-Detection-屏幕快照 2020-01-03 下午10.04.06.png" alt=""></p>
<p>论文的最后一段另外还提出了center-ness loss的概念，主要是想解决在具体实验中发现的FCOS会产生大量低质量的框环绕在gt周围（应该都是gt的边缘pixel产生的），因此提出了center-ness loss的概念：</p>
<p><img src="FCOS-Fully-Convolutional-One-Stage-Object-Detection-截屏2020-01-1200.31.54.png" alt=""></p>
<p>我们来看一下这个loss，l<sup>*</sup>和r<sup>*</sup>，t<sup>*</sup>和b<sup>*</sup>是两对相互关联的变量，如果某个pixel越靠近中心点(center)那么这两对值就会越接近，那么center-ness loss就会越趋向于1，如果某个pixel越远离中心点(center)，那么center-ness loss就会越趋向于0.所以可以理解为center-ness是一个度量离中心点越近的单位，inference的时候这个center-ness分支的结果会加权于score从而约束了偏离中心点的pixel，也抑制了大量低质量的框。</p>
<p>论文整体的内容应该就这些了，FCOS对后续的anchor free做法还是很有启发意义的。</p>
</div></div><a class="button-hover more" href="2020/01/07/FCOS-Fully-Convolutional-One-Stage-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/03/Focal-Loss-for-Dense-Object-Detection/">Focal Loss for Dense Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classic/">Classic</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf</a></p>
<p>重读经典系列第三篇：RetinaNet</p>
<p>ICCV 2017的Best Student Paper,也是 He Kaiming一篇很有代表性的工作，论文主要focus在One Stage Detector中样本不均衡这件事上，并且提出了<strong>Focal Loss</strong>来解决这样的问题，同时基于Focal Loss实现了一个One Stage Detector <strong>RetinaNet</strong>,可以达到Two Stage Detector的精度同时可以保持One Stage Detector的速度。</p>
<p>我们知道Two Stage Detector精度高速度慢，One Stage Detector速度快精度低，这几乎是所有人可以脱口而出的特性，那么作者认为One Stage Detector精度低的主要原因就是非常严重的class imbalance问题，基于anchor的检测器动则有10W+的anchor数目，其中Positive的anchor只有几十个，这样正负样本比几乎可以达到1:1000，大量的负样本中有很多的easy negative samples它们对模型的训练几乎无法贡献有效的信息,同时大量的这种样本本身对模型的训练也是很有害的，毕竟它们占据主要部分容易主导模型的训练。因此作者提出了Focal Loss：<br><strong>FL(p<sub>t</sub>) = −α<sub>t</sub>(1 − p<sub>t</sub>)<sup>γ</sup> log(p<sub>t</sub>)</strong><br>那么Focal Loss本身呢是来自于Cross Entropy Loss(以二分类为例):<br><img src="Focal-Loss-for-Dense-Object-Detection-屏幕快照 2020-01-03 下午8.14.46.png" alt=""><br>稍微简化一下：<br><img src="Focal-Loss-for-Dense-Object-Detection-屏幕快照 2020-01-03 下午8.15.32.png" alt=""><br>那么CE(p, y) = CE(p<sub>t</sub>) = − log(p<sub>t</sub>)<br>那么我们再来看看Focal Loss在CE Loss基础上增加的东西：</p>
<ol>
<li>−α<sub>t</sub>: 这就是简单的一个类别权重，比如可以将正样本的权重加大也是缓解class imbalance的一个选择</li>
<li>(1 − p<sub>t</sub>)<sup>γ</sup> : 这个可以理解为Focal Loss的核心吧，会整体通过模型的预测值动态的去调整loss的权重，如果某一个sample模型预测的类别是错误的那就意味着p<sub>t</sub>值会比较小（注意看p<sub>t</sub>的定义，对于每一个类别都是如此），那么Focal Loss整体就会和CE Loss差不多不会有什么影响，如果一个easy sample可以被模型很好的分类那么意味着p<sub>t</sub>值会比较大，那么Loss的权重就会变小从而优化过程中不会刻意处理，因此整个模型训练过程中都会刻意去优化hard sample。其中γ是平滑系数，论文中通过尝试γ = 2效果会比较好.</li>
</ol>
<p>至于论文中提到的RetinaNet整体其实没有什么特殊的，具体结构如下，是一个FPN的结构：<br><img src="Focal-Loss-for-Dense-Object-Detection-屏幕快照 2020-01-03 下午8.42.36.png" alt=""><br>需要注意的是:</p>
<ol>
<li>class subnet 和 box subnet参数在P3 - P7之间是共享的</li>
<li>P3 - P5通过Conv来源于C3 - C5，P6通过Conv来源于P5，P7通过Conv来源于P6</li>
<li>P3 - P7的结果会concat到一起最后一起NMS</li>
<li>为了训练的稳定性初始化有一些trick具体可以参考原论文</li>
</ol>
</div></div><a class="button-hover more" href="2020/01/03/Focal-Loss-for-Dense-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/30/ACNet-Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-Asymmetric-Convolution-Blocks/">ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1908.03930" target="_blank" rel="noopener">https://arxiv.org/abs/1908.03930</a></p>
<p>ICCV2019的一篇工作,论文提出了Asymmetric Convolutional Block这样一个新的模块，出发点是作者发现对于我们常用的dxd的卷积核来说通常对于效果影响最大的是中心十字架状的skeleton：</p>
<p><img src="ACNet-Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-Asymmetric-Convolution-Blocks-截屏2019-12-3022.14.00.png" alt=""></p>
</div></div><a class="button-hover more" href="2019/12/30/ACNet-Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-Asymmetric-Convolution-Blocks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/23/MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning/">MIC: Mining Interclass Characteristics for Improved Metric Learning </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Metric-Learning/">Metric Learning</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Roth_MIC_Mining_Interclass_Characteristics_for_Improved_Metric_Learning_ICCV_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2019/papers/Roth_MIC_Mining_Interclass_Characteristics_for_Improved_Metric_Learning_ICCV_2019_paper.pdf</a></p>
<p>ICCV2019一篇关于Metric Learning的论文，Metric Learning相关的工作是在做特征表示相关的内容，这篇论文的出发点是从object特征中去除非特征主体的特征从而能对于后续的任务更好的学习。论文不是很好懂需要仔细琢磨。</p>
<p><img src="MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning-截屏2019-12-2422.49.45.png" alt=""></p>
<p>对于一般的分类任务我们通常会把特征分成inter-class类间特征、intra-class类内特征, 那么这篇论文中类似，inter-class特征可以解释为区分特征主体的特征，比如区分狗和猫，intra-class特征可以解释为通用的一些特征，比如光照、角度等。对于一般的分类模型通常会忽略intra-class的特征直接用gt label来监督模型训练强行区分不同的类别。<br>那么本论文主要想细分这两个特征从而更精确的分类，假设输入图为img，f为抽特征的CNN，那么img的特征可以表示为f(img), 对于一般的metric learning通常会用一个encoder E在f(img)的基础上进一步对特征进行映射，从而基于E(f(img))做进一步的处理，比如计算相似度等，这个E的输出也可以理解为对这个img在高维特征空间的embedding。</p>
<p>类别一般的metric learning，本论文有两个encoder， encoder E<sub>α</sub> 编码inter-class特征，这个学习很容易，直接用gt的label监督就好，另一个encoder E<sub>β</sub> 编码intra-class特征，这部分不是很容易学习因为没有gt.那么论文中是怎么做的呢？</p>
<ol>
<li>首先对于类别n的所有的图片，计算每一张图的img的f(img)特征值，f通常是在ImageNet上pretrain过的某个model</li>
<li>计算所有f(img<sub>i</sub>)的mean和standard deviation（标准差）</li>
<li>最后利用公式Z<sub>i</sub> = (f(img<sub>i</sub>) - mean<sub>n</sub>) / StandardDeviation<sub>n</sub>进行特征转化</li>
</ol>
<p>这样做的目的是因为即使多张图片具体相同的intra-class特征，但是由于拍摄地点、拍摄行为等因素还是会导致不太一样，所以通过一个基本的转化得到的Z集合一定程度上消除了这种bias。</p>
<p>那么所有的图片经过上述的处理之后对于特征集合Z就可以聚类了，假设可以聚成K类{C<sub>1</sub>……C<sub>k</sub>},那么C集合就可以作为E<sub>β</sub>训练的label了！你可以把C集合想像成角度、光照、遮挡等一系列属性。</p>
<p>因为E<sub>β</sub>和E<sub>α</sub>共享f并且end2end训练，所以两者之前会相互影响，两者学习的特征也难免会有overlap，那么为了限制两者分别去学习inter-class类间特征和intra-class类内特征，作者也做了一些优化（ Minimizing Mutual Information）。具体可以参考下面这个公式，R是一个小网络用来将E<sub>β</sub>编码的信息映射到E<sub>α</sub>的空间上，⊙就是普通的element wise乘，r代表梯度反转，是对抗学习中常用的一种方式，和实际的需求也比较接近：</p>
<p><img src="MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning-截屏2019-12-2423.35.20.png" alt=""></p>
</div></div><a class="button-hover more" href="2019/12/23/MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/22/High-level-Semantic-Feature-Detection-A-New-Perspective-for-Pedestrian-Detection/">High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Face/">Face</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_High-Level_Semantic_Feature_Detection_A_New_Perspective_for_Pedestrian_Detection_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_High-Level_Semantic_Feature_Detection_A_New_Perspective_for_Pedestrian_Detection_CVPR_2019_paper.pdf</a><br>这是CVPR2019的行人检测论文，和Center and Scale Prediction: A Box-free Approach for Object Detection是同一篇论文，倒是后者的标题更能体现出论文所提的方法，论文主要就是异于Anchor Based的方法提出了预测中心点+框的scale的新方法来解决行人检测的问题或者严格来讲解决一般刚性物体的检测问题. 从行人检测的角度来说是一个比较新颖也比较值得去思考的方法。下图是它整体的Pipeline：</p>
<p><img src="High-level-Semantic-Feature-Detection-A-New-Perspective-for-Pedestrian-Detection-屏幕快照 2019-12-22 下午4.35.00.png" alt=""></p>
<p>论文所提的方法三言两语倒是可以说出大概，但是一些细节还是值得去琢磨的：</p>
<ol>
<li>模型结构很简单，Res50，stage 2 - 5通过deconv将高层feature map统一到一个固定的resolution然后concat到一起，其实比较类似FPN了，r倍downsample之后接一层Conv3x3然后再分别接两个Conv1x1引出两个branch，一个预测中心点，一个预测Scale，两者都是用heatmap出.</li>
<li>模型结构很简单，比较重要的就是gt是怎么生成的以及如何去监督了，首先说center的heatmap，它的大小（w x h）和downsample之后的feature map维度是一致的, 给定一个bounding box可以得到一个标注的中心，那么原则上只有这个点的gt为1，其余位置gt都为0，那么论文中为了更加符合实际场景同时也更加便于训练，用了一个比较常用的高斯分布来生成gt。而至于scale map，对于给定的中心点，它的取值是log(h),h为boundingbox的高度，这样可以通过事先定义的ratio反算出具体的框。同样的在实际生成gt的时候会围绕中心点半径2的范围内都置为log(h)其余为0；</li>
<li>Loss方面center的定位可以看为classification任务所以用ce loss，而scale的预测可以理解为regression任务所以用smoothL1 loss。</li>
<li>因为gt的监督是基于r倍downsample的所以为了更精确的预测会自然想到加一个offset分支来更精确的回归：<br>L = λ<sub>c</sub>L<sub>center</sub> + λ<sub>s</sub>L<sub>scale</sub> + λ<sub>o</sub>L<sub>offset</sub></li>
</ol>
<p>最后就是这篇论文刷的点还是比较高的，也会是做检测比较好的一个方向。</p>
</div></div><a class="button-hover more" href="2019/12/22/High-level-Semantic-Feature-Detection-A-New-Perspective-for-Pedestrian-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/22/Bi-box-Regression-for-Pedestrian-Detection-and-Occlusion-Estimation/">Bi-box Regression for Pedestrian Detection and Occlusion Estimation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf</a></p>
<p>ECCV2018的一篇行人检测的问题，论文的做法其实比较简单，就是让模型在学习全身框的同时也出可见框的结果，两者可以起到互补的作用，整体模型结构也比较简单就是在backbone之后接两个平行的业务层，一个出可见框的结果一个出全身框的结构，两个平行业务层的结构是一致的，具体的逻辑可以参考论文里的下面这张图：</p>
<p><img src="Bi-box-Regression-for-Pedestrian-Detection-and-Occlusion-Estimation-屏幕快照 2019-12-22 下午2.40.42.png" alt=""></p>
<p>那么这篇论文需要注意的更多的是一些细节：</p>
<ol>
<li>虽然模型最后是出两个平行的业务层一个出可见框的结果一个出全身框的结果，但是两者的proposal或者anchor是一摸一样的！假设目前对于同一个标注的gt，可见框为Box<sub>visible</sub>,全身框为Box<sub>full body</sub>,那么对于个给定的proposal P, 当IoU(P, Box<sub>full body</sub>) &gt; $\alpha$ &amp;&amp; IoB(P, Box<sub>visible</sub>) &gt;  $\beta$ 是P为正样本，否则P就是负样本。</li>
<li>因为两个branch用的是同一个proposal，然后基于这个proposal去分别回归offset，那么最后在后处理的时候这个proposal就只能有一个score，作者给了三个方法，第一只考虑可见分支，fc出的结果来个softmax就好，第二是只考虑全身分支，同样fc出的结果来个softmax就好，第三就是融合两个分支的结果把两个fc的结果对应相加然后再接个softmax就好，这样类似Bagging的做法来增强模型的鲁棒性。</li>
<li>论文另外一个需要注意的就是训练的细节了，全身框那个分支的回归就是和一般的Faster RCNN一样，直接只回归正样本的offset，但是对于可见框的回归是同时回归正样本和负样本的。这一点还是比较make sense的，因为论文中需要同一个proposal去同时回归全身框和可见框，所以对于那些高度遮挡的case，正样本proposal会比较少，反而是无遮挡或者轻微遮挡的case因为可见框和全身框的Overlap很大一般不会有明显的影响，因此这就会导致最后模型两个分支学的东西几乎是一样的。那么对于可见框分支对负样本怎么学习呢？作者将其学习目标定义为这个proposal的中心，所以它的gt为(0, 0, -INF, -INF)（实际使用的时候是(0, 0, -3, -3)），因为是log-space所以是-INF，这个可以看论文，这样就可以显示的将遮挡情况让模型进行感知。感觉这个做法会对FP的处理会比较有帮助。</li>
</ol>
</div></div><a class="button-hover more" href="2019/12/22/Bi-box-Regression-for-Pedestrian-Detection-and-Occlusion-Estimation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/19/SSA-CNN-Semantic-Self-Attention-CNN-for-Pedestrian-Detection/">SSA-CNN: Semantic Self-Attention CNN for Pedestrian Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-19</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Attention/">Attention</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1902.09080.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1902.09080.pdf</a></p>
<p>关于行人检测的论文，出发点的话感觉和这篇论文是比较类似的<a href="http://libanghuai.com/2019/12/08/Mask-Guided-Attention-Network-for-Occluded-Pedestrian-Detection/">Mask-Guided Attention Network for Occluded Pedestrian Detection</a>。主要想利用mask作为额外的监督来辅助提升检测的效果。两篇论文的Segmentation的标注也都是粗粒度的，MGAN是利用可见框标注作为mask，本篇论文是直接用的全身框作为mask。至于具体的做法可以参考论文里给的这张图：</p>
<p><img src="SSA-CNN-Semantic-Self-Attention-CNN-for-Pedestrian-Detection-屏幕快照 2019-12-19 下午10.04.51.png" alt=""></p>
<p>整体框架结构依然follow的Faster RCNN逻辑，论文所提的SSA-CNN主要分成两个部分：<strong>SSA-RPN</strong>和<strong>SSA-RCNN</strong>:</p>
<ol>
<li>SSA-RPN部分，从backbone（VGG16）的conv4_3和conv5_3分别引出一个segmentation分支，这个seg分支本身用全身框的mask去监督，这个分支的feature map然后再和对应的conv4_3或者conv5_3 feature map concat到一起引出正常的RPN业务分支cls和bbox，然后用预定义的target anchor去监督。有一个需要注意的地方是 <strong>只有conv5_3这个分支出的预测结果才会被接下来的SSA-RCNN使用，conv4_3这个分支出的seg类似一个外挂只做额外的监督，inference的时候不用</strong></li>
<li>SSA-RCNN部分，这个部分和传统的Faster RCNN的Fast RCNN分支处理不完全一样，一般的Fast RCNN是利用RPN的proposal经过ROIPooling得到对应的feature去做cls和reg，但对于SSA-RCNN则是直接利用SSA-RPN的proposal去原图中抠取，然后padding resize之后作为SSA-RCNN的输入，这样做的原因作者也在论文中阐明了: <strong>the pooling bins collapse if ROI’s input resolution is smaller than output</strong>，比如输入112x112映射到ROIPooling那就对应着7x7，但是CityPersons和 Caltech数据集有大量小于112 x 112大小的人，所以这个问题就会变得比较严重。其他SSA-RCNN模型部分就和SSA-RPN很像了，conv4_3和conv5_3引出的seg分支的feature map会先concat到一起，然后再和conv5_3 concat然后接入cls+reg来做行人的定位。</li>
</ol>
<p>这篇论文其他应该就没有需要注意的了。</p>
</div></div><a class="button-hover more" href="2019/12/19/SSA-CNN-Semantic-Self-Attention-CNN-for-Pedestrian-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/19/Pedestrian-Detection-with-Autoregressive-Network-Phases/">Pedestrian Detection with Autoregressive Network Phases</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Brazil_Pedestrian_Detection_With_Autoregressive_Network_Phases_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Brazil_Pedestrian_Detection_With_Autoregressive_Network_Phases_CVPR_2019_paper.pdf</a></p>
<p>不算老的论文，CVPR2019的关于行人检测的论文。</p>
<p><img src="Pedestrian-Detection-with-Autoregressive-Network-Phases-截屏2019-12-1923.59.25.png" alt=""></p>
</div></div><a class="button-hover more" href="2019/12/19/Pedestrian-Detection-with-Autoregressive-Network-Phases/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/16/RetinaFace-Single-stage-Dense-Face-Localisation-in-the-Wild/">RetinaFace: Single-stage Dense Face Localisation in the Wild</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-16</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Face/">Face</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Multi-Task/">Multi-Task</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1905.00641" target="_blank" rel="noopener">https://arxiv.org/abs/1905.00641</a></p>
<p>这篇论文就很糙快猛了，multitask(人脸检测+人脸关键点定位+人脸3D Mesh + 人脸分类)涨点：<br><img src="RetinaFace-Single-stage-Dense-Face-Localisation-in-the-Wild-屏幕快照 2019-05-05 下午5.45.42.jpg" alt=""><br>Loss也比较直接，Lcls为softmax loss，Lbox和Lpts为L1 Loss，Lpixel具体如下，R为3D图片在2D平面的投影，I为gt：<br><img src="RetinaFace-Single-stage-Dense-Face-Localisation-in-the-Wild-截屏2019-12-1622.39.25.png" alt=""><br>其他这篇论文似乎就没有什么可以描述的了…</p>
</div></div><a class="button-hover more" href="2019/12/16/RetinaFace-Single-stage-Dense-Face-Localisation-in-the-Wild/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/16/MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment/">MobileFAN: Transferring Deep Hidden Representation for Face Alignment</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-15</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Face/">Face</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Mimick/">Mimick</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1908.03839" target="_blank" rel="noopener">https://arxiv.org/abs/1908.03839</a></p>
<p>一篇人脸关键点的论文,论文的核心应该是模型蒸馏，但是单纯的基于mobilenetv2网络在多个benchmark也刷了挺不错的点，甚至是好于CVPR2018的LAB，MobileFAN就是mobilenetv2加上出heatmap直接出出来的结果，MobileFAN + KD就是加上知识蒸馏的结果，下表是WFLW上面的点,坦白讲点还是很高的：<br><img src="MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment-屏幕快照 2019-08-16 下午6.41.34.png" alt=""><br>知识蒸馏方面论文中提了两个方面：<br>一是<strong>Feature-Aligned Distillation</strong>，出发点是希望teacher和student网络学习的分布是一致的，两个网络deconv层的spatial维度是一致的差别就在于channel不一致，作者简单的用1x1把两者拉统一，Feature-Aligned Distillation和之前用大模型带小模型的做法基本是一致的，学习的时候直接mse监督两个feature map。<br><img src="MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment-Screenshot from 2019-08-15 10-54-20.png" alt=""><br>另一个是<strong>Feature-Similarity Distillation</strong>，这部分主要想让teacher和student网络表示的空间信息是一致的，比如人脸结构轮廓等，这一部分首先teacher和student网络分别去计算不同pixel之间的相似度：<br><img src="MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment-截屏2019-12-1622.24.41.png" alt=""><br>然后两个网络feature map之间计算相似度：<br><img src="MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment-截屏2019-12-1622.24.46.png" alt=""><br>总之感觉从结果上看还是挺惊艳的，模型蒸馏感觉对于小模型可以好好搞一下。</p>
</div></div><a class="button-hover more" href="2019/12/16/MobileFAN-Transferring-Deep-Hidden-Representation-for-Face-Alignment/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/15/Improving-Occlusion-and-Hard-Negative-Handling-for-Single-Stage-Pedestrian-Detectors/">Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Noh_Improving_Occlusion_and_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/Noh_Improving_Occlusion_and_CVPR_2018_paper.pdf</a><br>一篇行人检测的论文，论文选择在single stage架构上做文章，主要解决两个问题，一个是遮挡问题，一个是FP问题，先说遮挡问题，遮挡问题论文提出来的整体解法和zhangshifeng的那篇Occlusion-aware RCNN论文做法算是很像的了，Occlusion-aware RCNN把行人整体分成了5个part，每个part会出一个score来对feature map做attention，这篇论文就是把行人平均分成M x N(论文中去M = 6， N = 3)个part来做和Occlusion-aware RCNN类似的事情，细节的话可以参考下面这张图：</p>
<p><img src="Improving-Occlusion-and-Hard-Negative-Handling-for-Single-Stage-Pedestrian-Detectors-截屏2019-12-1523.50.06.png" alt=""></p>
<p>首先模型会出基本的reg结果，这个没有什么特殊的，这个方案特殊就是特殊在这个”Occlusion-aware Score”, 对于每一个anchor模型会去学一个M x N的矩阵，代表每一个part的confidence，这个矩阵的gt生成也比较直接，如果某一个grid中的可见行人部分(这个解决可见框标注)面积 &gt; t * area(grid)，论文中t取0.4，那么这个part的score就为1，否则就为0，因为对于每一个anchor我们只需要一个score，那么论文中也说出了两种从这个M x N矩阵中生成一个score的方法，1. argmax 2. 接几层网络以这个矩阵为输入去直接学这个score，后一个方法会考虑不同的遮挡pattern(w1….wp)会比第一个更灵活点（w也是学习的,维度也是MxN）.</p>
<p>论文另一个核心就是对FP的处理了，作者的出发点还是增强score的表现力，具体做法可以参考下面这张图，核心是这个”Grid Classifier”,对于网络中间的一些feature map比如是L1…Ln,假设对于第l个feature map，其大小为w<sub>l</sub> x h<sub>l</sub> x c<sub>l</sub>,那么通过一个1x1的conv就可以得到一个w<sub>l</sub> x h<sub>l</sub>的confidence map（至于这个confidence map如何监督也比较直接，gt就是把原来的输入图划分成w<sub>l</sub> x h<sub>l</sub> 大小，每一个小grid的gt值就是这个小grid于gt框的交面积与这个grid面积的比值），那么将这个confidence map插值到原来的输入图大小WxH，所有的1-L层feature map都做类似的事情，那么就会得到L个WxH大小的confidence map,取平均之后就可以得到最终的一个WxH的confidence map，那么对于某一个预测框，最终的score就是这个confidence map上对应预测框内部元素的均值，这个score和前面Occlusion-aware Score想乘就是最终的boundingbox的score。这一步主要是想通过多层的信息共同参与打分来增加模型的鲁棒性。</p>
<p><img src="Improving-Occlusion-and-Hard-Negative-Handling-for-Single-Stage-Pedestrian-Detectors-屏幕快照 2019-12-16 下午8.37.04.png" alt=""></p>
<p>这篇论文给我的感觉就是很复杂，trick的地方很多，分 part算score的做法也有太多类似的论文。另外论文报的点似乎没有和sota比较，在部分数据集上的结果与sota相比还是差距比较大的</p>
</div></div><a class="button-hover more" href="2019/12/15/Improving-Occlusion-and-Hard-Negative-Handling-for-Single-Stage-Pedestrian-Detectors/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/10/How-to-Write-Clean-Code/">How to Write Clean Code(Updating)</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-10</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Engineering/">Engineering</a></div></div><div class="post-content"><div class="main-content content"></div></div><a class="button-hover more" href="2019/12/10/How-to-Write-Clean-Code/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/10/Learning-Efficient-Single-stage-Pedestrian-Detectors-by-Asymptotic-Localization-Fitting/">Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-12</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf</a></p>
<p>ECCV2018行人检测的论文，在清一色的Faster RCNN系列文章中也算是一股清流了，论文用Single Shot的模型来做行人检测，也是为数不多把模型速度作为卖点和核心的论文。论文的整个核心可以理解为Cascade RCNN + RefineDet(ALF思想类似Cascade RCNN、One Stage的解决方案类似RefineDet，毕竟Cascade RCNN和RefineDet本身有些设计理念是很像的).</p>
<p><img src="Learning-Efficient-Single-stage-Pedestrian-Detectors-by-Asymptotic-Localization-Fitting-截屏2019-12-1123.53.08.png" alt=""></p>
<p>论文的核心概念是Asymptotic Localization Fitting(ALFNet), 上图示意图还是比较明显的，3个橙色feature maps是resnet/mobilenet的stage 3、4、5，绿色是直接在stage 5上接conv延伸出来的一层feature map，整体构成了类似FPN的逻辑。<br>在每个stage上都会通过CPB模块渐进式对框进行refine，这一步就和Cascade RCNN的逻辑比较像了，每个CPB都会对anchor进行一次refine，每次正负样本的IOU阈值都逐步提高，从而不断的提高框的回归精度，这里面有一个需要注意的地方就是anchor的生成只用了一个ratio: 0.41…就是CityPerson数据集的统计值，这还是有点hack数据集的意思的…</p>
</div></div><a class="button-hover more" href="2019/12/10/Learning-Efficient-Single-stage-Pedestrian-Detectors-by-Asymptotic-Localization-Fitting/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/08/Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd/">Repulsion Loss: Detecting Pedestrians in a Crowd</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL：<a href="https://zpascal.net/cvpr2018/Wang_Repulsion_Loss_Detecting_CVPR_2018_paper.pdf" target="_blank" rel="noopener">https://zpascal.net/cvpr2018/Wang_Repulsion_Loss_Detecting_CVPR_2018_paper.pdf</a></p>
<p>一篇解决行人检测遮挡场景的论文，切入点是loss，作者认为行人检测问题在遮挡场景一个比较显著的难点是boundingbox的<strong>shift</strong>问题，简言之就是由于多个人密集聚在一起的时候因为个体之间特征过于相似所以网络在预测定位的时候会产生偏移的现象，从而导致位置不够精确甚至会产生FP，论文的figure1给了一个比较简单的示意图(虚线红色框就是描述这个现象的dt框)：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-0923.54.13.png" alt=""></p>
<p>OK,那就承接上面这张图直接说一下这篇论文的核心内容，repulsion Loss:</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.03.37.png" alt=""></p>
<p>Repulsion Loss主要分成三个组成部分，L<sub>Attr</sub>, 就是一般的回归Loss比如L1、L2…之类的，毕竟为了refine定位框必要的监督信息还是需要的：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.06.10.png" alt=""></p>
<p>另一部分叫 L<sub>RepGT</sub>，这一项的目的是希望每一个proposal与相临近的gt框(不是当前proposal match的gt框)相远离，也就是对于第一张图，如果要预测紫色框，那么L<sub>RepGT</sub>就用来控制紫色框与蓝色框相远离从而缓解shift的问题，做法的话很直接，对于给定的proposal选择一个与之IoU最大的gt框然后计算Loss：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.14.24.png" alt=""></p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.14.41.png" alt=""></p>
<p>这里也引入了一个超参数来平滑loss：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.14.47.png" alt=""></p>
<p>Repulsion Loss的最后一项叫L<sub>RepBox</sub>, 其主要目的是希望对于任意match到不同gt的两个proposal之间需要尽可能的远离，然后形式上也比较类似了：</p>
<p><img src="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-1000.19.12.png" alt=""></p>
<p>整体感觉这篇论文讲的点都还make sense但是都有点治标不治本</p>
</div></div><a class="button-hover more" href="2019/12/08/Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/08/Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs/">Occluded Pedestrian Detection Through Guided Attention in CNNs</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-08</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Occluded_Pedestrian_Detection_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Occluded_Pedestrian_Detection_CVPR_2018_paper.pdf</a></p>
<p>同样是解决遮挡场景的行人检测问题，同样又是attention的逻辑…但是论文中对于基于attention去做的insight还是比较有意思的。作者在做实验可视化的时候发现网络输出的feature map对于人体不同的部位分别有不同的几组channel会对其高响应，从而也就引出了论文为什么要对channel进行attention的潜在原因，论文给的示意图还是比较直接的：</p>
<p><img src="Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs-截屏2019-12-0823.07.25.png" alt=""></p>
<p>方法上还是万年不变的faster rcnn框架再辅助一个attention分支（论文中示意的attention net），attention分支会出一个fc，维度和roi pooling之后的feature map channel数保持一致：</p>
<p><img src="Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs-截屏2019-12-0823.06.03.png" alt=""></p>
<p>至于如何attention作者也提出了三个想法，第一个就是SE Net的做法，roi pooling之后的feature直接作为attention net的输入，参数的更新直接自学习（下图的第一个Self attention net），第二个做法是利用标注的可见框，做法比较trick，需要去统计数据集，作者统计了CityPerson数据集然后把行人框分成(1) fully visible; (2) upper body visi- ble; (3) left body visible; (4) right body visible四个pattern，然后attention net核心就是一个分类模型，在学习过程中一旦pattern被确认会用conv再抽一波特征（论文没有说清楚我猜是类似ROI Pooling去crop pattern对应部位的特征），示意图就是下图的第二个Visible-box attention net。第三个做法…emm感觉更trick了，作者直接拿来一个预训练好的skeleton模型，既然模型不同channel对不同part高响应那就干脆把高响应的heatmap作为guidance来指导模型训练，做法很直接，但是个人觉得这种做法最后去分析问题的时候是不是还得去优化skeleton模型……</p>
<p><img src="Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs-截屏2019-12-0823.06.54.png" alt=""></p>
<p>然后这篇论文就没什么可以介绍的了。</p>
</div></div><a class="button-hover more" href="2019/12/08/Occluded-Pedestrian-Detection-Through-Guided-Attention-in-CNNs/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/08/Occlusion-aware-R-CNN-Detecting-Pedestrians-in-a-Crowd/">Occlusion-aware R-CNN - Detecting Pedestrians in a Crowd</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-08</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Shifeng_Zhang_Occlusion-aware_R-CNN_Detecting_ECCV_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/papers/Shifeng_Zhang_Occlusion-aware_R-CNN_Detecting_ECCV_2018_paper.pdf</a></p>
<p>这篇论文算是去年去参加ECCV2018 poster展台最火的一个了，论文同样是做遮挡场景的行人检测的，方法也是花样attention。</p>
<p>论文同样基于faster rcnn的框架来做行人检测，作者所提方法有两个核心，一个是<strong>Part Occlusion aware RoI Pooling Unit</strong>，作者把人的body分成5个part分别过ROI Pooling得到固定的输出大小，论文中是7x7，然后经过<strong>Occlusion process unit</strong>得到每个part的可见与否的置信度c，c再分别和对应的part feature相乘得到加权后的feature，最后连同body本身大part的feature通过element wise sum得到最后attention的feature：</p>
<p><img src="Occlusion-aware-R-CNN-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-0822.21.19.png" alt=""></p>
<p>论文所提方法的另外一个核心是<strong>Aggregation Loss</strong>，这玩意其实和<strong>Repulsion Loss</strong>这篇论文的理论算是很像的了，核心想法是希望同一个gt的proposals(anchors)之间需要尽可能的近:</p>
<p><img src="Occlusion-aware-R-CNN-Detecting-Pedestrians-in-a-Crowd-截屏2019-12-0822.34.14.png" alt=""></p>
<p>{t<sub>1</sub><sup>*</sup>, t<sub>2</sub><sup>*</sup>…}是涉及多个proposal（anchor）的gt集合，集合长度为ρ，{Φ1, · · · , Φρ}是上述对应的gt相对应的anchors的集合，公式写的其实很直白不赘述。</p>
</div></div><a class="button-hover more" href="2019/12/08/Occlusion-aware-R-CNN-Detecting-Pedestrians-in-a-Crowd/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/08/Mask-Guided-Attention-Network-for-Occluded-Pedestrian-Detection/">Mask-Guided Attention Network for Occluded Pedestrian Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Attention/">Attention</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1901.06651.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.06651.pdf</a></p>
<p>行人检测的一篇论文，用attention的方式解遮挡问题(这一类方法的确很多…而且做法很类似…), 这篇论文能中ICCV2019给我的感觉还是很方的…</p>
<p><img src="Mask-Guided-Attention-Network-for-Occluded-Pedestrian-Detection-截屏2019-12-0822.08.06.png" alt=""></p>
<p>做法很简单，上图中的蓝色框就是标准的基于Faster RCNN框架的行人检测逻辑，额外加了一个论文提出的MGA模块（上图中的红色框），MGA是啥呢…其实就是<strong>可见区域</strong>的mask，作者将经过ROI的feature送入到MGA模块中，几层conv之后经过sigmoid得到HxWx1的mask，这个mask再和原来的feature相乘就得到更新后的feature，这个feature再去做fast rcnn那一套逻辑，然后这篇论文的主要内容就没了…看指标点还是比较高的…之前在做人脸检测的时候用过一摸一样的方法只是当时没有明显的涨点（也有可能因为当时不是拿来做遮挡的场景的？）</p>
<p><img src="Mask-Guided-Attention-Network-for-Occluded-Pedestrian-Detection-截屏2019-12-0822.13.04.png" alt=""></p>
</div></div><a class="button-hover more" href="2019/12/08/Mask-Guided-Attention-Network-for-Occluded-Pedestrian-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/09/21/Robust-Convolutional-Neural-Network-Cascade-for-Facial-Land-mark-Localization-Exploiting-Training-Data-Augmentation/">Robust Convolutional Neural Network Cascade for Facial Land- mark Localization Exploiting Training Data Augmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Cascade/">Cascade</a></div></div><div class="post-content"><div class="main-content content"><p>2018年发表的一篇关于landmark localization的论文, 具体说一下论文的内容，论文的contribution分成两部分：cascade 的网络设计+ data augmentation</p>
<ol>
<li>Cascade网络结构：第一阶段网络根据检测器出的框crop出人脸回归出初始的landmark位置，这一部分网络通常比较大以保证第一阶段得到的landmark位置比较接近ground truth，第二阶段是一个component-wise的landmark refine网络，主要用来回归mouth、eye等形变比较常见的一些部位的landmark，最后结合第一阶段其他landmark的结果作为最终的输出。至于网络基本的结构是基于VGG的。</li>
</ol>
<p><img src="Robust-Convolutional-Neural-Network-Cascade-for-Facial-Land-mark-Localization-Exploiting-Training-Data-Augmentation-thumbnail_image002.png" alt=""></p>
<ol start="2">
<li>Data Augmentation：这一部分感觉也是比较常见的一些操作，论文主要阐述了平移的aug，范围是长宽的30%以内，加之random rotation、random Gaussian blurring、flip等，其实和目前postfilter的训练data aug是差不多的，论文强调了同样的aug在网络的第一阶段和第二阶段都应用了。<br>Data augmentation的示意：</li>
</ol>
<p><img src="Robust-Convolutional-Neural-Network-Cascade-for-Facial-Land-mark-Localization-Exploiting-Training-Data-Augmentation-thumbnail_image003.png" alt=""></p>
</div></div><a class="button-hover more" href="2019/09/21/Robust-Convolutional-Neural-Network-Cascade-for-Facial-Land-mark-Localization-Exploiting-Training-Data-Augmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/09/11/Deep-Convolutional-Network-Cascade-for-Facial-Point-Detection/">Deep Convolutional Network Cascade for Facial Point Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Landmark/">Landmark</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Cascade/">Cascade</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_cvpr_2013/papers/Sun_Deep_Convolutional_Network_2013_CVPR_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2013/papers/Sun_Deep_Convolutional_Network_2013_CVPR_paper.pdf</a></p>
<p>关于cascade landmark localization的一篇论文，是一篇比较老的CVPR论文，论文主要提出cascade的网络结构来更准的定位landmark位置</p>
<ol>
<li><p>Cascade结构：论文提出了三级的cascade结构，每一级网络都可以理解为是component-wise的设计</p>
<p>a) 第一级网络：这一级网络分成三个网路分支，第一个网络分支以整张脸为输入，回归全部的5个landmark点；第二个网络分支以脸的上半部为输入，回归两个眼睛中心点和鼻尖的位置共3个landmark点；第三个网络分支以脸的下半部为输入，回归鼻尖和两个嘴角的位置共3个landmark点；每个landmark的最终位置为三个网络的输出的均值；</p>
<p>b) 第二级和第三级网络：这两级网络的作用是一样的，每一级都有多路网络分支，每一个网络分支都只回归一个点的位置，每一个landmark点有两个网络分支去回归，两者的均值作为最终的结果，因此第二级和第三级网络分别都有10个网络分支，他们的输入都是基于第一级网络的输出landmark，从对应的landmark位置以一定的大小比例crop出输入，第三级比第二级的size更小。这两级网络学习的是相对位置的offset。</p>
<p><img src="Deep-Convolutional-Network-Cascade-for-Facial-Point-Detection-thumbnail_image002.png" alt=""></p>
<p>这是第一级网络分支的基本结构：</p>
<p><img src="Deep-Convolutional-Network-Cascade-for-Facial-Point-Detection-thumbnail_image003.png" alt=""><br>最终landmark的位置通过计算得到：</p>
<p><img src="Deep-Convolutional-Network-Cascade-for-Facial-Point-Detection-thumbnail_image004.png" alt=""></p>
</li>
</ol>
</div></div><a class="button-hover more" href="2019/09/11/Deep-Convolutional-Network-Cascade-for-Facial-Point-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/08/03/Deep-Convolutional-Neural-Networks-with-Merge-and-Run-Mappings/">Deep Convolutional Neural Networks with Merge-and-Run Mappings</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://www.ijcai.org/proceedings/2018/0440.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2018/0440.pdf</a></p>
<p>MSRA wangjingdong组的研究工作，貌似虽然中了2018年的IJCAI但是实际上这篇论文应该挂出来蛮久了。<br>论文的主要内容也很直接，就是想训练更深、更宽的网络结构，并因此提出了Merge and Run模块：</p>
<p><img src="Deep-Convolutional-Neural-Networks-with-Merge-and-Run-Mappings-截屏2019-12-2123.28.36.png" alt=""></p>
<p>Merge-and-Run模块其实和Inception-Resnet的结构有一点类似，Inception-Resnet结构在模块内是一个parallel的结构，Merge-and-Run模块本身也是两个分支parallel连接，只是不同于前者输入直接和输出sum，后者是把两个分支输入的均值分别和两个分支的输出sum，求均值和sum的操作分别被称作“merge”和 “run”, 本身结构的设计也有利于深度网络的学习,<br><img src="Deep-Convolutional-Neural-Networks-with-Merge-and-Run-Mappings-thumbnail_image004.png" alt=""> = M是幂等矩阵和resnet的恒等变换类似有助于深度网络的训练:</p>
<p><img src="Deep-Convolutional-Neural-Networks-with-Merge-and-Run-Mappings-截屏2019-12-2123.30.00.png" alt=""></p>
<p>作者也论证了这样的结构在保证相同数目结构单元的情况下比resnet、inception-restnet这样的结构更浅、更宽，因此也适合训练很深的网络结构：</p>
<ol>
<li><p>更浅：假设每一个block（上图中的1组conv）有B个layer，三个结构总的block数分别是2L、L、L个，那么对于一般的resnet结构平均深度就为BL + 2，resnet-inception结构平均深度为2BL/3 + 2，而本文提出的merge-and-run结构平均深度就为BL/3+2，+2为上图输出的FC和输入的conv，至于平均深度的计算个人理解就是block数除以可能的path数。</p>
</li>
<li><p>更宽：对于merge-and-run模块宽度更宽的论证论文做了简单的两组论证，都比较直观，inception-resnet parallel的结构比普通的resnet有更多的分支，merge-and-run与inception-resnet的差别是inception-resnet最后的结果会融合在一起而merge-and-run 最后的结果还会是两个分支：</p>
</li>
</ol>
<p><img src="Deep-Convolutional-Neural-Networks-with-Merge-and-Run-Mappings-截屏2019-12-2123.32.04.png" alt=""></p>
</div></div><a class="button-hover more" href="2019/08/03/Deep-Convolutional-Neural-Networks-with-Merge-and-Run-Mappings/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/07/16/AAR-CNNs-Auto-Adaptive-Regularized-Convolutional-Neural-Networks/">AAR-CNNs: Auto Adaptive Regularized Convolutional Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Regularization/">Regularization</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Attention/">Attention</a></div></div><div class="post-content"><div class="main-content content"><p>IJCAI2018一篇关于regularization的论文，论文的motivation是想在更细的维度做regularization，BN可以理解为batch维度的regularization，而本文提出的AAR-CNN则可以理解为channel维度和pixel维度的regularization，论文结合了SE-Net的相关内容，整篇看来其实可以理解为pixel（channel）维度的attention。</p>
<ol>
<li><p>AE-Net：Abstract Extent（AE）直译过来就是抽象程度的意思，作者认为CNN网络中不同的layer的抽象程度是不一样的，论文中也分别从pixel level和channel level来定义了AE。对于pixel level的AE，假设输入为(s1,s2,s3,s4), s1为mini-batch size，s2为channel，s3为h，s4为w，初始化AE的表达为(1,1,s3,s4),然后通过conv (1,s1,1,1) + dimshuffle + conv(1,s2,3,3)得到最后的输出v2(s1,s2,s3,s4),最后做pixel wise的tanh就得到最后的AE的表达了，channel level的做法类似通过两层FC得到：</p>
<p><img src="AAR-CNNs-Auto-Adaptive-Regularized-Convolutional-Neural-Networks-thumbnail_image002.png" alt=""></p>
</li>
<li><p>SE-Net：这一部分就是完全来自于之前读的Squeeze-and-Excitation Networks这篇论文，对于输入的每一个channel加attention：</p>
<p><img src="AAR-CNNs-Auto-Adaptive-Regularized-Convolutional-Neural-Networks-thumbnail_image003.png" alt=""></p>
</li>
</ol>
<p>所以整个pipeline就可以整合成如下的示意图，AE-net和SE-net的输出相乘就是最后regularization的表达，将其施加于输出之上产生的结果再和输出sum得到最后的输出：</p>
<p><img src="AAR-CNNs-Auto-Adaptive-Regularized-Convolutional-Neural-Networks-thumbnail_image004.png" alt=""></p>
</div></div><a class="button-hover more" href="2019/07/16/AAR-CNNs-Auto-Adaptive-Regularized-Convolutional-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/07/08/Learning-non-maximum-suppression/">Learning non-maximum suppression</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/NMS/">NMS</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Hosang_Learning_Non-Maximum_Suppression_CVPR_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/papers/Hosang_Learning_Non-Maximum_Suppression_CVPR_2017_paper.pdf</a></p>
<p>这篇论文的主要研究内容从论文题目就可以比较直观地看出来，作者想利用神经网络本身来实现NMS的功能将网络实现完全的end2end。针对目前检测模型固有的特性（如region proposal），作者提出要实现这样的功能需要从两个角度来做，一是从loss的角度对同一个物体多次检测加以惩罚，二是利用object的近邻信息来辅助网络来学习是否对同一个物体进行了多次检测，整篇文章也是从这两个角度分析。</p>
<p><img src="Learning-non-maximum-suppression-thumbnail_image002.png" alt=""></p>
<ol>
<li><strong>Loss</strong>：本文中作者设计的Loss Function，有一些细节在论文中没有提及，可以参考CVPR2016的End-to-end people detection in crowded scenes论文中的Hungarian Loss，利用GT和DT作为图的节点，GT和DT之间的距离、两者是否overlap等信息作为边的权重构造一个完整的图，利用匈牙利算法计算最大（小）匹配（对应上图的matching），yi {-1,1}就是匹配的结果,代表第i个DT是否被匹配上，Si为DT新的score信息，对应上图中的new detection score：</li>
</ol>
<p><img src="Learning-non-maximum-suppression-thumbnail_image003.png" alt=""></p>
<ol start="2">
<li><strong>GossipNet</strong>：作者设计的一个网络结构来整合DT的近邻信息来辅助判断（上图的整个示例），GNet主要有多个Block组成（论文中用的是16个），每一个block的基本结构如下图，因为是要整合近邻的信息，那么首先第一步是对于一个DTi，对于IoU(DTi, DTj) &gt; 0.2时，认为DTj属于DTi的近邻。第二步是构造下图的pairwise context，这个又可以分成两块，一块是concat(DTi, DTj), 另一块是两个DT之间的固有信息比如IoU、L1 distance、L2 distance、置信度等等，所有这些信息构成一个vector作为含3层FC的MLP网络的输入，输出就是pairwise context的另一部分。最后pairwise context信息经过多层FC和Pooling作为下一个block的输入:</li>
</ol>
<p><img src="Learning-non-maximum-suppression-thumbnail_image004.png" alt=""></p>
</div></div><a class="button-hover more" href="2019/07/08/Learning-non-maximum-suppression/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/07/01/Improving-Object-Detection-With-One-Line-of-Code/">Improving Object Detection With One Line of Code</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/NMS/">NMS</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1704.04503.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.04503.pdf</a><br>Source: <a href="https://github.com/bharatsingh430/soft-nms" target="_blank" rel="noopener">https://github.com/bharatsingh430/soft-nms</a></p>
<p>同样是一篇关于改进NMS的论文，不同于Learning non-maximum suppression，这篇论文主要分析了传统NMS算法的弊端，对其处理的逻辑进行了优化，并且号称只修改One Line of Code。传统的NMS本质是一个贪心的算法，首先选择一个confidence最高的bbox，然后剔除掉和它IoU比较大的bbox，循环这个步骤得到最后的bbox集合，那么它很明显的一个弊端就是下图的这种情况，利用传统的NMS被绿色框标记的有可能被抑制从而产生一个miss(IoU &gt; th)。如果把IoU阈值提高又有可能增加FP，因此论文针对目前的NMS弊端提出了Soft-NMS算法。Soft-nms的整个算法伪代码在下图的右侧，在原有NMS算法的基础上只进行了小幅度的改动，从复杂度上面看也几乎没有变化。</p>
<p> <img src="Improving-Object-Detection-With-One-Line-of-Code-屏幕快照 2019-12-19 下午9.47.59.png" alt=""></p>
<p> Soft-NMS：对于传统的NMS算法，对bbox的抑制可以理解为将bbox的score置为0，将bbox从候选框中完全剔除掉，那么作者为了缓解上图那样的问题提出soft-nms方法，将被抑制bbox的score设置为一个和IoU值相关的衰减函数，具体如下，传统的NMS算法则可以理解成soft-nms的特例，但考虑到上述的算分逻辑是不连续的，分数的突然抖动会对bbox的序列影响较大，因此作者follow相同的抑制逻辑，利用第二个指数形式的公式对bbox分数进行转化，同样满足离当前bbox越远的bbox分数影响越小，反之越大。</p>
<p><img src="Improving-Object-Detection-With-One-Line-of-Code-屏幕快照 2019-12-19 下午9.49.33.png" alt=""></p>
<p>Soft NMS直观的理解：对于传统的NMS，两个Object的框如果IoU比较大的话其中一个框有可能会被干掉，那么Soft NMS为了缓解这个问题会把其中一个分数较低的Object框分降低，这样它匹配的优先级会降低，在多轮迭代之后有可能会被保留下来，从而可以整体提高recall.</p>
<p>当然了SoftNMS本身的做法有点治标不治本，首先框没有明确的ID信息，这种做法同样有可能保留FP，另外对于这种Overlap也没有本质的解决</p>
</div></div><a class="button-hover more" href="2019/07/01/Improving-Object-Detection-With-One-Line-of-Code/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/06/28/Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm/">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Landmark/">Landmark</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://wywu.github.io/projects/LAB/LAB.html" target="_blank" rel="noopener">https://wywu.github.io/projects/LAB/LAB.html</a><br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image002.png" alt=""><br>本论文提出了一种基于边界信息的landmark定位方法，通过回归landmark 构成的boundary可以一定程度上解决遮挡等一些问题，boundary的一般性也得以融合多个不同的landmark标注数据集进行一同训练。此外论文也贡献了包含1w张图片的数据集WFLW。</p>
<p>论文所提的整个方法主要分成三个部分：</p>
<ol>
<li>Boundary heatmap estimator：这一部分是一个Hourglass结构，用来初步生成boundary的heatmap，需要说明的是，为了增强模型在有遮挡情况下的表现，论文引入了message passing layer来传递不同boundary之间的信息和同一个boundary不同stack之间的信息。这一部分的细节在这篇论文的补充材料里面写的比较清楚，不管是inter-level还是intra-level信息的传递都是不同feature 之间的特征融合（conv + entry-wise sum ），intra-level是不同的stack之间，inter-level是k个（k代表boundary的个数）boundary heatmap之间。<br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image003.png" alt=""></li>
<li>Boundary-aware landmarks regressor ：该模块主要用来回归heatmap：<br>a. Boundary由区域的landmark插值生成<br>b. Input image fusion： I为输入图片，Mi为第i个heatmap，乘号为element-wise dot product，加号为channel-wise concatenation<br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image004.png" alt=""><br>c.  Feature map fusion：F为feature map M为heatmap，其他和上面类似<br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image005.png" alt=""><br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image006.png" alt=""></li>
<li>Boundary effectiveness discriminator：这一部分主要引入对抗学习的思想，第一部分Boundary heatmap estimator生成的heatmap有效性被定义为：<br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image007.png" alt=""><br>M为生成的heatmap，S为对应的landmark集合，Dist为gt对应的distance matric map ，θ 和δ 分别是距离和概率的阈值，整个公式需要保证比较好的heatmap对应的landmark要尽可能多的离gt近<br>Discriminator的Loss：<br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image008.png" alt=""><br>Adversarial Loss：<br><img src="Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm-image009.png" alt=""></li>
<li>此外论文也提供了一个WFLW数据集，包含1w张图片<br>和目前不少的landmark localization方法类似，论文所提方法也是基于区域的想法去解决定位的问题，只是通过一些插值的操作做了比较细致的处理，感觉对于遮挡等一些问题会比较有帮助。</li>
</ol>
</div></div><a class="button-hover more" href="2019/06/28/Look-at-Boundary-A-Boundary-Aware-Face-Alignment-Algorithm/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/06/28/Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection/">Quantization Mimic: Towards Very Tiny CNN for Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Mimick/">Mimick</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1805.02152.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1805.02152.pdf</a><br>关于模型压缩的论文，论文致力于研究更加小型化的模型，论文定义“Very Tiny”为压缩模型的每一层channel数是原来模型的1/16或更小.从论文的标题也可以看出，论文提出的方法是结合目前比较常见的模型小型化方式：quantization 和 mimic。具体的模型结构在论文中给出了比较通俗的图例：<br><img src="Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection-image002.png" alt=""></p>
<p>具体：</p>
<ol>
<li>训练标准的大模型，论文中实验了R-FCN和Faster R-CNN两个网络模型。然后利用论文定义的量化函数Q将大模型转化成量化模型。<br><img src="Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection-image003.png" alt=""><br>公式中的三个参数α、β、γ取自D序列<img src="Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection-image004.png" alt="">, 其中s就是均匀量化的步长。函数Q在论文中说的比较不清楚，实际上参考论文中给出的示意图会比较好理解，如果在图例上以（0，0）为原点标上x, y轴看上去会比较直观。取s=1，α = 0、β = 1、γ = 2，那么对于0.5&lt;x&lt;1.5, Q(x) = 1， 然后再取α = 1、β = 2、γ = 3…依次可以得到论文中的图例。<br><img src="Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection-image005.png" alt=""></li>
<li>至于论文中的mimic部分，沿用之前曾阅读的一篇CVPR2017的论文《Mimicking Very Efficient Network for Object Detection》，利用Feature Map Mimic Learning来训练小模型，整个模型的损失函数如下：<br><img src="Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection-屏幕快照 2019-06-28 下午6.02.41.png" alt=""><br>Lm为mimic loss，其实就是两个模型feature map的L2 loss，L中的前两个Loss分别是RPN的cls和reg loss，后两个则是R-FCN或者Faster R-CNN cls和reg loss。</li>
<li>为了小模型能更好的从量化后的大模型中学习，论文对小模型也进行了量化，从模型的结构示意图中可以直观的看到。与其他的模型小型化方法在Wider Face数据集上的比较，可以看到Quantization Mimic效果还是比较明显的。<br><img src="Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection-image007.png" alt=""></li>
</ol>
</div></div><a class="button-hover more" href="2019/06/28/Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/06/24/Mimicking-Very-Efficient-Network-for-Object-Detection/">Mimicking Very Efficient Network for Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-11-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Mimick/">Mimick</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf</a><br>这篇论文可以理解为关于模型压缩的论文，论文采取大模型来训练小模型的方法，作者claim这是第一次将mimic方法运用到物体检测领域。<br>之前mimic方法通常用在分类任务中，Mimic方法的出发点是希望大模型学习到的特征可以传递给小模型，这篇论文主要有如下的contribution：</p>
<ol>
<li>Feature Map Mimic Learning：不同于分类任务中从大模型的soft targets或者logits来学习小模型，结合物体检测这个具体任务，论文提出大模型的feature map来监督训练小模型，但是CNN网络的最后一层feature map都是一些高维特征，对于一些小物体的表现会比较弱，因此论文以proposal为单位来监督训练，训练目标为：<br><img src="Mimicking-Very-Efficient-Network-for-Object-Detection-屏幕快照 2019-06-28 下午5.52.58.png" alt=""><br>L(w)为最终的Loss， Lm(W)为大小模型feature map的L2 Loss， Lgt(W)为RPN中cls和reg的Loss，ui是从大模型feature map采样得到的特征，vi是从小模型中采样得到的特征,  r为回归函数负责将vi映射到ui的维度上。后期作者多Lm（W）加上norm进行优化：<br><img src="Mimicking-Very-Efficient-Network-for-Object-Detection-image003.png" alt=""></li>
<li>具体模型结构分为两个阶段：<ol>
<li>第一阶段可以理解为对RPN的训练：大模型为预训练好的Faster RCNN或者R-FCN，小模型的最后是一个RPN网络，同一张训练图片同时经过大模型和小模型得到对应的feature maps，利用小模型RPN产生的proposal进行上面提到的feature map mimic learning进行训练。</li>
<li>第二阶段可以理解为对Faster RCNN或者R-FCN的训练，在这一部分在会加入分类任务中的logits mimic learning利用大模型的logits来监督学习。<br><img src="Mimicking-Very-Efficient-Network-for-Object-Detection-image004.png" alt=""><br>除此之外，论文还介绍了在小模型中加入deconv层来解决输入图片较小的情况：<br><img src="Mimicking-Very-Efficient-Network-for-Object-Detection-image005.png" alt=""></li>
</ol>
</li>
</ol>
</div></div><a class="button-hover more" href="2019/06/24/Mimicking-Very-Efficient-Network-for-Object-Detection/#more">阅读全文</a></div></div><div id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="page/4/">4</a><a class="extend next" rel="next" href="page/2/"><i class="fas fa-angle-right"></i></a></div></div></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fas fa-user"></i></span><span id="busuanzi_value_site_uv"></span><span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fas fa-eye"></i></span><span id="busuanzi_value_site_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Out of Memory</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--></body></html>