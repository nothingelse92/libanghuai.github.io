<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Live and Learn"><meta name="keywords" content=""><meta name="author" content="Out of Memory,undefined"><meta name="copyright" content="Out of Memory"><title>Live and Learn【Out of Memory】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="author-info"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Out of Memory</div><div class="author-info-description">Live and Learn</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/libanghuai" target="_blank">GitHub<i class="icon-dot bg-color1"></i></a><a class="links-button button-hover" href="mailto:libanghuai@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color10"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1185719433&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color1"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="archives"><span class="pull-top">日志</span><span class="pull-bottom">117</span></a><a class="author-info-articles-tags article-meta" href="tags"><span class="pull-top">标签</span><span class="pull-bottom">38</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="title-name" href="/">Out of Memory</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><div id="recent-posts"><!-- each post in page.posts.sort('date', -1).limit(10).toArray()--><!-- config中配置按照什么排序--><div class="recent-post-item"><a class="post-title" href="2020/05/06/CenterMask-single-shot-instance-segmentation-with-point-representation/">CenterMask: single shot instance segmentation with point representation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-05-07</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/2004.04446" target="_blank" rel="noopener">https://arxiv.org/abs/2004.04446</a></p>
<p>和上一篇论文重名了，也中了CVPR 2020，论文中作者将Instance Segmentation归结为两个问题，一个是如果区分Instance，另一个是如果得到更加精确的分割结果，作者提出的网络CenterMask相关设计也对应着这两部分:</p>
<p><img src="CenterMask-single-shot-instance-segmentation-with-point-representation-屏幕快照 2020-05-06 下午9.23.10.png" alt=""></p>
<p>论文中的网络结构设计主体follow CenterNet，模型总共有5个分支:</p>
<ol>
<li>Heatmap: 用来出物体的中心和类别</li>
<li>Offset: 用来refine Heatmap出的中心位置</li>
<li>Size: 用来出物体的H和W</li>
<li><p>Shape：利用Heatmap和Offset出的中心点对应的feature(长度是1 x 1 x S<sup>2</sup>)出粗分割的结果，这样Size + Shape就可以表示不同大小形态的物体了, 具体做法下图描述的挺清楚的:</p>
<p> <img src="CenterMask-single-shot-instance-segmentation-with-point-representation-屏幕快照 2020-05-06 下午9.26.37.png" alt=""></p>
</li>
<li><p>Saliency: Channel为1的输出，和Semantic Segmentation的作用一致，只是这里只需要区分前后背景不用区分具体类别</p>
</li>
<li>那么最后分割的结果就是 Mask<sub><em>result</em></sub> = Sigmoid(Mask<sub><em>Size + Shape</em></sub>) <em> Sigmoid(Mask<sub></sub></em>Saliency*)，加上Heatmap的预测结果就可以实现Instance Segmentation了</li>
<li><p>然后网络会直接监督Mask<sub><em>result</em></sub>，下图中的M<sub>k</sub>:</p>
<p> <img src="CenterMask-single-shot-instance-segmentation-with-point-representation-屏幕快照 2020-05-06 下午9.38.13.png" alt=""></p>
</li>
</ol>
<p>这篇论文的主要内容就是这些了，loss什么的细节可以具体看论文。这篇论文看到这的话我一开始有一个疑问就是既然Size + Shape目的是区分instance，那么比如框/点这样很粗的定位信息就够了…但是对于遮挡问题分割会更加友好一点..</p>
</div></div><a class="button-hover more" href="2020/05/06/CenterMask-single-shot-instance-segmentation-with-point-representation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/05/06/CenterMask-Real-Time-Anchor-Free-Instance-Segmentation/">CenterMask : Real-Time Anchor-Free Instance Segmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-05-07</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1911.06667" target="_blank" rel="noopener">https://arxiv.org/abs/1911.06667</a></p>
<p>CVPR2020的一篇分割的论文，整体novelty有限，相当于将FCOS应用到分割领域中:</p>
<p><img src="CenterMask-Real-Time-Anchor-Free-Instance-Segmentation-屏幕快照 2020-05-06 下午12.45.45.png" alt=""></p>
<p>上图就是论文所提的方法了，前半截就是标准的FCOS的流程，一个Regression分支+一个Classification分支+一个Centerness分支，后半截就是新加入的Mask分支，做法和Mask RCNN做法是一致的，首先利用Regression分支出的框去Crop Feature，这里面用的还是ROIAlign只是具体去到哪个FPN Layer上去Crop Feature作者在论文中做了一些说明，但实际上和Mask RCNN还是没差到哪去，只是在FCOS的框架下做了一下适配。</p>
<p>Mask分支的做法是这样的，首先Crop出来的14x14的Feature<strong>经过4层conv</strong>送入到一个叫SAM的Attention模块中，SAM模块呢做了这么几个事：</p>
<ol>
<li>分别经过两个Pooling，一个是Average Pooling ，一个是Max Pooling，然后把这两个Pooling的输出Concat到一起得到F<sub>concat</sub>，至于这样做有啥Insight就不太清楚了。</li>
<li>F<sub>concat</sub>再经过一个Conv得到新的Feature F<sub>new</sub>。</li>
<li>F<sub>new</sub>通过Sigmoid之后和原始的Feature相乘得到Attention之后的Feature F<sub>attention</sub>。</li>
<li>F<sub>attention</sub> 2倍Up-Sample之后再1x1去分类得到最终的结果.</li>
</ol>
<p>作者另外的改进就是VoVNetV2 backbone的改进了，两处:</p>
<ol>
<li>VoVNet + residual connection</li>
<li>SENet -&gt; eSENet（SENet原来是2层FC，第一层FC会砍channel数，第二层FC会还原回去，eSENet就省掉了第一个FC直接上第二个FC增加了不少的计算量，原因是作者认为第一个FC降channel会丢失channel的信息）</li>
</ol>
<p>各个模块的ablation(mask scoring就是Mask Score RCNN里面的方法):</p>
<p><img src="CenterMask-Real-Time-Anchor-Free-Instance-Segmentation-屏幕快照 2020-05-06 下午12.58.31.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/05/06/CenterMask-Real-Time-Anchor-Free-Instance-Segmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/24/LSTD-A-low-shot-transfer-detector-for-object-detection/">LSTD: A low-shot transfer detector for object detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1803.01529" target="_blank" rel="noopener">https://arxiv.org/abs/1803.01529</a></p>
<p>AAAI 2018的一篇论文，应该是最早做few shot for detection的论文，也是目前few shot detection论文都会引用对比的一篇论文，论文的一些想法还是比较make sense的。</p>
<p><img src="LSTD-A-low-shot-transfer-detector-for-object-detection-屏幕快照 2020-04-25 下午3.30.19.png" alt=""></p>
<p>这张图基本涵盖LSTD所有的内容了，首先从结构上来看下LSTD是怎么做的，LSTD把分类回归分成了两步，第一步做回归，分类信息只涉及是否是物体这样的二分类，然后在第二步对第一步的回归框进行细分类别而不再做回归的refine了，</p>
<ol>
<li><p>第一步的回归用的SSD，作者给出了原因:</p>
<ol>
<li><strong>SSD很显式的在做multi scale，对于小样本训练数据来说可以很好的缓解multi scale的问题</strong></li>
<li><strong>Faster RCNN最后回归的时候是N个分类器，所以它所有的分类都是类别相关的，但是SSD不是，SSD都是类别无关的相互share的，所以呢这种情况下在source domain下训练好的模型可以直接拿来做为target domain下模型的初始化</strong></li>
</ol>
</li>
<li><p>第二步最后的分类回归，主要是将Faster RCNN最后的FC层换成Conv层来实现分类，<strong>好处是减少了模型参数，一定程度上缓解了过拟合的情况</strong>。</p>
</li>
</ol>
<p>在LSTD的基础上为了更好的进行transfer learning作者还提出了两个regulazation的方法, 这两个方法主要在target domain fine tune的时候使用:</p>
<ol>
<li><p>第一个是背景抑制，这样做的好处是消除掉背景的干扰让模型专注的去处理目标信息:<br>做法很简单，用gt框生成一个mask拍到conv feature map上得到背景的特征F<sub>BD</sub>. L<sub>BD</sub> = ||F<sub>BD</sub>||<sub>2</sub><br>效果看上去还是比较明显的:</p>
<p> <img src="LSTD-A-low-shot-transfer-detector-for-object-detection-屏幕快照 2020-04-25 下午4.02.29.png" alt=""></p>
</li>
<li><p>第二个是source domain到target domain的知识迁移，出发点是source domain的信息和target domain的信息很多情况下是互通的，比如牛马羊之类的都是有一定的相似度的，所以在target domain训练数据比较缺失的情况下借助source domain的知识辅助进行学习是很有必要的。那么做法呢也很直接:</p>
<p>   首先我们有两个LSTD模型: LSTD<sub>source</sub>，这个是利用全量的source domain的数据训练的. 另一个LSTD<sub>target</sub>,它是用LSTD<sub>source</sub>进行初始化的，当然最后的分类层是随机初始化的，在一般的分类层之外还会加一个额外的分类层来把LSTD<sub>source</sub>的知识迁移过来，具体看上图很好理解。然后在学习的时候对于一张输入训练图会分别送入到LSTD<sub>source</sub>和LSTD<sub>target</sub>，得到的分类logits进行CE监督，这样就显式的将source domain的信息来监督target domain的学习:</p>
<p>   <img src="LSTD-A-low-shot-transfer-detector-for-object-detection-屏幕快照 2020-04-25 下午4.10.22.png" alt=""></p>
</li>
</ol>
<p>伪代码:</p>
<p><img src="LSTD-A-low-shot-transfer-detector-for-object-detection-屏幕快照 2020-04-25 下午4.14.32.png" alt=""></p>
<p>那么LSTD的全部就是这些内容了核心呢是降低模型学习的难度毕竟少量数据下很容易过拟合。</p>
</div></div><a class="button-hover more" href="2020/04/24/LSTD-A-low-shot-transfer-detector-for-object-detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/24/Meta-Learning-to-Detect-Rare-Objects/">Meta-Learning to Detect Rare Objects</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Meta-Learning_to_Detect_Rare_Objects_ICCV_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Meta-Learning_to_Detect_Rare_Objects_ICCV_2019_paper.pdf</a></p>
<p>ICCV 2019的论文，做few shot detection. 论文的insight作者说的比较清楚 <em>“we introduce a parameterized weight prediction meta-model that is trained on the space of model parameters to predict a category’s large-sample bounding box detection parameters from its few-shot parameters. The”</em>, 相同类别的一些数据当数据量比较小的时候(few shot)以及数据量比较大的时候学习到的分类信息肯定是不一样的，那么作者的出发点呢就是bridge这两者的差异，学习一个<strong>weight prediction meta-model</strong>来实现从few shot参数到large shot的参数转化。</p>
<p><img src="Meta-Learning-to-Detect-Rare-Objects-屏幕快照 2020-04-27 下午12.29.27.png" alt=""></p>
<p>作者将CNN模型里面的所有参数区分为类别有关的和类别无关的两种，比如对于Faster RCNN，backbone和RPN是类别无关的，而Fast RCNN部分是类别相关的。对于类别无关的参数, 直接在fine tune的时候用来初始化就好了，因为特征是类间共享的。而对于类别有关的参数，就学习一个<strong>weight prediction meta-model T</strong>来实现few shot参数到large shot参数的的转化。</p>
<p><img src="Meta-Learning-to-Detect-Rare-Objects-屏幕快照 2020-04-27 下午12.35.11.png" alt=""></p>
<p>上图呢就是在一个episode里面学习的loss监督，假设对于类别c，w<sub>det</sub><sup>c,*</sup>代表基于large shot训练出来的参数(fast rcnn的分类+定位参数), w<sub>det</sub><sup>c</sup>代表基于few shot训练出来的对应的参数，那么作者参考以前的论文将w<sub>det</sub><sup>c</sup>到w<sub>det</sub><sup>c,*</sup>的转化定义为一个回归任务φ,φ就是用一个FC实现的，所以loss上的前半截就是直接约束这个回归任务，loss的后半截就是一个标准的cls+reg的loss。</p>
<p>那么最终在训练的时候呢，第一步在全量数据上先训练一个基本的检测器，第二步呢fix住模型无关的参数(backbone + rpn), 放开模型相关的参数在MAML配置下联合训练w<sub>det</sub><sup>c,*</sup>和w<sub>det</sub><sup>c</sup>，有点蒸馏的感觉。<br>在Meta-testing阶段，meta training阶段学到的weight prediction meta-model T参数不变作为一个regulization项来监督novel class的fine tune.</p>
<p>实验结果, 放到现在点也不算高了，那篇直接fine tune head的论文可以把点刷到30+了:</p>
<p><img src="Meta-Learning-to-Detect-Rare-Objects-屏幕快照 2020-04-27 下午12.45.09.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/04/24/Meta-Learning-to-Detect-Rare-Objects/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/19/Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks/">Large-Margin Softmax Loss for Convolutional Neural Networks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://arxiv.org/abs/1612.02295" target="_blank" rel="noopener">http://arxiv.org/abs/1612.02295</a></p>
<p>ICLR2016的一篇论文，出发点挺直接的一篇论文，主要在优化Softmax Loss，提出了Large-Margin Softmax Loss来促使网络学到更加有区分度的Feature.</p>
<p>首先来分解一下Softmax Loss的计算:</p>
<p><img src="Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks-屏幕快照 2020-04-19 下午7.52.31.png" alt=""></p>
<p>这里的f就是最后一层FC的输出： f<sub>yi</sub> = W<sub>yi</sub><sup>T</sup> * x<sub>i</sub>, x<sub>i</sub>代表第i张图抽取出来的feature，W<sub>yi</sub>就是最后一层FC连接到前一层feature的weights，<strong>其实可以理解为y<sub>i</sub>类在高维空间的类中心</strong>. 又因为向量的内积可以直接转化成cosine运算:</p>
<p>W<sub>yi</sub><sup>T</sup> * x<sub>i</sub> = ||W<sub>j</sub>|| ||x<sub>i</sub>|| cos(θ<sub>j</sub>), θ<sub>j</sub>当然就是这两个向量之间的夹角了. 所以Softmax Loss又可以转化成:</p>
<p><img src="Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks-屏幕快照 2020-04-19 下午7.59.03.png" alt=""></p>
<p>那么Large-Margin Softmax Loss提出了和SVM类似的分类平面的问题，它希望inter-class之间的margin可以大一些，所以在Softmax Loss的基础上引入一个变量m来控制分类平面的margin.</p>
<p>在原始的Softmax Loss中针对一个二分类问题，假设样本x属于类别1，那么我们希望||W<sub>1</sub>|| ||x|| cos(θ<sub>1</sub>) &gt; ||W<sub>2</sub>|| ||x|| cos(θ<sub>2</sub>), (cosine在[0,π]区间内是一个单调递减的函数，同时指数函数还是一个单调递增的函数)，那么在Large-Margin Softmax Loss上就转化成||W<sub>1</sub>|| ||x|| cos(mθ<sub>1</sub>) &gt; ||W<sub>2</sub>|| ||x|| cos(θ<sub>2</sub>)，所以Large-Margin Softmax Loss：</p>
<p><img src="Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks-屏幕快照 2020-04-19 下午8.06.10.png" alt=""></p>
<p>其中:</p>
<p><img src="Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks-屏幕快照 2020-04-19 下午8.06.33.png" alt=""></p>
<p>同时满足D(π/m) = cos(π/m). 论文里面取:</p>
<p><img src="Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks-屏幕快照 2020-04-19 下午8.07.44.png" alt=""></p>
<p>论文里给的一张示意图挺能说明Large-Margin Softmax Loss的insight的：</p>
<p><img src="Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks-屏幕快照 2020-04-19 下午8.08.13.png" alt=""></p>
<p>效果上面当然也挺好了:</p>
<p><img src="Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks-屏幕快照 2020-04-19 下午8.09.06.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/04/19/Large-Margin-Softmax-Loss-for-Convolutional-Neural-Networks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/18/FeatureNMS-Non-Maximum-Suppression-by-Learning-Feature-Embeddings/">FeatureNMS: Non-Maximum Suppression by Learning Feature Embeddings</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/NMS/">NMS</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/2002.07662.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2002.07662.pdf</a></p>
<p>从NMS的角度解决行人crowd的问题，解决方法很直接，抛开IoU从框面积的角度去解overlap问题，论文直接对每一个anchor再学习一个embedding feature去辅助判断不同的物体, 伪代码写的比较清楚,集合D就是最后的输出了:</p>
<p><img src="FeatureNMS-Non-Maximum-Suppression-by-Learning-Feature-Embeddings-屏幕快照 2020-04-18 下午5.39.53.png" alt=""></p>
<p>做法上也很直接，在RetinaNet输出head，cls head + reg head之外再加一个embedding head，纬度为32维，当然这个值可以随意定了，监督方式Margin Loss:</p>
<p><img src="FeatureNMS-Non-Maximum-Suppression-by-Learning-Feature-Embeddings-屏幕快照 2020-04-18 下午5.41.28.png" alt=""><br><img src="FeatureNMS-Non-Maximum-Suppression-by-Learning-Feature-Embeddings-屏幕快照 2020-04-18 下午5.41.56.png" alt=""></p>
<p>f<sub>i</sub>和f<sub>j</sub>是两个不同的anchor对应的embeding feature，至于obj(i)就是用来判断anchor<sub>i</sub>属于哪一个object(gt box). NMS距离计算方面就是普通的L2.</p>
<p>β 和 α为两个超参数 : <em>α determines the margin between positive and negative examples, and the parameter β determines the decision threshold. (α = 0.2,β = 1.0)</em></p>
<p>从结果上看当然是涨点了，但是感觉没有说服力，所有的对比没有和sota的方法进行一个公平对比，模型学习的设置也是和sota的方法完全不一样的，比如论文里应该只用了visible box来训练：</p>
<p><img src="FeatureNMS-Non-Maximum-Suppression-by-Learning-Feature-Embeddings-屏幕快照 2020-04-18 下午5.48.08.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/04/18/FeatureNMS-Non-Maximum-Suppression-by-Learning-Feature-Embeddings/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/18/Prototypical-networks-for-few-shot-learning/">Prototypical networks for few-shot learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1703.05175.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1703.05175.pdf</a></p>
<p>NIPS的一篇论文，few shot learning比较早期的工作了，这个论文的做法和simple shot是很类似的，当然simple shot在后了，首先作者定义一个embedding function f可以将经过feature extractor的图片映射到高维空间,在这个高维空间上找到各个class的类别中心(prototype)，通过距离计算将query图片assign到对应的类别中心完成分类：</p>
<p>c<sub>k 就是类别k的prototype, S<sub>k</sub>为类别属于k的样本集合，f<sub>φ</sub>就是上面说的那个embedding function，x<sub>i</sub>就是输入图片经过feature extractor得到的特征了.</sub></p>
<p><img src="Prototypical-networks-for-few-shot-learning-屏幕快照 2020-04-18 下午4.52.53.png" alt=""></p>
<p>类别归属的判断, d就是一种距离的度量，论文里面就直接用的欧式距离:</p>
<p><img src="Prototypical-networks-for-few-shot-learning-屏幕快照 2020-04-18 下午4.56.27.png" alt=""></p>
<p>论文提供的伪代码还是比较清楚的:</p>
<p><img src="Prototypical-networks-for-few-shot-learning-屏幕快照 2020-04-18 下午4.58.37.png" alt=""></p>
<p>感觉整个论文的核心就在这了，因为是投的nips论文另外的笔墨都花在了机制的解释和论证上了，有兴趣可以直接看原文</p>
</div></div><a class="button-hover more" href="2020/04/18/Prototypical-networks-for-few-shot-learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/16/Meta-Transfer-Learning-for-Few-Shot-Learning/">Meta-Transfer Learning for Few-Shot Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Meta-Learning/">Meta Learning</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1812.02391.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1812.02391.pdf</a></p>
<p>MAML一族的解决few shot learning的问题，对MAML的基本做法也做了比较详细的了解，论文的motivation呢是解决目前神网络在few shot任务下容易过拟合的问题，所以在fix backbone的基础上学了一个比较轻量的scale/shift网络来meta learning. 论文所提的方法有两大主要内容:</p>
<ol>
<li>Meta-Transfer Learning (MTL), 方法主要的Pipeline</li>
<li>Hard Task (HT) Meta-Batch, meta learning形式下的hard mining，做法比较简单粗暴</li>
</ol>
<p><img src="Meta-Transfer-Learning-for-Few-Shot-Learning-屏幕快照 2020-04-16 下午3.08.46.png" alt=""></p>
<p>整个Pipeline:</p>
<ol>
<li>首先需要明确整个pipeline里面涉及到的模型: <strong>feature extractor(图片的特征提取),base learner(一个标注的分类模型), SS(一个用于学习kernel的scale和bias的shift的小子网络)</strong></li>
<li>先用base class的所有数据训练一个feature extractor，这算是常规的操作, <strong>这个feature extractor在后续所有的操作中都会被fix住</strong></li>
<li><p>Scaling and Shifting (SS), 这是Meta Transfer的核心，在每一轮meta task上它会和base learner同步一起学习，下图可以很好的解释SS的操作，为了避免过拟合，SS选择在feature extractor的基础上额外加一个小网络来学习transfer能力，而保证原来的feature extractor fix住，Scale学习的是kernel的scale，而Shift学习的是bias的shift，另外一种对现有网络的学习方式，下图也给了和一般的全面fine tune方式的比较：</p>
<p> <img src="Meta-Transfer-Learning-for-Few-Shot-Learning-屏幕快照 2020-04-16 下午3.21.15.png" alt=""></p>
<p> 当前这种考虑我的第一反应是SKNet之类对Kernel进行attention的方法感觉有点类似</p>
</li>
<li><p>HT Meta Batch: hide mining, 在每个meta test任务上选择acc最低的类别，然后把这些数据或者这些数据对应的类别拿出来再重点refine</p>
</li>
<li>最后在meta test阶段fix住SS只在noval class上fine tune base learner.</li>
</ol>
<p>伪代码:</p>
<p><img src="Meta-Transfer-Learning-for-Few-Shot-Learning-屏幕快照 2020-04-16 下午3.27.24.png" alt=""></p>
<p><img src="Meta-Transfer-Learning-for-Few-Shot-Learning-屏幕快照 2020-04-16 下午3.27.53.png" alt=""></p>
<p>贴一下点, 放到现在点就不算很高了:</p>
<p><img src="Meta-Transfer-Learning-for-Few-Shot-Learning-屏幕快照 2020-04-16 下午3.29.39.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/04/16/Meta-Transfer-Learning-for-Few-Shot-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/14/Learning-to-Compare-Relation-Network-for-Few-Shot-Learning/">Learning to Compare: Relation Network for Few-Shot Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-29</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf</a></p>
<p>CVPR2018的论文，整体pipeline很简单，做法也比较make sense，感觉比较不错的一篇论文，论文解决few shot的出发点是构建test image和support image之间的关系来给test image赋label。本质上和那些基于最近邻的逻辑是一样的，只是它把这个compare的过程放到网络里显式的去学习了。</p>
<p>![](Learning-to-Compare-Relation-Network-for-Few-Shot-</p>
<p>Learning-屏幕快照 2020-04-14 下午5.57.44.png)<br>上面这张图其实画的挺清楚的（5 way 1 shot），整个pipeline主要有两个模块，一个embedding module，一堆conv对image抽一下特征，另一个是relation module，这个主要是用来构建support image和query image的关系，细节做法：</p>
<ol>
<li>所有的图片过embedding module得到对应的feature map，对于C shot的C &gt; 1的话就把所有同类图片的feature map做一下element wise sum得到一个代表这个类别的feature map</li>
<li>得到C个feature map之后依次和query image的feature map concat到一起送入到一个conv block里面做回归，la</li>
<li>bel的话就是如果query image属于某个类，那个位置就为1否则为0，让网络去学习图片与类别之间的关系</li>
</ol>
<p>注意点：</p>
<ol>
<li>模型的训练还是去fine tune，先在训练集上构建episode训练然后再到support集上fine tune</li>
<li>对于zero shot，一般会给出关于类别的语意信息，那么就把原先support set产生的feature map直接改成类别的semantic embedding就好，然后其他ppl就和K shot保持一致</li>
</ol>
<p>下面这张图就是RN网络的细化：</p>
<p><img src="Learning-to-Compare-Relation-Network-for-Few-Shot-Learning-屏幕快照 2020-04-14 下午6.12.30.png" alt=""></p>
<p>贴一下结果吧,放到现在在miniImageNet上的结果就不高了:</p>
<p><img src="Learning-to-Compare-Relation-Network-for-Few-Shot-Learning-屏幕快照 2020-04-14 下午7.13.04.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/04/14/Learning-to-Compare-Relation-Network-for-Few-Shot-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/04/10/SimpleShot-Revisiting-Nearest-Neighbor-Classification-for-Few-Shot-Learning/">SimpleShot: Revisiting Nearest-Neighbor Classification for Few-Shot Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-04-10</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1911.04623.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1911.04623.pdf</a></p>
<p>一篇打脸的论文，现在做few shot任务的方法通常有meta learning/metric learning等方法，作者在论文里claim其实只要对feature extrator输出的feature进行最近邻统计就可以在few shot benchmark上得到很高的指标，这一类简单且高效的few shot方法可以参考另外两篇论文：<em>Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?</em>, 以及做检测的: <em>Frustratingly Simple Few-Shot Object Detection</em>。 论文很简短，做法很简单，效果很不错。</p>
<p>看看论文怎么做的吧，首先数据集上面会划分成D<sup>base</sup>, D<sup>noval</sup>, 然后用一般的CNN网络基于D<sup>base</sup>去训练一个分类器，这里作者也用了五个不同的backbone，Conv-4，WRN-28-10， DenseNet-121，ResNet-10/18，MobileNet.</p>
<p>当分类模型训练好以后变成feature extractor，对novel class里面的test数据提取特征，同时也对novel class里的support数据提取特征，通过计算两者的相似度(最短距离)来match test数据的label，如果support数据集里每一类的图片有多张K-shot，那么就做个平均作为这一类的类别中心，test图片的feature就和这个类别中心算最短距离就好<br>。</p>
<p>然后对于feature extractor的数据作者也做了一些ablation，一是比做任何操作，直接去计算距离，二是L2 norm一下，三是对feature减均值再做norm，作者分别做了比较详细的对比实验：</p>
<p><img src="SimpleShot-Revisiting-Nearest-Neighbor-Classification-for-Few-Shot-Learning-屏幕快照 2020-04-10 下午12.44.47.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/04/10/SimpleShot-Revisiting-Nearest-Neighbor-Classification-for-Few-Shot-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/15/Few-Shot-Object-Detection-with-Attention-RPN-and-Multi-Relation-Detector/">Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-21</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/pdf/1908.01998.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1908.01998.pdf</a></p>
<p>这应该是CVPR2020的论文了, 看完之后我觉得挺失望的。做法上我觉得是完全把用在tracking任务上的siamese网络原封不动的迁移过来了，然后换了一个few shot detection的故事在讲，虽然论文中作者在siamese网络的后面花了不少笔墨写了Patch-Relation Head, Local-Correlation Head, Global-Relation Head 但是注意看Table 3 实际上点上面最大的收益就是siamese网络，加上一堆的trick之后实际上只涨了一个点(68.6% -&gt; 69.8%).</p>
<p><img src="Few-Shot-Object-Detection-with-Attention-RPN-and-Multi-Relation-Detector-截屏2020-03-2118.40.42.png" alt=""></p>
<p>论文所提的网络结构是基于孪生网络来做的，support set和query set分别送入孪生网络的两个分支，两个分支的backbone参数是共享了，两者抽完feature之后会对两个feature做一个相似度计算得到一个新的feature map，这个feature map会送去产生proposal，这一步就是论文里面提到的<strong>attention mechanism</strong>. 然后就是这两组roi pooling之后feature的交互了，分为三种:</p>
<ol>
<li><strong>patch-relation head</strong> 两个分支的roi feature concat到一起送入一个子网络出cls + reg</li>
<li><strong>global-relation head</strong> 两个分支的roi feature concat到一起，然后走一个avg pooling 再经过两层FC后出一个score</li>
<li><strong>local-correlation head</strong> 用和孪生网络差不多的cross correlation两个feature 卷一把走fc后又出了一个分</li>
</ol>
<p>最后的分就是三者之和，reg的结果就用<strong>patch-relation head</strong>出的结果</p>
</div></div><a class="button-hover more" href="2020/03/15/Few-Shot-Object-Detection-with-Attention-RPN-and-Multi-Relation-Detector/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/14/RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection/">RepMet: Representative-based metric learning for classification and few-shot object detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-15</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1806.04728" target="_blank" rel="noopener">https://arxiv.org/abs/1806.04728</a></p>
<p>这是CVPR2019的论文了，用metric learning来做few shot的，作者在论文里主要提了一个<em>插件subnet</em>可以用来替换掉分类任务或者定位任务中的class分支。下面这张图可以比较好的阐述问题：</p>
<p><img src="RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection-截屏2020-03-1519.19.31.png" alt=""></p>
<p>因为论文所提的这个结构是一个subnet，可以无缝的插入到现有的网络结构中，所以它的输入通常是具体的feature，对于检测任务来说就是<strong>ROI的feature</strong>，输入的feature首先会经过一个embedding module，论文中对于分类任务这个embedding module就是2层FC，对于检测任务就是3层FC，这样就可以把输入的feature压缩到很小的维度(e), 再来看论文中提出的核心概念(<em>‘representatives’</em>), 作者把每一个类别抽象成k个mode，每个mode定义为e长度的vector(为了和embedding module保持一致)，具体实现的时候就是一个N x k x e大小的fc在具体操作的时候reshape一把就好，这个fc输入恒为1。</p>
<p>得到这些信息之后就可以计算距离了，embedding之后的结果和representatives直接相乘就可以得到一个N x k的结果，默认服从高斯分布, 这样可以得到每一个class对应的每一个mode的概率:</p>
<p><img src="RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection-截屏2020-03-1519.27.31.png" alt=""></p>
<p>那么embedding之后的feature或者输入feature对应这些具体类别的概率呢, 取mode里最大的概率:</p>
<p><img src="RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection-截屏2020-03-1519.27.36.png" alt=""></p>
<p>然后这篇论文不太好理解的地方是模型的训练，对于few shot检测来说，训练依然服从c way k shot这样的采样逻辑来小规模训练，每次训练的时候会用新类别的embedding结果直接替换掉原来的<strong>representatives</strong>进行fine tune学习。可能这一步就是针对小样本学习设定的。</p>
</div></div><a class="button-hover more" href="2020/03/14/RepMet-Representative-based-metric-learning-for-classification-and-few-shot-object-detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/13/Conditional-Convolutions-for-Instance-Segmentation/">Conditional Convolutions for Instance Segmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-14</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/2003.05664.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2003.05664.pdf</a></p>
<p>最新放出来的论文，做Instance Segmentation，和PolarMask一样都是follow FCOS大的框架，不同于Mask RCNN出框crop然后做Seg的方式，论文所提的方法更加类似FCN直接出，整个框架的设计感觉还是比较精巧的。具体看下怎么做：</p>
<p><img src="Conditional-Convolutions-for-Instance-Segmentation-屏幕快照 2020-03-13 下午2.42.24.png" alt=""></p>
<p>我把这个结构分成两个部分:</p>
<ol>
<li>FCOS: 也就是上图的上半部分，整个pipeline和FCOS没啥差别，只是head层的输出略有不一样，那么对于FPN每一个layer的每一个pixel，主要出三个东西:<ol>
<li>Classification Head: 和原版FCOS含义一样</li>
<li>Center-ness Head： 这个定义也是和原版FCOS一致的，用来抑制不太好的预测结果</li>
<li>Controller Head： 这个就是本篇论文的核心了，他负责出Mask FCN Head的参数，假设Mask FCN Head的总参数量是X，那么Controller Head的维度就是X，论文中当X取169的时候性能就比较不错了。</li>
</ol>
</li>
<li>Mask FCN Head: Mask FCN Head是论文的核心点，上图下半部分，它的结构就是一般的FCN，但是它的特点在于FCN的参数是动态的，不同的instance有不同的参数，这就会造成多个Mask FCN Head的感觉，同时功能上也类似Mask RCNN出框的作用-区分Instance. Mask FCN Head接在P3 Layer之后, 经过几层Conv之后得到一个H x W x C的feature map, 论文中C = 8，作者claim C的取值对分割的性能影响不大. 甚至C = 2的时候性能也只是下降0.3%！因为Mask FCN Head负责出instance，而其参数又是由P3 - P7的head层所得，所以为了构建两者的联系，在Mask FCN Head输入层F<sub>mask</sub> Concat了F<sub>mask</sub>到P3 - P7的相对位移，假设F<sub>mask</sub>的维度为H x W x C，P<sub>i</sub>的维度为H<sub>i</sub> x W<sub>i</sub> x C<sub>i</sub>, 那么把F<sub>mask</sub>的每一个pixel映射到P<sub>i</sub>，映射前后坐标的offset就会和原始的F<sub>mask</sub> Concat到一起作为<strong>Mask FCN Head</strong>的输入。</li>
</ol>
<p>最后模型的效果就是instance aware的segmentation:<br><img src="Conditional-Convolutions-for-Instance-Segmentation-屏幕快照 2020-03-13 下午3.51.40.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/03/13/Conditional-Convolutions-for-Instance-Segmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/13/Meta-R-CNN-Towards-General-Solver-for-Instance-level-Low-shot-Learning/">Meta R-CNN : Towards General Solver for Instance-level Low-shot Learning</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-14</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1909.13032" target="_blank" rel="noopener">https://arxiv.org/abs/1909.13032</a></p>
<p>ICCV 2019的论文，论文标题比较直白，meta learning + Faster/Mask RCCN做物体检测/分割。作者在论文里强调了few shot场景下物体分类和物体检测的差异，物体分类通常对全图做，物体检测由于全图有不同的多个物体所以并不适用，那么自然而然的就想到在ROI后面做。那么论文就想办法把meta learning的机制融合进two stage的pipeline里面。</p>
<p><img src="Meta-R-CNN-Towards-General-Solver-for-Instance-level-Low-shot-Learning-截屏2020-03-1423.34.50.png" alt=""></p>
<p>论文所提的网络结构也比较简单，首先保留了Faster/Mask RCNN的整个pipeline，在此基础上增加了一个Predictor-head Remodeling Network(PRN)子网络，这个自网络也是这篇论文的主要贡献所在，出发点的话和很多的few shot的论文很类似就是想办法增强网络对于不同类别的区分度，手段也是一样的就是各种atttention。PRN的作用呢也是用来学习不同class的attention编码(class-attentive vectors)。<strong>这个编码和同是ICCV2019论文的feature reweight方法的不一样点是那篇论文最后学的是channel attention，每一个channel代表一个类，以此来增强某一个或某几个channel，而这篇论文学的是对于给定类别的roi feature学习类内每个channel的权重，所以PRN输出是固定长度的vector代表每个类别对应的attention</strong>。</p>
<p>论文整体的结构就是这样，主要需要注意的应该是模型的过程了，首先模型训练的时候还是会遵循一般meta learning的策略，构建C way k shot的数据集，support集 D<sub>train</sub> 和 query 集D<sub>meta</sub>,其中D<sub>train</sub>作为Faster/Mask RCNN的输入，D<sub>meta</sub>作为PRN的输入，这样两者联动同时加上Loss<sub>cls</sub> / Loss<sub>reg</sub> / Loss<sub>meta</sub>来共同监督网络的训练。Loss<sub>meta</sub>用的是ce loss.</p>
<p>至于在最后的inference可以采取和feature reweight那篇论文一样的策略，class-attentive vector可以根据已有数据直接预处理得到就好。</p>
</div></div><a class="button-hover more" href="2020/03/13/Meta-R-CNN-Towards-General-Solver-for-Instance-level-Low-shot-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/13/Incremental-Few-Shot-Object-Detection/">Incremental Few-Shot Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-13</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Incremental-Learning/">Incremental Learning</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/2003.04668" target="_blank" rel="noopener">https://arxiv.org/abs/2003.04668</a></p>
<p>CVPR2020的论文，论文算是提出了一个新的子任务，Few Shot + Incremental Learning，这个任务论文中也给了明确的定义：</p>
<ol>
<li>首先模型的训练是可以基于量比较大的base class的数据来训练的，这样可以得到一个还不错的检测模型</li>
<li>novel class的数据可以随时<em>注册</em>，同时数据量很少，这样模型可以做到随时部署同时可以cover新增的类别</li>
</ol>
<p><img src="Incremental-Few-Shot-Object-Detection-屏幕快照 2020-03-13 下午12.10.03.png" alt=""></p>
<p>然后我们具体来看看论文是怎么做的, 首先模型方面被拆成两个部分</p>
<ol>
<li>feature extractor: 论文中描述为class-agnostic, 相当于这一块是类别无关的base class和novel class公用这一部分用来特征提取</li>
<li>class code generator: 顾名思义，学习不同类别的编码，和上述提取出来的feature整合作为最后detection的输入</li>
</ol>
<p>模型的学习也按照上面的两个模块分成了两个stage：</p>
<p>stage I: <strong>feature extractor</strong>的学习，这部分内容没有特别的地方就是CenterNet，训练完之后把detection head去掉之后就是feature extractor了</p>
<p>stage II: <strong>class code generator</strong>的学习, 这一部分的理解需要把CenterNet heatmap输出那一层进行一定的分解，假设feature extractor最后输出的feature map维度是H x W x C，总共有N个类别，那么逻辑上来讲heatmap维度应该是h x w x N，这个维度转化可以通过1 x 1的卷积达到目的，那么分解一下对于其中的一层feature map相当于H x W x C经过 1 x 1 x C的conv得到！</p>
<p>作者把这个理解为class independent的一个编码，这也是feature map做法的好处类别之间是相对独立的. <em>class code generator</em> 就被定义成学习这个class independent的编码. 至于怎么学呢, meta learning, 每一轮学习通过sample support集和query集去refine模型，对于给定一组输入，走 <em>feature extractor</em> 得到对应的feature，走 <em>class code generator</em> 得到对应的code，两者一结合就可以得到一个h x w的heatmap，然后拿这个heatmap去和gt做L1 loss从而来监督<em>class code generator</em>的学习</p>
<p>那么在Inference的时候呢对于base class的数据就直接上训练好的CenterNet，对于novel class的数据就分别走一遍<em>feature extractor</em>和<em>class code generator</em>然后得到最后的输出，这就是最后的定位结果。</p>
</div></div><a class="button-hover more" href="2020/03/13/Incremental-Few-Shot-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/03/12/Objects-as-Points/">Objects as Points</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-12</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL：<a href="https://arxiv.org/abs/1904.07850" target="_blank" rel="noopener">https://arxiv.org/abs/1904.07850</a></p>
<p>Anchor free做检测的论文CenterNet，应该是中了CVPR2019，整体是把做Pose常用的heatmap的做法放到了检测任务上，当然也可以理解为Pose Estimation/Detection的一种统一，毕竟论文里面也是给出了同样框架下在Pose Estimation和3D Detection任务上面的表现。</p>
<p>具体看下在普通的检测任务上是怎么做的吧：</p>
<p><img src="Objects-as-Points-屏幕快照 2020-03-12 下午6.27.05.png" alt=""></p>
<p>对问题的建模比较好理解，通常物体的框标注为(x1, y1, x2, y2)，那么论文中作者把物体框定义为Center Point + Width/Height，整个网络基于heatmap出结果，每一个pixel出C + 4维输出，所以heatmap最后输出的channel应该是(C + 4), C代表类别，C个channel每一个pixel位置的值就代表这个pixel属于这个类别的概率，剩下来的4维分别是基于这个pixel的offset(2维) 和 object的长宽(2维), 分类用的focal loss，回归用的L1 Loss，至于在inference的时候论文里写的也挺清楚的：找出C每个heatmap的峰值(最多100个)，峰值的定义就是比周围8个近邻大的点，找到这100个峰值之后接可以利用对应4个channel的输出还原出对应的100个框，这就是最后的结果，这些框不再需要走NMS，可以通过简单的卡阈值完成模型的输出。</p>
<p>CenterNet整体的思想就是这些，十分简单有效是一篇不错的工作，另外一个好处是这篇论文最后的代码也开源了: <a href="https://github.com/xingyizhou/CenterNet" target="_blank" rel="noopener">https://github.com/xingyizhou/CenterNet</a></p>
</div></div><a class="button-hover more" href="2020/03/12/Objects-as-Points/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/02/28/Few-shot-Object-Detection-via-Feature-Reweighting/">Few-shot Object Detection via Feature Reweighting</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-14</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1812.01866.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1812.01866.pdf</a></p>
<p>ICCV 2019的一篇论文, 做few shot detection，论文主要的贡献点是提出了一个新的few shot pipeline，主要分成三个部分： <strong>meta extractor</strong>，有点类似meta learning里面的逻辑，学习object有区分度的信息,不同的meta信息用最后feature map中不同的channel表示；<strong>reweight module</strong> 类似SE的逻辑来针对性加强不同的meta信息对于不同的输入图片,所以它输出的维度是和channle数一致的(#channel)；最后就是一个<strong>prediction module</strong>论文中用的就是yolo系列的检测框架来完成检测任务的，前面两个模块相乘之后新的feature map会作为<strong>prediction module</strong>的输入，具体的pipeline可以参考论文提供的这张图还是比较详细的：</p>
<p><img src="Few-shot-Object-Detection-via-Feature-Reweighting-截屏2020-03-0723.08.54.png" alt=""></p>
<p>那么接下来让我们看看这些模块以及最后的模型具体是怎么训练的，怎么work的，整个学习过程分为两个阶段:</p>
<ol>
<li><em>base training</em>, 用比较丰富的训练数据(base set)去学习比较好的meta extractor/reweight module/prediction module</li>
<li><em>few-shot fine-tuning</em>，用base set和novel set去一起学习，由于novel class的训练数据很少所以为了数据均衡，base class训练的时候采样和novel class保持一致，然后剩下来的过程就是<em>base training</em>阶段的就一摸一样了，只是训练时间会短很多。其实就是fine tune了。</li>
</ol>
<p>然后论文似乎主要的内容就都在这了，需要说明的几点：</p>
<ol>
<li><strong>meta extractor</strong>和<strong>prediction module</strong>虽然论文是分开说的，但应该是一体的，reweight参数修饰的feature map应该是backbone的输出，prediction module更多应该代表head部分，整个模型是基于yolov2的</li>
<li><strong>reweight module</strong>的输入是roi，也好理解毕竟是要突出具体的object，论文的做法是在rgb3个channel之上再append一个mask channel，有目标target的地方值为1其他地方值为0，在ablation里面还是解释一下直接抠图然后再append到原图和用mask这样做的差别，点上要高一点</li>
<li>论文里面还特意提了一句<strong>reweight module</strong>在inference的时候可以干掉，原因是如果我知道我将要检测哪个类别的数据，我只要把k个sample直接喂进去对weight vector取平均作为最后的weight vector，然后再inference，这个不要理解错，实际上这玩意还是去不掉的，本质和se没看到啥差别。</li>
</ol>
</div></div><a class="button-hover more" href="2020/02/28/Few-shot-Object-Detection-via-Feature-Reweighting/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/02/25/Few-shot-Adaptive-Faster-R-CNN/">Few-shot Adaptive Faster R-CNN</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-03-12</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Few-Shot/">Few Shot</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Domain-Adaptation/">Domain Adaptation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Few-Shot_Adaptive_Faster_R-CNN_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Few-Shot_Adaptive_Faster_R-CNN_CVPR_2019_paper.pdf</a></p>
<p>CVPR 2019的论文，这篇比较好的地方是在abstract部分就直接阐明了目前few shot问题存在的几个根本问题，我也比较认可论文提到的这几个痛点：</p>
<ol>
<li>target domain数据量很少，从任务本身来说就比较难，从一个domain transfer到另一个domain，在如此受限的数据情况下</li>
<li>同样也因为target domain数据量很少，导致过拟合问题是一个不得不处理的事情</li>
<li>few shot for detection目前来看实际工作不是很多，因为detection任务需要兼顾object定位和分类，所以任务本身上看比较难</li>
</ol>
<p>我们具体看下这篇论文是怎么做的，整体PPL看上去还是比较复杂的:</p>
<p><img src="Few-shot-Adaptive-Faster-R-CNN-屏幕快照 2020-02-28 下午2.41.16.png" alt=""></p>
<p>从大面上论文主要分成2个部分:</p>
<ol>
<li><strong>Image Level Domain Adaptation</strong>: 上图的上半部分就是Image Level Domain Adaptation部分，这里引入了一个SP操作（Split Pooling），这个主要是作者参考已有的论文结论觉得局部的patch可以更好的表示图片的特征。做法呢也比较简单，有一个初始框(w,h),随机再生成两个shift(δ<sub>w</sub>, δ<sub>h</sub>，这是相比较图像左上角的偏移)，这样就可以组成一个明确位置和大小的框了，然后因为是基于anchor的检测框架，所有这里的(w, h)是和anchor scale/anchor ratio保持一致的，论文里是给了9个框(3个ratio x 3个scale)，这样最后可以抽出9个patch的feature。那么论文的<strong>Pair Sampling</strong>则是GAN类似的对抗学习的过程，我们把这9个patch feature按照scale大小分成larege、medium、small三大部分，然后每一个部分分别来对抗学习，对于每一个部分组成两组pair:{s,s}, {s,t}(s代表source image,t代表target image)，然后GAN要学习的就是区分开这两者。</li>
<li><strong>Instance Level Domain Adaptation</strong>: 上图的中间部分可以理解为正常的Faster RCNN逻辑，而下半部分就可以理解为这里的Instance Level Domain Adaptation，这一部分的作用倒也可以理解，adaptation的粒度不一样，这里作者提了一个概念叫<em>Instance ROI Sampling</em>. 相比较普通的ROI Sampling主要的差别就是IOU卡的很高(&gt;0.7)这样得到的ROI更加贴近Install本身，另外就是所有的Positive都会送入到下一阶段做判断，而不是一般的RPN在处理的时候会控制前背景的比例，然后至于怎么去用GAN去区分就和<em>Image Level Domain Adaptation</em>里面提到的<strong>Pair Sampling</strong>逻辑是一摸一样的了。</li>
</ol>
<p>那么针对作者提出的过拟合问题，论文了也给了一个解法:<strong>Source Model Feature Regularization Training</strong>. 想法比较直接，现在论文提出的FAFRCNN方法是针对src + target domain的，作者另外训练一个source domain的detector，然后给定相同的souce domain的数据让这两个模型提取出来的特征尽可能相近：</p>
<p><img src="Few-shot-Adaptive-Faster-R-CNN-屏幕快照 2020-03-10 下午12.57.14.png" alt=""></p>
<p>然后论文的主要内容大概就这些，整体ppl有点太复杂不太实用。</p>
</div></div><a class="button-hover more" href="2020/02/25/Few-shot-Adaptive-Faster-R-CNN/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/02/22/Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation/">Context Contrasted Feature and Gated Multi-scale Aggregation for Scene Segmentation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-22</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ding_Context_Contrasted_Feature_CVPR_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/papers/Ding_Context_Contrasted_Feature_CVPR_2018_paper.pdf</a></p>
<p>CVPR2018的一篇语意分割的论文，整体读起来的感觉就是很复杂！主要内容的话还是围绕分割任务中context信息/local信息的获取问题，融合了CNN/RNN整个结构看起来相当的复杂。好在最后的点看上去还可以。</p>
<p><img src="Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation-截屏2020-02-2222.14.40.png" alt=""></p>
<p>作者首先阐述了针对语意分割需要解决的问题，比如上图是论文解释问题的一张图，对于pixel a那么很显然需要把它分类为car，但是在cnn处理的过程中通常整张图的特征会被占主体的一对父子所主导，因此往往对于pixel a的分类不是很准，所以需要特别注意提取pixel a的local特征，但是只有local特征很显然也是不行的，pixel a附近的context信息是很重要的起码可以形成明显的区分度帮助模型学习，所以论文核心解决的问题就是<strong>context信息/local信息的获取/使用问题</strong>。<br>先看下论文所提方法的pipeline然后在具体说一下novelty的点,直观上看就是在一般的分割pipeline基础上增加了一个ccl模块和g+(gated sum)模块,ccl用来处理论文提出的问题关于context和local信息的使用问题，gated sum可以理解为multi scale的加权问题，是对传统的multi scale通过sum/concat方法的优化:</p>
<p><img src="Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation-截屏2020-02-2222.19.47.png" alt=""></p>
<p>论文的两个novelty点:</p>
<ol>
<li><strong>Context Contrasted Local (CCL)</strong>: 简言之就是context和local信息相减来突出local信息同时不失context的信息(CCL = F<sub>l</sub>(F, Θ<sub>l</sub>) − F<sub>c</sub>(F, Θ<sub>c</sub>))，context信息利用dilation conv来解决，local信息就是普通的conv:</li>
</ol>
<p><img src="Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation-截屏2020-02-2222.23.21.png" alt=""><br>然后为了处理multi scale的问题多个ccl block存在一个级联的过程，因为conv操作本身多次重复之后就是对feature有multi scale的作用。</p>
<ol start="2">
<li><p><strong>Gated Sum</strong>: 这个操作作者提出来主要是考虑到一般的分割任务中对multi scale的feature通常采用sum或者concat的方法进行融合，作者claim对于不同的feature有可能一部分是有害的所以需要加以甄别，那么gated sum可以理解为一个对multi feature进行加权融合的过程，实现方法很复杂…用rnn做的，有兴趣的同学可以细看论文，这里不赘述，是RNN的常规应用，只是细节的确很是繁琐:</p>
<p><img src="Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation-截屏2020-02-2222.34.40.png" alt=""><br>最后点还是不错的，在coco上mean iou能到0.357，在pascal context上可以到0.516，另外看了下ablation，gated sum的操作还是可以带来2-3个点的涨幅的。</p>
</li>
</ol>
</div></div><a class="button-hover more" href="2020/02/22/Context-Contrasted-Feature-and-Gated-Multi-scale-Aggregation-for-Scene-Segmentation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/02/18/Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-28</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Label-Assign/">Label Assign</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1912.02424.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1912.02424.pdf</a></p>
<p>Label Assign另一篇比较有代表性的工作, 论文主要想探究Anchor-based方法和Anchor-free方法本质的差异性(这里主要考虑one stage的RetinaNet和FCOS)，结论是<strong>正负样本取样的差异性导致的</strong>。</p>
<p>作者首先对比了一下RetinaNet和FOCS点上面的差异，加上一堆trick之后，RetinaNet AP能到37.0%，而FCOS能到37.8%，之间有0.8%的GAP，那么就其本身这两个方法现在就剩两个不一样的地方了：</p>
<ol>
<li>正负样本定义不一样，RetinaNet是anchor-based，通过卡IOU来区分正负样本，每个pixel有多个不同的anchor，FCOS是基于点的，每个点有一个anchor point(正or负)，取决于gt框的大小和不同layer定义的回归scale。</li>
<li>回归的方式不一样，FCOS从<strong>anchor point</strong>回归，RetinaNet从<strong>anchor box</strong>回归。</li>
</ol>
<p>针对上面提到的不同的两点作者也做了一个实验，RetinaNet采用FCOS的策略可以把点从37.0%涨到37.8%，而如果FCOS采用RetinaNet的策略点就会从37.8%下降到36.9%：</p>
<p><img src="Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection-屏幕快照 2020-02-18 下午4.10.40.png" alt=""></p>
<p>那么横向看上面这张表，可以方面无论是基于anchor point回归还是基于anchor box回归点是差不多的，所以作者得出结论，RetinaNet和FCOS点有差的最大原因就是<strong>正负样本取样的差异性导致的</strong>.</p>
<p>然后就是论文的核心ATSS(Adaptive Training Sample Selection)策略了,解决<em>how to define positive and negative training samples</em>的问题.<br>具体怎么做呢：</p>
<ol>
<li>对于给定的gt框，在FPN的每一个layer上找Top K个离gt框中心最近的anchor box(距离的话就用两个框中心点的L2距离衡量)作为正样本，那么假设FPN有L个layer那么对于一个gt框就有L x K个postive正样本。</li>
<li>然后计算L x K个正样本anchor与gt框的IOU，统计出均值和标注差m和v，那么由此计算出给定gt框的IOU阈值为t = m + v. 那么最后就选择IoU大于等于t的作为正样本其余作为负样本。均值m可以控制某个layer anchor的质量，那么当然大于均值的anchor要更好了，v控制gt更加适合哪个layer。</li>
<li>在实际使用的时候(论文中的伪代码)限制选中的anchor box的中心点需要在gt框内部</li>
<li>如果一个anchor框可以和多个gt框匹配那么就去IOU最大的</li>
</ol>
<p>至于atss的解释论文给了比较多的阐述，实际也是FCOS做法的insight。最后点上面还是可以的。</p>
<p>具体算法的执行步骤论文中给的伪代码看上去会更加直接:</p>
<p><img src="Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection-屏幕快照 2020-02-20 上午11.44.01.png" alt=""></p>
</div></div><a class="button-hover more" href="2020/02/18/Bridging-the-Gap-Between-Anchor-based-and-Anchor-free-Detection-via-Adaptive-Training-Sample-Selection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/21/CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection/">CityPersons: A Diverse Dataset for Pedestrian Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-18</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="https://arxiv.org/abs/1702.05693" target="_blank" rel="noopener">https://arxiv.org/abs/1702.05693</a></p>
<p>做行人检测比较经典的数据集了，CityPersons，数据集是基于cityscapes的数据集进行refine的，CityPersons选择了CityScapes中精标注的5000张图片进行标注的(来自欧洲27个城市)，所谓的精标注就可以理解为标准的instance segmentation的标注，共有30类的类别标签，per pixel标注。CityPersons只标注CityScapes中Person和Rider两类，并将两类进一步细分为：<strong>pedestrian</strong>(walking, running or standing up), <strong>rider</strong>(riding bi- cycles or motorbikes),<strong>sitting person</strong>,and <strong>other person</strong>(with unusual postures, e.g. stretching)。</p>
<p>标注方法可以参考下图，对于Pedestrian和Rider<strong>可见框就是seg标注的最小外接矩形</strong>,<strong>全身框的话则是先标头顶到两脚中间点的直线，然后按照固定的长宽ratio 0.41拓展出框的宽度从而完成整个全身框的标注</strong>，至于遮挡的比例那就是上述两个面积的比值，而至于<strong>其他两个类别则直接用的seg的最小外接矩形没有再标注全身框</strong>，而对于假人这样的object直接标为ignore：</p>
<p><img src="CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection-屏幕快照 2020-01-21 下午3.24.25.png" alt=""></p>
<p>数据集划分：</p>
<p><img src="CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection-屏幕快照 2020-01-21 下午3.43.44.png" alt=""></p>
<p>那么在后续的论文中其实我们还会比较常见Heavy，Bare，Partial这样的划分，这个划分是Repulsion Loss那篇论文中根据CityPersons的Reseanable子集继续细分出来的，具体的可以参考Repulsion Loss这篇论文，主要是根据遮挡程度来划分的。</p>
</div></div><a class="button-hover more" href="2020/01/21/CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/15/Object-as-Distribution/">Object as Distribution</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-02-09</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1907.12929" target="_blank" rel="noopener">https://arxiv.org/abs/1907.12929</a></p>
<p>NIPS 2019的一篇论文，论文的内容还是很新颖的，在检测领域传统的物体表示是通过框来表示，那么这种表示有一个很大的弊端是结合后处理NMS针对crowd的场景几乎是无解的，所以作者提出一个思考用一个框来定位一个物体是不是合理的一种表示方式，因此论文中作者提出了用分布来标志物体，作者给的几个sample还是比较有意思的，涵盖了作者claim的crowd的场景：</p>
<p><img src="Object-as-Distribution-屏幕快照 2020-01-15 下午10.33.53.png" alt=""></p>
<p>具体的话论文中选择用二元正态分布来描述一个物体，公式没什么特别的就是标准的二元正态分布的定义:</p>
<p><img src="Object-as-Distribution-屏幕快照 2020-01-20 下午9.57.09.png" alt=""></p>
<p>从公式中我们也可以看到，那么网络需要学习的有5个参数: μ<sub>xi</sub>,μ<sub>yi</sub>,σ<sub>xi</sub>,σ<sub>yi</sub>,ρ<sub>i</sub>, 这五个参数刚好可以恢复出一个分布出来，比如 μ<sub>xi</sub>、μ<sub>yi</sub>两个参数起码已经恢复出物体的中心来了,σ<sub>xi</sub>、σ<sub>yi</sub> 这两个参数又直接和形状相关，所以用分布来表示物体本身就是make sense的，同时因为分布可以严格区分开物体的中心，从逻辑上讲是有利于解遮挡的场景的。那么为了让学习的目标是和位置无关的，对于具体的学习目标作者进行了转换：m−μ<sub>xi</sub>,n−μ<sub>yi</sub>,logσ<sub>xi</sub>,logσ<sub>yi</sub>,tanh<sup>−1</sup>ρi,(m,n)代表一个处于某个物体内的pixel坐标。</p>
<p>那么接下来就说说如何来优化训练模型，首先对于分布来说自然而然就想到用KL散度来监督两个分布之间的距离，同时论文中基于DeepLab的框架选择以联合训练的方式来训练模型，那么总的监督loss就是L<sub>seg</sub> + L<sub>KL</sub> + L<sub>cls</sub>, 那么在具体训练的时候为了加速训练以及节省资源，作者选择downsample之后监督而不是再resize，所以inference的时候就需要有一个upsample的过程，这个过程无法对边缘点有一个很好的判断，有可能会把边缘点误判为另一个分布，所以作者借助(Mixure Density Network<sub>暂时没读过这篇论文</sub>)以一种bagging的逻辑对于一个object预测多个distribution，所以那个L<sub>cls</sub>就来自于这。</p>
<p>另外需要注意的一个细节是L<sub>KL</sub>为：</p>
<p><img src="Object-as-Distribution-屏幕快照 2020-01-20 下午10.56.04.png" alt=""></p>
<p>m<sub>rep</sub>为mask，只有当cls值最高或者KL loss最小的那个分布mask才为1否则都为0！用作者论文中的话说，模型希望最好的distribution(KL Loss最低和目前选择的distribtion(cls值最大)越接近目标distribution！</p>
<p>其他论文就没有啥注意的了，NMS最后用KL距离就好。<br>论文另外有一个比较大的槽点就是实际上最后的点很低！！！</p>
</div></div><a class="button-hover more" href="2020/01/15/Object-as-Distribution/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/07/FCOS-Fully-Convolutional-One-Stage-Object-Detection/">FCOS: Fully Convolutional One-Stage Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-05-07</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classic/">Classic</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1904.01355.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.01355.pdf</a></p>
<p>2019年备受推崇的一篇anchor free的论文，最后也应该中了ICCV2019，但是对于熟悉检测领域的同学来说，看到这篇论文应该略有眼熟，这篇论文其实和Densebox应该属于一脉相承，都希望利用FCN的逻辑统一检测/分割等任务。</p>
<p>FCOS的具体想法呢是这样的，基于FPN的结构，P3 - P7得到的feature map假设分别是H<sub>i</sub> x W<sub>i</sub> x C<sub>i</sub>，那么基于H x W这么多个pixel，每个pixel都作为一个中心点去回归一个目标bbox，每个bbox的定义同样包含5个值，一个是分类score，一个是回归offset，只是这个offset的值当前这个pixel距离gt框四条边的具体(和anchor based模型一样，feature map的pixel gt的计算只需要按stride映射会原图就好)，具体的示意图如下：</p>
<p><img src="FCOS-Fully-Convolutional-One-Stage-Object-Detection-屏幕快照 2020-01-06 上午12.20.12.png" alt=""></p>
<p>几个需要注意的点:</p>
<ol>
<li>gt的生成，对于feature map上的某个pixel (x, y), 如果(x, y) 映射到原图的点(x<sup>‘</sup>, y<sup>‘</sup>)落在了某个gt框里，那么对应的offset就算(x<sup>‘</sup>, y<sup>‘</sup>)到gt框四条边的offset，分类gt也就沿用这个gt框的class标注。</li>
<li>FPN的好处一是可以fuse feature另外一个不同的layer可以针对性的回归不同scale的框，那么在FCOS中这部分是怎么做的呢，论文将P3 - P7 5个FPN层用6个值进行区间划分 ，论文中是用的区间m = [0, 64, 128, 256, 512 , ∞]，对于一个gt: (l<sup>∗</sup>, t<sup>∗</sup>, r<sup>∗</sup> ,b<sup>∗</sup>),如果满足max(l<sup>∗</sup>, t<sup>∗</sup>, r<sup>∗</sup> ,b<sup>∗</sup>) &gt; m<sub>i</sub> or max(l<sup>∗</sup>, t<sup>∗</sup>, r<sup>∗</sup> ,b<sup>∗</sup>) &lt; m<sub>i - 1</sub>，那么P<sub>3 + i</sub>就会将这个gt视为negative sample，这层不负责回归这个gt框。这么做其实会有一个问题，对于靠近gt框边缘的点有可能会落到其他layer上(也就是一个gt框内所有的pixel不一定都在一个layer里)这其实在某种程度上违背了FPN的初衷，只是在具体的实现的时候似乎可以卡中心的一些ratio来人为的干掉边缘pixel。</li>
<li>FPN另一个好处可以缓解一个pixel对目标回归的不确定性，比如下图，手拿网球拍的运动员，小的蓝色框的大部分pixel同时也落在来橙色的人体框中，这就导致一个问题，这些overlap的pixel具体需要负责去回归哪个框，通过FPN上述的分层处理可以大大缓解这个问题，论文的ablation里是有具体的数据的，感兴趣的同学可以参考原论文，那么假设在这样的情况下还是有少数不确定的pixel，那么这些pixel就负责回顾最小的那个框！</li>
</ol>
<p><img src="FCOS-Fully-Convolutional-One-Stage-Object-Detection-屏幕快照 2020-01-03 下午10.04.06.png" alt=""></p>
<p>通过上面的各种优化之后作者发现FCOS相比较anchor based的方法指标上还是有差距，问题主要是一些低质量的框环绕在gt周围（应该都是gt的边缘pixel产生的),那么为了解决这个问题，作者在cls和reg分支的基础上平行的增加了一个centerness分支，用来预测当前location(anchor point)想较gt中心的距离，centerness的gt定义如下：</p>
<p><img src="FCOS-Fully-Convolutional-One-Stage-Object-Detection-截屏2020-01-1200.31.54.png" alt=""></p>
<p>我们来看一下这个表达式，l<sup>*</sup>和r<sup>*</sup>，t<sup>*</sup>和b<sup>*</sup>是两对相互关联的变量，如果某个pixel越靠近中心点(center)那么这两对值就会越接近，那么center-ness值就会越趋向于1，如果某个pixel越远离中心点(center)，那么center-ness就会越趋向于0.所以可以理解为center-ness是一个度量离中心点越近的单位，inference的时候这个center-ness分支的结果会加权于score从而约束了偏离中心点的pixel，也抑制了大量低质量的框。因为上述的公式将centerness的值约束到了(0,1)范围内，所以最后centerness分支的学习就是用的binary cross entropy (BCE) Loss.</p>
<p>论文整体的内容应该就这些了，FCOS对后续的anchor free做法还是很有启发意义的。</p>
</div></div><a class="button-hover more" href="2020/01/07/FCOS-Fully-Convolutional-One-Stage-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2020/01/03/Focal-Loss-for-Dense-Object-Detection/">Focal Loss for Dense Object Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Loss/">Loss</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classic/">Classic</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf</a></p>
<p>重读经典系列第三篇：RetinaNet</p>
<p>ICCV 2017的Best Student Paper,也是 He Kaiming一篇很有代表性的工作，论文主要focus在One Stage Detector中样本不均衡这件事上，并且提出了<strong>Focal Loss</strong>来解决这样的问题，同时基于Focal Loss实现了一个One Stage Detector <strong>RetinaNet</strong>,可以达到Two Stage Detector的精度同时可以保持One Stage Detector的速度。</p>
<p>我们知道Two Stage Detector精度高速度慢，One Stage Detector速度快精度低，这几乎是所有人可以脱口而出的特性，那么作者认为One Stage Detector精度低的主要原因就是非常严重的class imbalance问题，基于anchor的检测器动则有10W+的anchor数目，其中Positive的anchor只有几十个，这样正负样本比几乎可以达到1:1000，大量的负样本中有很多的easy negative samples它们对模型的训练几乎无法贡献有效的信息,同时大量的这种样本本身对模型的训练也是很有害的，毕竟它们占据主要部分容易主导模型的训练。因此作者提出了Focal Loss：<br><strong>FL(p<sub>t</sub>) = −α<sub>t</sub>(1 − p<sub>t</sub>)<sup>γ</sup> log(p<sub>t</sub>)</strong><br>那么Focal Loss本身呢是来自于Cross Entropy Loss(以二分类为例):<br><img src="Focal-Loss-for-Dense-Object-Detection-屏幕快照 2020-01-03 下午8.14.46.png" alt=""><br>稍微简化一下：<br><img src="Focal-Loss-for-Dense-Object-Detection-屏幕快照 2020-01-03 下午8.15.32.png" alt=""><br>那么CE(p, y) = CE(p<sub>t</sub>) = − log(p<sub>t</sub>)<br>那么我们再来看看Focal Loss在CE Loss基础上增加的东西：</p>
<ol>
<li>−α<sub>t</sub>: 这就是简单的一个类别权重，比如可以将正样本的权重加大也是缓解class imbalance的一个选择</li>
<li>(1 − p<sub>t</sub>)<sup>γ</sup> : 这个可以理解为Focal Loss的核心吧，会整体通过模型的预测值动态的去调整loss的权重，如果某一个sample模型预测的类别是错误的那就意味着p<sub>t</sub>值会比较小（注意看p<sub>t</sub>的定义，对于每一个类别都是如此），那么Focal Loss整体就会和CE Loss差不多不会有什么影响，如果一个easy sample可以被模型很好的分类那么意味着p<sub>t</sub>值会比较大，那么Loss的权重就会变小从而优化过程中不会刻意处理，因此整个模型训练过程中都会刻意去优化hard sample。其中γ是平滑系数，论文中通过尝试γ = 2效果会比较好.</li>
</ol>
<p>至于论文中提到的RetinaNet整体其实没有什么特殊的，具体结构如下，是一个FPN的结构：<br><img src="Focal-Loss-for-Dense-Object-Detection-屏幕快照 2020-01-03 下午8.42.36.png" alt=""><br>需要注意的是:</p>
<ol>
<li>class subnet 和 box subnet参数在P3 - P7之间是共享的</li>
<li>P3 - P5通过Conv来源于C3 - C5，P6通过Conv来源于P5，P7通过Conv来源于P6</li>
<li>P3 - P7的结果会concat到一起最后一起NMS</li>
<li>为了训练的稳定性初始化有一些trick具体可以参考原论文</li>
</ol>
</div></div><a class="button-hover more" href="2020/01/03/Focal-Loss-for-Dense-Object-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/30/ACNet-Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-Asymmetric-Convolution-Blocks/">ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-03</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Basemodel/">Basemodel</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/abs/1908.03930" target="_blank" rel="noopener">https://arxiv.org/abs/1908.03930</a></p>
<p>ICCV2019的一篇工作,论文提出了Asymmetric Convolutional Block这样一个新的模块，出发点是作者发现对于我们常用的dxd的卷积核来说通常对于效果影响最大的是中心十字架状的skeleton：</p>
<p><img src="ACNet-Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-Asymmetric-Convolution-Blocks-截屏2019-12-3022.14.00.png" alt=""></p>
</div></div><a class="button-hover more" href="2019/12/30/ACNet-Strengthening-the-Kernel-Skeletons-for-Powerful-CNN-via-Asymmetric-Convolution-Blocks/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/23/MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning/">MIC: Mining Interclass Characteristics for Improved Metric Learning </a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-24</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Classification/">Classification</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Metric-Learning/">Metric Learning</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Roth_MIC_Mining_Interclass_Characteristics_for_Improved_Metric_Learning_ICCV_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2019/papers/Roth_MIC_Mining_Interclass_Characteristics_for_Improved_Metric_Learning_ICCV_2019_paper.pdf</a></p>
<p>ICCV2019一篇关于Metric Learning的论文，Metric Learning相关的工作是在做特征表示相关的内容，这篇论文的出发点是从object特征中去除非特征主体的特征从而能对于后续的任务更好的学习。论文不是很好懂需要仔细琢磨。</p>
<p><img src="MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning-截屏2019-12-2422.49.45.png" alt=""></p>
<p>对于一般的分类任务我们通常会把特征分成inter-class类间特征、intra-class类内特征, 那么这篇论文中类似，inter-class特征可以解释为区分特征主体的特征，比如区分狗和猫，intra-class特征可以解释为通用的一些特征，比如光照、角度等。对于一般的分类模型通常会忽略intra-class的特征直接用gt label来监督模型训练强行区分不同的类别。<br>那么本论文主要想细分这两个特征从而更精确的分类，假设输入图为img，f为抽特征的CNN，那么img的特征可以表示为f(img), 对于一般的metric learning通常会用一个encoder E在f(img)的基础上进一步对特征进行映射，从而基于E(f(img))做进一步的处理，比如计算相似度等，这个E的输出也可以理解为对这个img在高维特征空间的embedding。</p>
<p>类别一般的metric learning，本论文有两个encoder， encoder E<sub>α</sub> 编码inter-class特征，这个学习很容易，直接用gt的label监督就好，另一个encoder E<sub>β</sub> 编码intra-class特征，这部分不是很容易学习因为没有gt.那么论文中是怎么做的呢？</p>
<ol>
<li>首先对于类别n的所有的图片，计算每一张图的img的f(img)特征值，f通常是在ImageNet上pretrain过的某个model</li>
<li>计算所有f(img<sub>i</sub>)的mean和standard deviation（标准差）</li>
<li>最后利用公式Z<sub>i</sub> = (f(img<sub>i</sub>) - mean<sub>n</sub>) / StandardDeviation<sub>n</sub>进行特征转化</li>
</ol>
<p>这样做的目的是因为即使多张图片具体相同的intra-class特征，但是由于拍摄地点、拍摄行为等因素还是会导致不太一样，所以通过一个基本的转化得到的Z集合一定程度上消除了这种bias。</p>
<p>那么所有的图片经过上述的处理之后对于特征集合Z就可以聚类了，假设可以聚成K类{C<sub>1</sub>……C<sub>k</sub>},那么C集合就可以作为E<sub>β</sub>训练的label了！你可以把C集合想像成角度、光照、遮挡等一系列属性。</p>
<p>因为E<sub>β</sub>和E<sub>α</sub>共享f并且end2end训练，所以两者之前会相互影响，两者学习的特征也难免会有overlap，那么为了限制两者分别去学习inter-class类间特征和intra-class类内特征，作者也做了一些优化（ Minimizing Mutual Information）。具体可以参考下面这个公式，R是一个小网络用来将E<sub>β</sub>编码的信息映射到E<sub>α</sub>的空间上，⊙就是普通的element wise乘，r代表梯度反转，是对抗学习中常用的一种方式，和实际的需求也比较接近：</p>
<p><img src="MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning-截屏2019-12-2423.35.20.png" alt=""></p>
</div></div><a class="button-hover more" href="2019/12/23/MIC-Mining-Interclass-Characteristics-for-Improved-Metric-Learning/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/22/High-level-Semantic-Feature-Detection-A-New-Perspective-for-Pedestrian-Detection/">High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Object-Detection/">Object Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Face/">Face</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_High-Level_Semantic_Feature_Detection_A_New_Perspective_for_Pedestrian_Detection_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_High-Level_Semantic_Feature_Detection_A_New_Perspective_for_Pedestrian_Detection_CVPR_2019_paper.pdf</a><br>这是CVPR2019的行人检测论文，和Center and Scale Prediction: A Box-free Approach for Object Detection是同一篇论文，倒是后者的标题更能体现出论文所提的方法，论文主要就是异于Anchor Based的方法提出了预测中心点+框的scale的新方法来解决行人检测的问题或者严格来讲解决一般刚性物体的检测问题. 从行人检测的角度来说是一个比较新颖也比较值得去思考的方法。下图是它整体的Pipeline：</p>
<p><img src="High-level-Semantic-Feature-Detection-A-New-Perspective-for-Pedestrian-Detection-屏幕快照 2019-12-22 下午4.35.00.png" alt=""></p>
<p>论文所提的方法三言两语倒是可以说出大概，但是一些细节还是值得去琢磨的：</p>
<ol>
<li>模型结构很简单，Res50，stage 2 - 5通过deconv将高层feature map统一到一个固定的resolution然后concat到一起，其实比较类似FPN了，r倍downsample之后接一层Conv3x3然后再分别接两个Conv1x1引出两个branch，一个预测中心点，一个预测Scale，两者都是用heatmap出.</li>
<li>模型结构很简单，比较重要的就是gt是怎么生成的以及如何去监督了，首先说center的heatmap，它的大小（w x h）和downsample之后的feature map维度是一致的, 给定一个bounding box可以得到一个标注的中心，那么原则上只有这个点的gt为1，其余位置gt都为0，那么论文中为了更加符合实际场景同时也更加便于训练，用了一个比较常用的高斯分布来生成gt。而至于scale map，对于给定的中心点，它的取值是log(h),h为boundingbox的高度，这样可以通过事先定义的ratio反算出具体的框。同样的在实际生成gt的时候会围绕中心点半径2的范围内都置为log(h)其余为0；</li>
<li>Loss方面center的定位可以看为classification任务所以用ce loss，而scale的预测可以理解为regression任务所以用smoothL1 loss。</li>
<li>因为gt的监督是基于r倍downsample的所以为了更精确的预测会自然想到加一个offset分支来更精确的回归：<br>L = λ<sub>c</sub>L<sub>center</sub> + λ<sub>s</sub>L<sub>scale</sub> + λ<sub>o</sub>L<sub>offset</sub></li>
</ol>
<p>最后就是这篇论文刷的点还是比较高的，也会是做检测比较好的一个方向。</p>
</div></div><a class="button-hover more" href="2019/12/22/High-level-Semantic-Feature-Detection-A-New-Perspective-for-Pedestrian-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/22/Bi-box-Regression-for-Pedestrian-Detection-and-Occlusion-Estimation/">Bi-box Regression for Pedestrian Detection and Occlusion Estimation</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-23</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf</a></p>
<p>ECCV2018的一篇行人检测的问题，论文的做法其实比较简单，就是让模型在学习全身框的同时也出可见框的结果，两者可以起到互补的作用，整体模型结构也比较简单就是在backbone之后接两个平行的业务层，一个出可见框的结果一个出全身框的结构，两个平行业务层的结构是一致的，具体的逻辑可以参考论文里的下面这张图：</p>
<p><img src="Bi-box-Regression-for-Pedestrian-Detection-and-Occlusion-Estimation-屏幕快照 2019-12-22 下午2.40.42.png" alt=""></p>
<p>那么这篇论文需要注意的更多的是一些细节：</p>
<ol>
<li>虽然模型最后是出两个平行的业务层一个出可见框的结果一个出全身框的结果，但是两者的proposal或者anchor是一摸一样的！假设目前对于同一个标注的gt，可见框为Box<sub>visible</sub>,全身框为Box<sub>full body</sub>,那么对于个给定的proposal P, 当IoU(P, Box<sub>full body</sub>) &gt; $\alpha$ &amp;&amp; IoB(P, Box<sub>visible</sub>) &gt;  $\beta$ 是P为正样本，否则P就是负样本。</li>
<li>因为两个branch用的是同一个proposal，然后基于这个proposal去分别回归offset，那么最后在后处理的时候这个proposal就只能有一个score，作者给了三个方法，第一只考虑可见分支，fc出的结果来个softmax就好，第二是只考虑全身分支，同样fc出的结果来个softmax就好，第三就是融合两个分支的结果把两个fc的结果对应相加然后再接个softmax就好，这样类似Bagging的做法来增强模型的鲁棒性。</li>
<li>论文另外一个需要注意的就是训练的细节了，全身框那个分支的回归就是和一般的Faster RCNN一样，直接只回归正样本的offset，但是对于可见框的回归是同时回归正样本和负样本的。这一点还是比较make sense的，因为论文中需要同一个proposal去同时回归全身框和可见框，所以对于那些高度遮挡的case，正样本proposal会比较少，反而是无遮挡或者轻微遮挡的case因为可见框和全身框的Overlap很大一般不会有明显的影响，因此这就会导致最后模型两个分支学的东西几乎是一样的。那么对于可见框分支对负样本怎么学习呢？作者将其学习目标定义为这个proposal的中心，所以它的gt为(0, 0, -INF, -INF)（实际使用的时候是(0, 0, -3, -3)），因为是log-space所以是-INF，这个可以看论文，这样就可以显示的将遮挡情况让模型进行感知。感觉这个做法会对FP的处理会比较有帮助。</li>
</ol>
</div></div><a class="button-hover more" href="2019/12/22/Bi-box-Regression-for-Pedestrian-Detection-and-Occlusion-Estimation/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/19/SSA-CNN-Semantic-Self-Attention-CNN-for-Pedestrian-Detection/">SSA-CNN: Semantic Self-Attention CNN for Pedestrian Detection</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-19</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Attention/">Attention</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Segmentation/">Segmentation</a></div></div><div class="post-content"><div class="main-content content"><p>URL: <a href="https://arxiv.org/pdf/1902.09080.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1902.09080.pdf</a></p>
<p>关于行人检测的论文，出发点的话感觉和这篇论文是比较类似的<a href="http://libanghuai.com/2019/12/08/Mask-Guided-Attention-Network-for-Occluded-Pedestrian-Detection/">Mask-Guided Attention Network for Occluded Pedestrian Detection</a>。主要想利用mask作为额外的监督来辅助提升检测的效果。两篇论文的Segmentation的标注也都是粗粒度的，MGAN是利用可见框标注作为mask，本篇论文是直接用的全身框作为mask。至于具体的做法可以参考论文里给的这张图：</p>
<p><img src="SSA-CNN-Semantic-Self-Attention-CNN-for-Pedestrian-Detection-屏幕快照 2019-12-19 下午10.04.51.png" alt=""></p>
<p>整体框架结构依然follow的Faster RCNN逻辑，论文所提的SSA-CNN主要分成两个部分：<strong>SSA-RPN</strong>和<strong>SSA-RCNN</strong>:</p>
<ol>
<li>SSA-RPN部分，从backbone（VGG16）的conv4_3和conv5_3分别引出一个segmentation分支，这个seg分支本身用全身框的mask去监督，这个分支的feature map然后再和对应的conv4_3或者conv5_3 feature map concat到一起引出正常的RPN业务分支cls和bbox，然后用预定义的target anchor去监督。有一个需要注意的地方是 <strong>只有conv5_3这个分支出的预测结果才会被接下来的SSA-RCNN使用，conv4_3这个分支出的seg类似一个外挂只做额外的监督，inference的时候不用</strong></li>
<li>SSA-RCNN部分，这个部分和传统的Faster RCNN的Fast RCNN分支处理不完全一样，一般的Fast RCNN是利用RPN的proposal经过ROIPooling得到对应的feature去做cls和reg，但对于SSA-RCNN则是直接利用SSA-RPN的proposal去原图中抠取，然后padding resize之后作为SSA-RCNN的输入，这样做的原因作者也在论文中阐明了: <strong>the pooling bins collapse if ROI’s input resolution is smaller than output</strong>，比如输入112x112映射到ROIPooling那就对应着7x7，但是CityPersons和 Caltech数据集有大量小于112 x 112大小的人，所以这个问题就会变得比较严重。其他SSA-RCNN模型部分就和SSA-RPN很像了，conv4_3和conv5_3引出的seg分支的feature map会先concat到一起，然后再和conv5_3 concat然后接入cls+reg来做行人的定位。</li>
</ol>
<p>这篇论文其他应该就没有需要注意的了。</p>
</div></div><a class="button-hover more" href="2019/12/19/SSA-CNN-Semantic-Self-Attention-CNN-for-Pedestrian-Detection/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="2019/12/19/Pedestrian-Detection-with-Autoregressive-Network-Phases/">Pedestrian Detection with Autoregressive Network Phases</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2019-12-20</time><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="tags/Pedestrian-Detection/">Pedestrian Detection</a></div></div><div class="post-content"><div class="main-content content"><p>URL:<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Brazil_Pedestrian_Detection_With_Autoregressive_Network_Phases_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Brazil_Pedestrian_Detection_With_Autoregressive_Network_Phases_CVPR_2019_paper.pdf</a></p>
<p>不算老的论文，CVPR2019的关于行人检测的论文。</p>
<p><img src="Pedestrian-Detection-with-Autoregressive-Network-Phases-截屏2019-12-1923.59.25.png" alt=""></p>
</div></div><a class="button-hover more" href="2019/12/19/Pedestrian-Detection-with-Autoregressive-Network-Phases/#more">阅读全文</a></div></div><div id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="page/4/">4</a><a class="extend next" rel="next" href="page/2/"><i class="fas fa-angle-right"></i></a></div></div></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fas fa-user"></i></span><span id="busuanzi_value_site_uv"></span><span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fas fa-eye"></i></span><span id="busuanzi_value_site_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Out of Memory</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--></body></html>